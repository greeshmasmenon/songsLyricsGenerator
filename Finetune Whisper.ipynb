{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02679f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install evaluate\n",
    "!pip install git+https://github.com/huggingface/datasets\n",
    "!pip install git+https://github.com/huggingface/transformers\n",
    "!pip install librosa\n",
    "!pip install evaluate>=0.3.0\n",
    "!pip install jiwer\n",
    "!pip install gradio\n",
    "!pip install more-itertools\n",
    "! pip install transformers[torch]\n",
    "! pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f3df696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from datasets import DatasetDict, IterableDatasetDict, interleave_datasets, load_dataset\n",
    "from torch.utils.data import IterableDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce895fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoFeatureExtractor,\n",
    "    AutoModelForSpeechSeq2Seq,\n",
    "    AutoProcessor,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainerCallback,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "from transformers.trainer_pt_utils import IterableDatasetShard\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "362f4f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "check_min_version(\"4.25.0.dev0\")\n",
    "require_version(\"datasets>=1.18.2\", \"To fix: pip install -r examples/pytorch/speech-recognition/requirements.txt\")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa0badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    feature_extractor_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"feature extractor name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
    "                \"with private models).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    freeze_feature_encoder: bool = field(\n",
    "        default=True, metadata={\"help\": \"Whether to freeze the feature encoder layers of the model.\"}\n",
    "    )\n",
    "    freeze_encoder: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to freeze the entire encoder of the seq2seq model.\"}\n",
    "    )\n",
    "    forced_decoder_ids: List[List[int]] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"A list of pairs of integers which indicates a mapping from generation indices to token indices \"\n",
    "                \"that will be forced before sampling. For example, [[0, 123]] means the first generated token \"\n",
    "                \"will always be a token of index 123.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    suppress_tokens: List[int] = field(\n",
    "        default=None, metadata={\"help\": \"A list of tokens that will be suppressed at generation.\"}\n",
    "    )\n",
    "    model_index_name: str = field(default=None, metadata={\"help\": \"Pretty name for the model card.\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4c9d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name: str = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    text_column: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The name of the column in the datasets containing the full texts (for summarization).\"},\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    audio_column_name: str = field(\n",
    "        default=\"audio\",\n",
    "        metadata={\"help\": \"The name of the dataset column containing the audio data. Defaults to 'audio'\"},\n",
    "    )\n",
    "    text_column_name: str = field(\n",
    "        default=\"text\",\n",
    "        metadata={\"help\": \"The name of the dataset column containing the text data. Defaults to 'text'\"},\n",
    "    )\n",
    "    max_duration_in_seconds: float = field(\n",
    "        default=20.0,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Truncate audio files that are longer than `max_duration_in_seconds` seconds to\"\n",
    "                \" 'max_duration_in_seconds`\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    min_duration_in_seconds: float = field(\n",
    "        default=0.0, metadata={\"help\": \"Filter audio files that are shorter than `min_duration_in_seconds` seconds\"}\n",
    "    )\n",
    "    train_split_name: str = field(\n",
    "        default=\"train\",\n",
    "        metadata={\n",
    "            \"help\": \"The name of the training data set split to use (via the datasets library). Defaults to 'train'\"\n",
    "        },\n",
    "    )\n",
    "    eval_split_name: str = field(\n",
    "        default=\"test\",\n",
    "        metadata={\n",
    "            \"help\": \"The name of the training data set split to use (via the datasets library). Defaults to 'train'\"\n",
    "        },\n",
    "    )\n",
    "    do_lower_case: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether the target text should be lower cased.\"},\n",
    "    )\n",
    "    do_remove_punctuation: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether the target text should be striped of punctuation.\"},\n",
    "    )\n",
    "    do_normalize_eval: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to normalise the references and predictions in the eval WER calculation.\"},\n",
    "    )\n",
    "    language: str = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Language for multilingual fine-tuning. This argument should be set for multilingual fine-tuning \"\n",
    "                \"only. For English speech recognition, it should be set to `None`.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    task: str = field(\n",
    "        default=\"transcribe\",\n",
    "        metadata={\"help\": \"Task, either `transcribe` for speech recognition or `translate` for speech translation.\"},\n",
    "    )\n",
    "    shuffle_buffer_size: Optional[int] = field(\n",
    "        default=500,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The number of streamed examples to download before shuffling them. The large the buffer, \"\n",
    "                \"the closer it is to real offline shuffling.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    streaming: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use streaming mode to load and pre-process the data.\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c052a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor ([`WhisperProcessor`])\n",
    "            The processor used for processing the data.\n",
    "        decoder_start_token_id (`int`)\n",
    "            The begin-of-sentence of the decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        model_input_name = self.processor.model_input_names[0]\n",
    "        input_features = [{model_input_name: feature[model_input_name]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dd4937",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_maybe_streaming_dataset(dataset_name, dataset_config_name, split=\"train\", streaming=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Utility function to load a dataset in streaming mode. For datasets with multiple splits,\n",
    "    each split is loaded individually and then splits combined by taking alternating examples from\n",
    "    each (interleaving).\n",
    "    \"\"\"\n",
    "    if \"+\" in split:\n",
    "        # load multiple splits separated by the `+` symbol with streaming mode\n",
    "        dataset_splits = [\n",
    "            load_dataset(dataset_name, dataset_config_name, split=split_name, streaming=streaming, **kwargs)\n",
    "            for split_name in split.split(\"+\")\n",
    "        ]\n",
    "        # interleave multiple splits to form one dataset\n",
    "        interleaved_dataset = interleave_datasets(dataset_splits)\n",
    "        return interleaved_dataset\n",
    "    else:\n",
    "        # load a single split *with* streaming mode\n",
    "        dataset = load_dataset(dataset_name, dataset_config_name, split=split, streaming=streaming, **kwargs)\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf59eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "# 1. Parse input arguments\n",
    "# See all possible arguments in src/transformers/training_args.py\n",
    "# or by passing the --help flag to this script.\n",
    "# We now keep distinct sets of args, for a cleaner separation of concerns.\n",
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments))\n",
    "\n",
    "if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
    "    # If we pass only one argument to the script and it's the path to a json file,\n",
    "    # let's parse it to get our arguments.\n",
    "    model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
    "else:\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "# Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n",
    "# information sent is the one passed as arguments along with your Python/PyTorch versions.\n",
    "send_example_telemetry(\"run_speech_recognition_seq2seq_streaming\", model_args, data_args)\n",
    "\n",
    "# 2. Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "logger.setLevel(logging.INFO if is_main_process(training_args.local_rank) else logging.WARN)\n",
    "\n",
    "# Log on each process the small summary:\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")\n",
    "\n",
    "# Set the verbosity to info of the Transformers logger (on main process only):\n",
    "if is_main_process(training_args.local_rank):\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "# 3. Detecting last checkpoint and eventually continue from last checkpoint\n",
    "last_checkpoint = None\n",
    "if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "        logger.info(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )\n",
    "\n",
    "# Set seed before initializing model.\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "# 4. Load dataset\n",
    "raw_datasets = IterableDatasetDict() if data_args.streaming else DatasetDict()\n",
    "\n",
    "if training_args.do_train:\n",
    "    raw_datasets[\"train\"] = load_maybe_streaming_dataset(\n",
    "        data_args.dataset_name,\n",
    "        data_args.dataset_config_name,\n",
    "        split=data_args.train_split_name,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "        streaming=data_args.streaming,\n",
    "    )\n",
    "\n",
    "if training_args.do_eval:\n",
    "    raw_datasets[\"eval\"] = load_maybe_streaming_dataset(\n",
    "        data_args.dataset_name,\n",
    "        data_args.dataset_config_name,\n",
    "        split=data_args.eval_split_name,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "        streaming=data_args.streaming,\n",
    "    )\n",
    "\n",
    "raw_datasets_features = list(next(iter(raw_datasets.values())).features.keys())\n",
    "\n",
    "if data_args.audio_column_name not in raw_datasets_features:\n",
    "    raise ValueError(\n",
    "        f\"--audio_column_name '{data_args.audio_column_name}' not found in dataset '{data_args.dataset_name}'. \"\n",
    "        \"Make sure to set `--audio_column_name` to the correct audio column - one of \"\n",
    "        f\"{', '.join(raw_datasets_features)}.\"\n",
    "    )\n",
    "\n",
    "if data_args.text_column_name not in raw_datasets_features:\n",
    "    raise ValueError(\n",
    "        f\"--text_column_name {data_args.text_column_name} not found in dataset '{data_args.dataset_name}'. \"\n",
    "        \"Make sure to set `--text_column_name` to the correct text column - one of \"\n",
    "        f\"{', '.join(raw_datasets_features)}.\"\n",
    "    )\n",
    "\n",
    "# 5. Load pretrained model, tokenizer, and feature extractor\n",
    "#\n",
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "\n",
    "config.update({\"forced_decoder_ids\": model_args.forced_decoder_ids, \"suppress_tokens\": model_args.suppress_tokens})\n",
    "\n",
    "if training_args.gradient_checkpointing:\n",
    "    config.update({\"use_cache\": False})\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    model_args.feature_extractor_name if model_args.feature_extractor_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_fast=model_args.use_fast_tokenizer,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    config=config,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "\n",
    "if model.config.decoder_start_token_id is None:\n",
    "    raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
    "\n",
    "if model_args.freeze_feature_encoder:\n",
    "    model.freeze_feature_encoder()\n",
    "\n",
    "if model_args.freeze_encoder:\n",
    "    model.freeze_encoder()\n",
    "\n",
    "if data_args.language is not None:\n",
    "    # We only need to set the task id when the language is specified (i.e. in a multilingual setting)\n",
    "    tokenizer.set_prefix_tokens(language=data_args.language, task=data_args.task)\n",
    "\n",
    "# 6. Resample speech dataset if necessary\n",
    "dataset_sampling_rate = next(iter(raw_datasets.values())).features[data_args.audio_column_name].sampling_rate\n",
    "if dataset_sampling_rate != feature_extractor.sampling_rate:\n",
    "    raw_datasets = raw_datasets.cast_column(\n",
    "        data_args.audio_column_name, datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate)\n",
    "    )\n",
    "\n",
    "# 7. Preprocessing the datasets.\n",
    "# We need to read the audio files as arrays and tokenize the targets.\n",
    "max_input_length = data_args.max_duration_in_seconds * feature_extractor.sampling_rate\n",
    "min_input_length = data_args.min_duration_in_seconds * feature_extractor.sampling_rate\n",
    "audio_column_name = data_args.audio_column_name\n",
    "text_column_name = data_args.text_column_name\n",
    "model_input_name = feature_extractor.model_input_names[0]\n",
    "do_lower_case = data_args.do_lower_case\n",
    "do_remove_punctuation = data_args.do_remove_punctuation\n",
    "normalizer = BasicTextNormalizer()  # 'official' text normalizer from OpenAI\n",
    "\n",
    "if data_args.max_train_samples is not None:\n",
    "    raw_datasets[\"train\"] = (\n",
    "        raw_datasets[\"train\"].take(data_args.max_train_samples)\n",
    "        if data_args.streaming\n",
    "        else raw_datasets[\"train\"].select(range(data_args.max_train_samples))\n",
    "    )\n",
    "\n",
    "if data_args.max_eval_samples is not None:\n",
    "    raw_datasets[\"eval\"] = (\n",
    "        raw_datasets[\"eval\"].take(data_args.max_eval_samples)\n",
    "        if data_args.streaming\n",
    "        else raw_datasets[\"eval\"].select(range(data_args.max_eval_samples))\n",
    "    )\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # process audio\n",
    "    sample = batch[audio_column_name]\n",
    "    inputs = feature_extractor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"])\n",
    "    # process audio length\n",
    "    batch[model_input_name] = inputs.get(model_input_name)[0]\n",
    "    batch[\"input_length\"] = len(sample[\"array\"])\n",
    "\n",
    "    # process targets\n",
    "    input_str = batch[text_column_name].lower() if do_lower_case else batch[text_column_name]\n",
    "    if do_remove_punctuation:\n",
    "        input_str = normalizer(input_str).strip()\n",
    "    batch[\"labels\"] = tokenizer(input_str).input_ids\n",
    "    return batch\n",
    "\n",
    "with training_args.main_process_first(desc=\"dataset map pre-processing\"):\n",
    "    vectorized_datasets = raw_datasets.map(\n",
    "        prepare_dataset,\n",
    "        remove_columns=raw_datasets_features,\n",
    "    ).with_format(\"torch\")\n",
    "\n",
    "    if training_args.do_train and data_args.streaming:\n",
    "        # manually shuffle if streaming (done by the trainer for non-streaming)\n",
    "        vectorized_datasets[\"train\"] = vectorized_datasets[\"train\"].shuffle(\n",
    "            buffer_size=data_args.shuffle_buffer_size,\n",
    "            seed=training_args.seed,\n",
    "        )\n",
    "\n",
    "# filter training data that is shorter than min_input_length or longer than\n",
    "# max_input_length\n",
    "def is_audio_in_length_range(length):\n",
    "    return min_input_length < length < max_input_length\n",
    "\n",
    "if training_args.do_train:\n",
    "    vectorized_datasets[\"train\"] = vectorized_datasets[\"train\"].filter(\n",
    "        is_audio_in_length_range,\n",
    "        input_columns=[\"input_length\"],\n",
    "    )\n",
    "\n",
    "# 8. Load Metric\n",
    "metric = evaluate.load(\"wer\")\n",
    "do_normalize_eval = data_args.do_normalize_eval\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = tokenizer.batch_decode(pred.label_ids, skip_special_tokens=True)\n",
    "\n",
    "    if do_normalize_eval:\n",
    "        pred_str = [normalizer(pred) for pred in pred_str]\n",
    "        label_str = [normalizer(label) for label in label_str]\n",
    "        # filtering step to only evaluate the samples that correspond to non-zero references:\n",
    "        pred_str = [pred_str[i] for i in range(len(pred_str)) if len(label_str[i]) > 0]\n",
    "        label_str = [label_str[i] for i in range(len(label_str)) if len(label_str[i]) > 0]\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "# 9. Create a single speech processor\n",
    "if is_main_process(training_args.local_rank):\n",
    "    # save feature extractor, tokenizer and config\n",
    "    feature_extractor.save_pretrained(training_args.output_dir)\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    config.save_pretrained(training_args.output_dir)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(training_args.output_dir)\n",
    "\n",
    "# 10. Define data collator\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")\n",
    "\n",
    "# 11. Configure Trainer\n",
    "# Trainer callback to reinitialise and reshuffle the streamable datasets at the beginning of each epoch\n",
    "# Only required for streaming: Trainer automatically shuffles non-streaming datasets\n",
    "class ShuffleCallback(TrainerCallback):\n",
    "    def on_epoch_begin(self, args, state, control, train_dataloader, **kwargs):\n",
    "        if isinstance(train_dataloader.dataset, IterableDatasetShard):\n",
    "            pass  # set_epoch() is handled by the Trainer\n",
    "        elif isinstance(train_dataloader.dataset, IterableDataset):\n",
    "            train_dataloader.dataset.set_epoch(train_dataloader.dataset._epoch + 1)\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=vectorized_datasets[\"train\"] if training_args.do_train else None,\n",
    "    eval_dataset=vectorized_datasets[\"eval\"] if training_args.do_eval else None,\n",
    "    tokenizer=feature_extractor,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics if training_args.predict_with_generate else None,\n",
    "    callbacks=[ShuffleCallback()] if data_args.streaming else None,\n",
    ")\n",
    "\n",
    "# 12. Training\n",
    "if training_args.do_train:\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    elif last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    trainer.save_model()  # Saves the feature extractor too for easy upload\n",
    "\n",
    "    metrics = train_result.metrics\n",
    "    if data_args.max_train_samples:\n",
    "        metrics[\"train_samples\"] = data_args.max_train_samples\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "\n",
    "# 13. Evaluation\n",
    "results = {}\n",
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "    metrics = trainer.evaluate(\n",
    "        metric_key_prefix=\"eval\",\n",
    "        max_length=training_args.generation_max_length,\n",
    "        num_beams=training_args.generation_num_beams,\n",
    "    )\n",
    "    if data_args.max_eval_samples:\n",
    "        metrics[\"eval_samples\"] = data_args.max_eval_samples\n",
    "\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "# 14. Write Training Stats\n",
    "kwargs = {\n",
    "    \"finetuned_from\": model_args.model_name_or_path,\n",
    "    \"tasks\": \"automatic-speech-recognition\",\n",
    "    \"tags\": \"whisper-event\",\n",
    "}\n",
    "if data_args.dataset_name is not None:\n",
    "    kwargs[\"dataset_tags\"] = data_args.dataset_name\n",
    "    if data_args.dataset_config_name is not None:\n",
    "        kwargs[\"dataset\"] = f\"{data_args.dataset_name} {data_args.dataset_config_name}\"\n",
    "    else:\n",
    "        kwargs[\"dataset\"] = data_args.dataset_name\n",
    "    if \"common_voice\" in data_args.dataset_name:\n",
    "        kwargs[\"language\"] = data_args.dataset_config_name.split('-')[0]\n",
    "    if model_args.model_index_name is not None:\n",
    "        kwargs[\"model_name\"] = model_args.model_index_name\n",
    "\n",
    "if training_args.push_to_hub:\n",
    "    trainer.push_to_hub(**kwargs)\n",
    "else:\n",
    "    trainer.create_model_card(**kwargs)\n",
    "\n",
    "return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7e1110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c81aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a7db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, accelerator\n",
    "torch.cuda.empty_cache()\n",
    "accelerator.free_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b41da5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 23 01:33:06 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:1D:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    41W / 300W |      3MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "    print('Not connected to a GPU')\n",
    "else:\n",
    "    print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f469829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78a671ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e932dd11d64ab0955893c093c0ff94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad9bbbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import interleave_datasets, load_dataset\n",
    "\n",
    "def load_streaming_dataset(dataset_name, dataset_config_name, split, **kwargs):\n",
    "    if \"+\" in split:\n",
    "        # load multiple splits separated by the `+` symbol *with* streaming mode\n",
    "        dataset_splits = [load_dataset(dataset_name, dataset_config_name, split=split_name, streaming=True, **kwargs) for split_name in split.split(\"+\")]\n",
    "        # interleave multiple splits to form one dataset\n",
    "        interleaved_dataset = interleave_datasets(dataset_splits)\n",
    "        return interleaved_dataset\n",
    "    else:\n",
    "        # load a single split *with* streaming mode\n",
    "        dataset = load_dataset(dataset_name, dataset_config_name, split=split, streaming=True, **kwargs)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56644dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/gmenon/.local/lib/python3.8/site-packages/datasets/load.py:2066: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=True' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import IterableDatasetDict\n",
    "\n",
    "raw_datasets = IterableDatasetDict()\n",
    "\n",
    "raw_datasets[\"train\"] = load_streaming_dataset(\"mozilla-foundation/common_voice_11_0\", \"es\", split=\"train\", use_auth_token=True)  # set split=\"train+validation\" for low-resource\n",
    "raw_datasets[\"test\"] = load_streaming_dataset(\"mozilla-foundation/common_voice_11_0\", \"es\", split=\"test\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aef6658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Spanish\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78372561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'client_id': Value(dtype='string', id=None),\n",
       " 'path': Value(dtype='string', id=None),\n",
       " 'audio': Audio(sampling_rate=48000, mono=True, decode=True, id=None),\n",
       " 'sentence': Value(dtype='string', id=None),\n",
       " 'up_votes': Value(dtype='int64', id=None),\n",
       " 'down_votes': Value(dtype='int64', id=None),\n",
       " 'age': Value(dtype='string', id=None),\n",
       " 'gender': Value(dtype='string', id=None),\n",
       " 'accent': Value(dtype='string', id=None),\n",
       " 'locale': Value(dtype='string', id=None),\n",
       " 'segment': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3a60e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "raw_datasets = raw_datasets.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dc2abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "do_lower_case = False\n",
    "do_remove_punctuation = False\n",
    "\n",
    "normalizer = BasicTextNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a229c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and (possibly) resample audio datato 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    # compute input length of audio sample in seconds\n",
    "    batch[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
    "    \n",
    "    # optional pre-processing steps\n",
    "    transcription = batch[\"sentence\"]\n",
    "    if do_lower_case:\n",
    "        transcription = transcription.lower()\n",
    "    if do_remove_punctuation:\n",
    "        transcription = re.sub(punctuation_to_remove_regex, \" \", transcription).strip()\n",
    "    \n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = processor.tokenizer(transcription).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07960588",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_datasets = raw_datasets.map(prepare_dataset, \n",
    "                                       remove_columns=list(next(iter(raw_datasets.values())).features)).with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "919ae54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_datasets[\"train\"] = vectorized_datasets[\"train\"].shuffle(\n",
    "    buffer_size=500,\n",
    "    seed=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b119ecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 30.0\n",
    "\n",
    "def is_audio_in_length_range(length):\n",
    "    return length < max_input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a85443da",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_datasets[\"train\"] = vectorized_datasets[\"train\"].filter(\n",
    "    is_audio_in_length_range,\n",
    "    input_columns=[\"input_length\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "112a85a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8c4e8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b808c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe6db32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate with the 'normalised' WER\n",
    "do_normalize_eval = True\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    if do_normalize_eval:\n",
    "        pred_str = [normalizer(pred) for pred in pred_str]\n",
    "        label_str = [normalizer(label) for label in label_str]\n",
    "        # filtering step to only evaluate the samples that correspond to non-zero references:\n",
    "        pred_str = [pred_str[i] for i in range(len(pred_str)) if len(label_str[i]) > 0]\n",
    "        label_str = [label_str[i] for i in range(len(label_str)) if len(label_str[i]) > 0]\n",
    "    \n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\":wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6910d05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "688e4af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "341e9c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-es\",  # your repo name\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=5000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=2,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "   # report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34d1f1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "from transformers.trainer_pt_utils import IterableDatasetShard\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "# trainer callback to reinitialise and reshuffle the streamable datasets at the beginning of each epoch\n",
    "class ShuffleCallback(TrainerCallback):\n",
    "    def on_epoch_begin(self, args, state, control, train_dataloader, **kwargs):\n",
    "        if isinstance(train_dataloader.dataset, IterableDatasetShard):\n",
    "            pass  # set_epoch() is handled by the Trainer\n",
    "        elif isinstance(train_dataloader.dataset, IterableDataset):\n",
    "            train_dataloader.dataset.set_epoch(train_dataloader.dataset._epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ea82ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/gmenon/whisper-small-es into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=vectorized_datasets[\"train\"],\n",
    "    eval_dataset=vectorized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    "    callbacks=[ShuffleCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c457e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(training_args.output_dir)\n",
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83895f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/gmenon/.local/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Reading metadata...: 230467it [00:22, 10454.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1001' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1001/5000 06:10 < 24:44, 2.69 it/s, Epoch 0.20/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 15520it [00:01, 10865.34it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995c1ced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda125ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaaa66b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcc5bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
