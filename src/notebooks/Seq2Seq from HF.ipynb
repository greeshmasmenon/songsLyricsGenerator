{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59871922-1ecb-4a60-a7eb-5daf4184f569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from evaluate) (2.14.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from evaluate) (1.23.5)\n",
      "Requirement already satisfied: dill in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from evaluate) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from evaluate) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from evaluate) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from evaluate) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from evaluate) (0.16.4)\n",
      "Requirement already satisfied: packaging in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from evaluate) (23.1)\n",
      "Collecting responses<0.19 (from evaluate)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate) (12.0.1)\n",
      "Requirement already satisfied: aiohttp in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: filelock in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from requests>=2.19.0->evaluate) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Installing collected packages: responses, evaluate\n",
      "Successfully installed evaluate-0.4.0 responses-0.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b2d127-ce66-4334-bbb1-32b6149231b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import torch\n",
    "from datasets import DatasetDict, load_dataset, Dataset, Audio\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoFeatureExtractor,\n",
    "    AutoModelForSpeechSeq2Seq,\n",
    "    AutoProcessor,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6e02b5b-9b86-4875-985c-664bf3f7425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor ([`WhisperProcessor`])\n",
    "            The processor used for processing the data.\n",
    "        decoder_start_token_id (`int`)\n",
    "            The begin-of-sentence of the decoder.\n",
    "        forward_attention_mask (`bool`)\n",
    "            Whether to return attention_mask.\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "    forward_attention_mask: bool\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        model_input_name = self.processor.model_input_names[0]\n",
    "        input_features = [{model_input_name: feature[model_input_name]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        if self.forward_attention_mask:\n",
    "            batch[\"attention_mask\"] = torch.LongTensor([feature[\"attention_mask\"] for feature in features])\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ec39e6e-6157-4997-b29a-5c87c8712314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Parse input arguments\n",
    "# # See all possible arguments in src/transformers/training_args.py\n",
    "# # or by passing the --help flag to this script.\n",
    "# # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
    "# parser = HfArgumentParser((ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments))\n",
    "\n",
    "# if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
    "#     # If we pass only one argument to the script and it's the path to a json file,\n",
    "#     # let's parse it to get our arguments.\n",
    "#     model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
    "# else:\n",
    "#     model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "# if model_args.use_auth_token is not None:\n",
    "#     warnings.warn(\"The `use_auth_token` argument is deprecated and will be removed in v4.34.\", FutureWarning)\n",
    "#     if model_args.token is not None:\n",
    "#         raise ValueError(\"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\")\n",
    "#     model_args.token = model_args.use_auth_token\n",
    "\n",
    "# # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n",
    "# # information sent is the one passed as arguments along with your Python/PyTorch versions.\n",
    "# send_example_telemetry(\"run_speech_recognition_seq2seq\", model_args, data_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4498e839-2573-4d37-8ed3-1b00fa17194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/scratch/users/gmenon/hf_seq2seq\"\n",
    "# 3. Detecting last checkpoint and eventually continue from last checkpoint\n",
    "last_checkpoint = None\n",
    "last_checkpoint = get_last_checkpoint(output_dir)\n",
    "# Set seed before initializing model.\n",
    "set_seed(123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "675f3c7c-0cb5-4b0e-a79b-33ddb6de3ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE_PATH = \"/scratch/users/gmenon/train_song_metadata_en_demucs_cleaned_filtered_095.csv\"\n",
    "TEST_FILE_PATH = \"/scratch/users/gmenon/validation_song_metadata_en_demucs_cleaned_filtered_005.csv\"\n",
    "train_df = pd.read_csv(TRAIN_FILE_PATH)\n",
    "validation_df = pd.read_csv(TEST_FILE_PATH)\n",
    "\n",
    "train_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(train_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(train_df[\"transcription\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "val_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(validation_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(validation_df[\"transcription\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72889895-70b2-4636-a51a-96a98f4cac49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'transcription'],\n",
       "        num_rows: 9538\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['audio', 'transcription'],\n",
       "        num_rows: 507\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 4. Load dataset\n",
    "audio_dataset = DatasetDict()\n",
    "audio_dataset[\"train\"] = train_dataset\n",
    "audio_dataset[\"eval\"] = val_dataset\n",
    "\n",
    "audio_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c32b74dc-7acc-4b5b-b902-e74d3c4b3794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f73525b31b46848a4c0e7d6f07bc89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/795 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7751c20c87ee4f548e31a75ddaef23b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3381eff1b634626b39d979ef6b7a36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4aa72b432ce41e8b0323152cfff7d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/795 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf7d18cbf7d49cea61fd47390997e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4680b4925544e587dcb7078d667f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/795 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec5437133e244cdad503e288bc9758e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97929aa22d3146189d131bc6c2f146bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/795 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a545ac84b39144b799a3d39d3fc3838d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d1eb481e394d8aa1cd40a77f393d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/795 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec747699f58b46c19343013d0f30832f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06bdee5d61cb4ab3b427e268afdcfef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/795 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94590f762b14aa0b65d05237cdcb2a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "accb788acdee467b9691f879c4b3cd21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/795 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5163c23c6e6e4c5982b00d6aaa10ef2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b93e14408ee496fa2f58a335f4102c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/795 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f92d0b43834b55af5947fc4c84a110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a249a557a4f54791a6da32e7492d0dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/795 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a23ab5974247309e2dfca94c0f521e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d49713b985b4845a3987a9be0e3b33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/795 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb7ff22d5bb45158776d33379dd9381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d728b4fd059248008b8dfd4eea264ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/794 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d24601bea44338b754ff657a767316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a27a3d968bf4a3388235d3ebf2e0f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/794 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb827671c234fcba8701f6fbf63cdf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1645fb1255eb4b17b184292bf3517cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5fdf94f52d8449098a262dcdea45e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/507 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c728125f5c4c46ac26527076e9fa10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b649c88e80455797d0da153db3bbf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03af3bf9787491fa3162f177f1cf391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44bba843bf434d3a9bb9a8830924b62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/631 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#audio_dataset.push_to_hub(\"gmenon/slt-lyrics-audio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a641c4ea-9ad8-4ccc-929a-c39bcfa2ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_BACKBONE = \"\" #FILL IN\n",
    "FEATURE_EXTRACTOR_NAME=\"\" #FILL IN\n",
    "TOKENIZER_NAME=\"\" #FILL IN\n",
    "model_type_whiser = False #FILL IN\n",
    "decoder_start_token_id=101 #FILL IN\n",
    "freeze_feature_encoder=True #FILL IN\n",
    "freeze_encoder=True  #FILL IN\n",
    "\n",
    "# 5. Load pretrained model, tokenizer, and feature extractor\n",
    "#\n",
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "config = AutoConfig.from_pretrained(MODEL_BACKBONE)\n",
    "config.update({\"forced_decoder_ids\": None, \"suppress_tokens\": None})\n",
    "\n",
    "# SpecAugment for whisper models\n",
    "if model_type_whiser:\n",
    "    config.update({\"apply_spec_augment\": True})\n",
    "else:\n",
    "    config.update({\"apply_spec_augment\": False})\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(FEATURE_EXTRACTOR_NAME) #See if some other name is required\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME) #See if some other name is required\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(MODEL_BACKBONE,config=config)\n",
    "\n",
    "if model.config.decoder_start_token_id is None:\n",
    "    raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
    "\n",
    "if freeze_feature_encoder:\n",
    "    model.freeze_feature_encoder()\n",
    "\n",
    "if freeze_encoder:\n",
    "    model.freeze_encoder()\n",
    "    model.model.encoder.gradient_checkpointing = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0f3917-8bf5-4e91-b850-9d365cda5e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. Preprocessing the datasets.\n",
    "NUM_WORKERS = 4\n",
    "do_lower_case = True\n",
    "\n",
    "\n",
    "# We need to read the audio files as arrays and tokenize the targets.\n",
    "#max_input_length = data_args.max_duration_in_seconds * feature_extractor.sampling_rate\n",
    "#min_input_length = data_args.min_duration_in_seconds * feature_extractor.sampling_rate\n",
    "#audio_column_name = data_args.audio_column_name\n",
    "#num_workers = data_args.preprocessing_num_workers\n",
    "#text_column_name = data_args.text_column_name\n",
    "\n",
    "model_input_name = feature_extractor.model_input_names[0]\n",
    "\n",
    "# if SpecAugment is used for whisper models, return attention_mask to guide the mask along time axis\n",
    "forward_attention_mask = (\n",
    "    getattr(config, \"model_type\", None) == \"whisper\"\n",
    "    and getattr(config, \"apply_spec_augment\", False)\n",
    "    and getattr(config, \"mask_time_prob\", 0) > 0\n",
    ")\n",
    "\n",
    "\n",
    "#audio_datasets[\"train\"] = audio_datasets[\"train\"].select(range(data_args.max_train_samples))\n",
    "\n",
    "#raw_datasets[\"eval\"] = raw_datasets[\"eval\"].select(range(data_args.max_eval_samples))\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # process audio\n",
    "    sample = batch[audio_column_name]\n",
    "    inputs = feature_extractor(\n",
    "        sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_attention_mask=forward_attention_mask\n",
    "    )\n",
    "    # process audio length\n",
    "    batch[model_input_name] = inputs.get(model_input_name)[0]\n",
    "    batch[\"input_length\"] = len(sample[\"array\"])\n",
    "    if forward_attention_mask:\n",
    "        batch[\"attention_mask\"] = inputs.get(\"attention_mask\")[0]\n",
    "\n",
    "    # process targets\n",
    "    input_str = batch[text_column_name].lower() if do_lower_case else batch[text_column_name]\n",
    "    batch[\"labels\"] = tokenizer(input_str).input_ids\n",
    "    return batch\n",
    "\n",
    "#with training_args.main_process_first(desc=\"dataset map pre-processing\"):\n",
    "vectorized_datasets = raw_datasets.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=next(iter(raw_datasets.values())).column_names,\n",
    "    num_proc=data_args.preprocessing_num_workers,\n",
    "    desc=\"preprocess train dataset\"\n",
    ")\n",
    "\n",
    "# filter data that is shorter than min_input_length or longer than\n",
    "# max_input_length\n",
    "def is_audio_in_length_range(length):\n",
    "    return length > min_input_length and length < max_input_length\n",
    "\n",
    "vectorized_datasets = vectorized_datasets.filter(\n",
    "    is_audio_in_length_range,\n",
    "    num_proc=num_workers,\n",
    "    input_columns=[\"input_length\"],\n",
    ")\n",
    "\n",
    "# for large datasets it is advised to run the preprocessing on a\n",
    "# single machine first with `args.preprocessing_only` since there will mostly likely\n",
    "# be a timeout when running the script in distributed mode.\n",
    "# In a second step `args.preprocessing_only` can then be set to `False` to load the\n",
    "# cached dataset\n",
    "#if data_args.preprocessing_only:\n",
    "cache = {k: v.cache_files for k, v in vectorized_datasets.items()}\n",
    "logger.info(f\"Data preprocessing finished. Files cached at {cache}.\")\n",
    "return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9fd7eb-191f-44e6-8c9c-21ae326162ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8. Load Metric\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = tokenizer.batch_decode(pred.label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b739500-22d2-4aae-8267-bcd6be3a9373",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 9. Create a single speech processor\n",
    "# make sure all processes wait until data is saved\n",
    "with training_args.main_process_first():\n",
    "    # only the main process saves them\n",
    "        # save feature extractor, tokenizer and config\n",
    "    feature_extractor.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    config.save_pretrained(output_dir)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(output_dir)\n",
    "\n",
    "# 10. Define data collator\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    "    forward_attention_mask=forward_attention_mask,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00dae2c-7c1e-428e-88cb-3357f5e46d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 11. Initialize Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=vectorized_datasets[\"train\"] if training_args.do_train else None,\n",
    "    eval_dataset=vectorized_datasets[\"eval\"] if training_args.do_eval else None,\n",
    "    tokenizer=feature_extractor,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics if training_args.predict_with_generate else None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbed9dfb-4336-4b3e-8d63-9527593f5650",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 12. Training\n",
    "checkpoint = None\n",
    "if training_args.resume_from_checkpoint is not None:\n",
    "    checkpoint = training_args.resume_from_checkpoint\n",
    "elif last_checkpoint is not None:\n",
    "    checkpoint = last_checkpoint\n",
    "train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "trainer.save_model()  # Saves the feature extractor too for easy upload\n",
    "metrics = train_result.metrics\n",
    "max_train_samples = (\n",
    "    data_args.max_train_samples\n",
    "    if data_args.max_train_samples is not None\n",
    "    else len(vectorized_datasets[\"train\"])\n",
    ")\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(vectorized_datasets[\"train\"]))\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3490a9b-7263-4845-9f70-9c5daef18634",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BEAMS = 5\n",
    "# 13. Evaluation\n",
    "results = {}\n",
    "logger.info(\"*** Evaluate ***\")\n",
    "metrics = trainer.evaluate(\n",
    "    metric_key_prefix=\"eval\",\n",
    "    #max_length=training_args.generation_max_length,\n",
    "    num_beams=NUM_BEAMS\n",
    ")\n",
    "max_eval_samples = len(vectorized_datasets[\"eval\"])\n",
    "metrics[\"eval_samples\"] = max_eval_samples\n",
    "\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a64ca2-210e-4b54-a23c-f26237fb41b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 14. Write Training Stats\n",
    "kwargs = {\"finetuned_from\": model_args.model_name_or_path, \"tasks\": \"automatic-speech-recognition\"}\n",
    "    kwargs[\"dataset_tags\"] = data_args.dataset_name\n",
    "    if data_args.dataset_config_name is not None:\n",
    "        kwargs[\"dataset_args\"] = data_args.dataset_config_name\n",
    "        kwargs[\"dataset\"] = f\"{data_args.dataset_name} {data_args.dataset_config_name}\"\n",
    "    else:\n",
    "        kwargs[\"dataset\"] = data_args.dataset_name\n",
    "\n",
    "if training_args.push_to_hub:\n",
    "    trainer.push_to_hub(**kwargs)\n",
    "else:\n",
    "    trainer.create_model_card(**kwargs)\n",
    "\n",
    "return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
