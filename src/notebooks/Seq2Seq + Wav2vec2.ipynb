{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46358ffa-ac47-48c6-917e-54d7918483d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os. chdir('/home/users/gmenon/workspace/songsLyricsGenerator/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3852a9e9-9f11-40e7-be68-d372a242db5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgreeshmasmenon\u001b[0m (\u001b[33msongslyricstranscription\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230821_041340-g1mt1fdt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20transfer%20learning/runs/g1mt1fdt' target=\"_blank\">ruby-valley-52</a></strong> to <a href='https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20transfer%20learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20transfer%20learning' target=\"_blank\">https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20transfer%20learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20transfer%20learning/runs/g1mt1fdt' target=\"_blank\">https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20transfer%20learning/runs/g1mt1fdt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TRAIN_FILE_PATH\": \"/scratch/users/gmenon/train_song_metadata_en_demucs_cleaned_filtered_095.csv\",\n",
      "    \"TEST_FILE_PATH\": \"/scratch/users/gmenon/validation_song_metadata_en_demucs_cleaned_filtered_005.csv\",\n",
      "    \"MODEL_BACKBONE\": \"facebook/wav2vec2-large-960h-lv60-self\",\n",
      "    \"BATCH_SIZE\": 1,\n",
      "    \"NUM_EPOCHS\": 15,\n",
      "    \"MODEL_SAVE_PATH\": \"/scratch/users/gmenon//model_artefacts/wav2vec2_demucs_en_large-960h-lv60-self_freeze_unfreeze_15epochs_adamw.pt\",\n",
      "    \"FINETUNE_STRATEGY\": [\n",
      "        \"freeze_unfreeze\",\n",
      "        10\n",
      "    ],\n",
      "    \"LR_SCHEDULER\": \"reduce_on_plateau_schedule\"\n",
      "}\n",
      "Namespace(batch_size=2, learning_rate=1e-06, lm_model='bert-base-uncased', vocab_size=15000, wav2vec2_model='facebook/wav2vec2-large-960h-lv60-self')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-large-960h-lv60-self\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_attn_dim\": null,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"mean\",\n",
      "  \"ctc_zero_infinity\": true,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "CPU Count = 4\n",
      "In Stage = Fit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8424396173b94ae4b43bb62717455afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4e5d7403e442be9f4bfe5f1c8ac6c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type            | Params\n",
      "---------------------------------------------\n",
      "0 | wav2vec2 | Wav2Vec2ForCTC  | 315 M \n",
      "1 | seq2seq  | BertLMHeadModel | 109 M \n",
      "---------------------------------------------\n",
      "420 M     Trainable params\n",
      "4.2 M     Non-trainable params\n",
      "424 M     Total params\n",
      "1,699.943 Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f82a30043ec460e9f86b2ed99d1e7e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entering val data loader\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2, -1]' is invalid for input of size 7",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Wav2Vec2ForCTC, Wav2Vec2Model\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmir_constants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingArgs, WAV2VEC2_ARGS\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lyrics_finetune\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataclass, asdict\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/src/training/lyrics_finetune.py:311\u001b[0m\n\u001b[1;32m    308\u001b[0m hparams\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-6\u001b[39m\n\u001b[1;32m    309\u001b[0m hparams\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 311\u001b[0m model,trainer \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/src/training/lyrics_finetune.py:300\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(hparams)\u001b[0m\n\u001b[1;32m    298\u001b[0m model \u001b[38;5;241m=\u001b[39m Wav2SeqModel(hparams)\n\u001b[1;32m    299\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger\u001b[38;5;241m=\u001b[39mwandb_logger)\n\u001b[0;32m--> 300\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mSpeechRecognitionDataModule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWAV2VEC2_ARGS\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, trainer\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 608\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[38;5;241m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    643\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    646\u001b[0m     ckpt_path,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    648\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    649\u001b[0m )\n\u001b[0;32m--> 650\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[0;32m-> 1112\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1191\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1204\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_training_routine()\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;66;03m# enable train mode\u001b[39;00m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1276\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m-> 1276\u001b[0m     \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:152\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_dataloaders \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    151\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataloader_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataloader_idx\n\u001b[0;32m--> 152\u001b[0m dl_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_max_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs\u001b[38;5;241m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:137\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_started()\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# lightning module methods\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:234\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The evaluation step (validation_step or test_step depending on the trainer's state).\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m    the outputs of the step\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    233\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 234\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1494\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1494\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mval_step_context():\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, ValidationStep)\n\u001b[0;32m--> 390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/src/training/lyrics_finetune.py:249\u001b[0m, in \u001b[0;36mWav2SeqModel.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    247\u001b[0m label_attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_attention_masks\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m#print(f\"audio={audio}, attention_mask = {attention_mask}, label_attention_mask = {label_attention_mask}\")\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(audio,attention_mask,labels,label_attention_mask)\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m    251\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq2seq\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[2, -1]' is invalid for input of size 7"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_lightning import LightningModule\n",
    "import pytorch_lightning as pl\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Model\n",
    "from constants.mir_constants import TrainingArgs, WAV2VEC2_ARGS\n",
    "from training import lyrics_finetune\n",
    "from dataclasses import dataclass, asdict\n",
    "import json\n",
    "import argparse\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23a1124d-49d5-4576-b8f2-f775c7239d84",
   "metadata": {},
   "source": [
    "from flash.audio import SpeechRecognitionData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b62468-9929-43e3-8aeb-7daaeb74b47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(asdict(WAV2VEC2_ARGS), indent = 4))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "690c412e-6b90-431e-98d9-1dfac1e94f98",
   "metadata": {},
   "source": [
    "datamodule = SpeechRecognitionData.from_csv(\"consolidated_file_path\",\n",
    "                                                         \"transcription_capitalized\",\n",
    "                                                         train_file=WAV2VEC2_ARGS.TRAIN_FILE_PATH,\n",
    "                                                         test_file=WAV2VEC2_ARGS.TEST_FILE_PATH,\n",
    "                                                         batch_size=WAV2VEC2_ARGS.BATCH_SIZE\n",
    "                                                         )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0809f82-32b0-4995-bdb0-08779a8c0211",
   "metadata": {},
   "source": [
    "datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831d6096-d0e4-4d39-8828-57e3a4a208cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flash\n",
    "import torchaudio\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# class SpeechDataModule(pl.LightningDataModule):\n",
    "#     def __init__(self, args: WAV2VEC2_ARGS):\n",
    "#         super().__init__()\n",
    "#         self.batch_size = 2\n",
    "\n",
    "#     def prepare_data(self):\n",
    "#         # Download and extract the audio files and transcripts, if necessary.\n",
    "#         pass\n",
    "\n",
    "#     def setup(self, stage=None):\n",
    "#         # Create the dataset\n",
    "#         # datamodule = \n",
    "#         self.dataset = SpeechRecognitionData.from_csv(\"consolidated_file_path\",\n",
    "#                                                          \"transcription_capitalized\",\n",
    "#                                                          train_file=WAV2VEC2_ARGS.TRAIN_FILE_PATH,\n",
    "#                                                          test_file=WAV2VEC2_ARGS.TEST_FILE_PATH,\n",
    "#                                                          batch_size=2\n",
    "#                                                          )\n",
    "\n",
    "#         # Create the dataloader\n",
    "#         self.train_dataloader = self.dataset.train_dataloader\n",
    "#         # torch.utils.data.DataLoader(\n",
    "#         #     self.dataset,\n",
    "#         #     batch_size=self.batch_size,\n",
    "#         #     shuffle=True,\n",
    "#         #     num_workers=4,\n",
    "#         # )\n",
    "#         self.val_dataloader = self.dataset.val_dataloader\n",
    "#         # torch.utils.data.DataLoader(\n",
    "#         #     self.dataset,\n",
    "#         #     batch_size=self.batch_size,\n",
    "#         #     shuffle=False,\n",
    "#         #     num_workers=4,\n",
    "#         # )\n",
    "\n",
    "#     def train_dataloader(self):\n",
    "#         return self.train_dataloader\n",
    "\n",
    "#     def val_dataloader(self):\n",
    "#         return self.val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3354031-3fb6-46ee-9fed-c3c06d23348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DALIDataset(pl.LightningDataModule):\n",
    "#     def __init__(self, batch_size: int = 4,\n",
    "#                   train_path :Optional[str] = None,\n",
    "#                     validation_path: Optional[str] = None,\n",
    "#                       model_backbone: pl.LightningModule = None,\n",
    "#                       args: TrainingArgs = WAV2VEC2_ARGS\n",
    "#                       ):\n",
    "        \n",
    "#         super().__init__()\n",
    "#         self.train_path = train_path if train_path is not None else args.TRAIN_FILE_PATH\n",
    "#         self.validation_path = validation_path if validation_path is not None else args.TEST_FILE_PATH\n",
    "#         self.model_backbone = model_backbone if model_backbone is not None else args.MODEL_BACKBONE\n",
    "\n",
    "#         def prepare_data(self):\n",
    "#             pass\n",
    "        \n",
    "#         def setup(self):\n",
    "#             train_df = pd.read_csv(WAV2VEC2_ARGS.TRAIN_FILE_PATH) \n",
    "#             validation_df = pd.read_csv(WAV2VEC2_ARGS.TEST_FILE_PATH)\n",
    "#             songs_metadata = pd.concat([train_df,validation_df], ignore_index = True)\n",
    "#             audio_dataset = Dataset.from_dict(\n",
    "#                 {\"audio\": list(songs_metadata[\"file_name\"]),\n",
    "#                  \"transcription\": list(songs_metadata[\"transcription\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "#             audio_dataset[\"transcription\"] = audio_dataset[\"transcription\"] = re.sub(WAV2VEC2_ARGS.CHARS_TO_REMOVE_FROM_TRANSCRIPTS, '', audio_dataset[\"transcription\"]).upper()\n",
    "#             audio_dataset = audio_dataset.train_test_split(test_size=0.2, shuffle=True)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a239b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n",
    "from datasets import load_dataset,Dataset,Audio\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# class SpeechRecognitionDataset(Dataset):\n",
    "#     def __init__(self, dataset,args:TrainingArgs,processor):\n",
    "#         self.model_backbone = args.MODEL_BACKBONE\n",
    "#         self.dataset = dataset\n",
    "#         self.processor = processor\n",
    "#         self.tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(self.model_backbone)\n",
    "#         self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(self.model_backbone)\n",
    "#         self.processor = Wav2Vec2Processor(feature_extractor=self.feature_extractor, tokenizer=self.tokenizer)\n",
    "        \n",
    "#     def __getitem__(self, index):\n",
    "#         audio = self.dataset[index][\"audio\"][0]\n",
    "#         transcription = self.dataset[index][\"transcription\"][0]\n",
    "\n",
    "#         input_values = self.processor(audio[\"array\"], sampling_rate=16_000).input_values[0]\n",
    "#         with self.processor.as_target_processor():\n",
    "#             print(\"Entering the label encoder\")\n",
    "#             labels = self.processor(transcription,return_tensors = 'pt').input_ids\n",
    "#        # attention_mask = inputs.attention_mask.squeeze()\n",
    "#         return input_values, labels\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8e8b187-08b6-4e27-b3ec-7e1da0ad5ead",
   "metadata": {},
   "source": [
    "class SpeechRecognitionDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, WAV2VEC2_ARGS: WAV2VEC2_ARGS, num_workers):\n",
    "        super().__init__()\n",
    "        self.batch_size = WAV2VEC2_ARGS.BATCH_SIZE\n",
    "        self.num_workers = num_workers\n",
    "        self.tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(WAV2VEC2_ARGS.MODEL_BACKBONE)\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(WAV2VEC2_ARGS.MODEL_BACKBONE)\n",
    "        self.processor = Wav2Vec2Processor(feature_extractor=self.feature_extractor, tokenizer=self.tokenizer)\n",
    "        self.cpu_count = 1\n",
    "        print(f\"CPU Count = {self.cpu_count}\")\n",
    "        self.padding: Union[bool, str] = True\n",
    "        self.max_length: Optional[int] = None\n",
    "        self.max_length_labels: Optional[int] = None\n",
    "        self.pad_to_multiple_of: Optional[int] = None\n",
    "        self.pad_to_multiple_of_labels: Optional[int] = None\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        train_df = pd.read_csv(WAV2VEC2_ARGS.TRAIN_FILE_PATH).head(10)\n",
    "        validation_df = pd.read_csv(WAV2VEC2_ARGS.TEST_FILE_PATH).head(10)\n",
    "        #songs_metadata = pd.concat([train_df,validation_df], ignore_index = True)\n",
    "        if stage == 'fit' or stage is None:\n",
    "            print(\"In Stage = Fit\")\n",
    "            train_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(train_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(train_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "            \n",
    "            val_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(validation_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(validation_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "            \n",
    "            train_dataset = train_dataset.map(self.transform_huggingface_dataset, num_proc = self.cpu_count)\n",
    "            train_dataset =train_dataset.remove_columns(['audio','transcription','input_length'])\n",
    "            self.train_dataset = train_dataset.map(self.data_collate, num_proc = self.cpu_count)\n",
    "\n",
    "            \n",
    "            val_dataset = val_dataset.map(self.transform_huggingface_dataset, num_proc = self.cpu_count,)\n",
    "            val_dataset =val_dataset.remove_columns(['audio','transcription','input_length'])\n",
    "            self.val_dataset = val_dataset.map(self.data_collate, num_proc = self.cpu_count)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            print(\"In Stage = Test\")\n",
    "            val_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(validation_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(validation_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "          #  val_dataset[\"transcription\"] = val_dataset[\"transcription\"] = re.sub(WAV2VEC2_ARGS.CHARS_TO_REMOVE_FROM_TRANSCRIPTS, '', val_dataset[\"transcription\"]).upper()\n",
    "            val_dataset = val_dataset.map(self.transform_huggingface_dataset, num_proc = self.cpu_count)\n",
    "            #self.test_dataset = SpeechRecognitionDataset(val_dataset,WAV2VEC2_ARGS,self.processor)\n",
    "            self.test_dataset = val_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        print(\"entering train data loader\")\n",
    "        return DataLoader(\n",
    "            self.train_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        print(\"entering val data loader\")\n",
    "        return DataLoader(\n",
    "            self.val_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        print(\"entering test data loader\")\n",
    "        return DataLoader(\n",
    "            self.test_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "    \n",
    "    def transform_huggingface_dataset(self, batch: Dataset) -> Dataset:\n",
    "        audio = batch[\"audio\"]\n",
    "        batch[\"input_values\"] = self.processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "        batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "        with self.processor.as_target_processor():\n",
    "            batch[\"labels\"] = self.processor(batch[\"transcription\"]).input_ids\n",
    "        return batch\n",
    "\n",
    "    def data_collate(self, batch: Dataset) :\n",
    "        input_features = [{\"input_values\": batch[\"input_values\"]}]\n",
    "        label_features = [{\"input_ids\": batch[\"labels\"]}]\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )      \n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ed33833-367d-48be-a594-389fa4aef1d8",
   "metadata": {},
   "source": [
    "data = SpeechRecognitionDataModule(WAV2VEC2_ARGS, num_workers = 0)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d7bd241-6c23-48c0-a557-fd3cd663add3",
   "metadata": {},
   "source": [
    "data.setup()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "64526ed0-9e12-4065-b4d3-4e484d573e92",
   "metadata": {},
   "source": [
    "next(iter(data.val_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bc4a02-0e0b-4462-bb7b-79e70cbec785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96306ffe-8df9-4e84-a994-d7d92f614a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Union, List, Dict\n",
    "# import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "# from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "\n",
    "# class DataCollatorCTCWithPadding:\n",
    "#     \"\"\"\n",
    "#     Data collator that will dynamically pad the inputs received.\n",
    "#     Args:\n",
    "#         processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "#             The processor used for proccessing the data.\n",
    "#         padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "#             Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "#             among:\n",
    "#             * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "#               sequence if provided).\n",
    "#             * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "#               maximum acceptable input length for the model if that argument is not provided.\n",
    "#             * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "#               different lengths).\n",
    "#         max_length (:obj:`int`, `optional`):\n",
    "#             Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "#         max_length_labels (:obj:`int`, `optional`):\n",
    "#             Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "#         pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "#             If set will pad the sequence to a multiple of the provided value.\n",
    "#             This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "#             7.5 (Volta).\n",
    "#     \"\"\"\n",
    "\n",
    "#     processor: Wav2Vec2Processor\n",
    "#     padding: Union[bool, str] = True\n",
    "#     max_length: Optional[int] = None\n",
    "#     max_length_labels: Optional[int] = None\n",
    "#     pad_to_multiple_of: Optional[int] = None\n",
    "#     pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "#     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "#         # split inputs and labels since they have to be of different lengths and need\n",
    "#         # different padding methods\n",
    "#         input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "#         label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "#         batch = self.processor.pad(\n",
    "#             input_features,\n",
    "#             padding=self.padding,\n",
    "#             max_length=self.max_length,\n",
    "#             pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "#             return_tensors=\"pt\",\n",
    "#         )\n",
    "#         with self.processor.as_target_processor():\n",
    "#             labels_batch = self.processor.pad(\n",
    "#                 label_features,\n",
    "#                 padding=self.padding,\n",
    "#                 max_length=self.max_length_labels,\n",
    "#                 pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "#                 return_tensors=\"pt\",\n",
    "#             )\n",
    "\n",
    "#         # replace padding with -100 to ignore loss correctly\n",
    "#         labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "#         batch[\"labels\"] = labels\n",
    "\n",
    "#         return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cdad31-2268-4085-a6d4-6f86eb7558c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer, BertModel\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "# text = \"Replace me by any text you'd like.\"\n",
    "# encoded_input = tokenizer(text, return_tensors='pt')\n",
    "# output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c217f6c-0f28-48c6-92b0-0ca5755253b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Dict, List\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        label_attention_features =[{\"input_ids\": feature[\"label_attention_mask\"]} for feature in features]\n",
    "        attention_features =[{\"input_values\": feature[\"attention_mask\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\")\n",
    "\n",
    "        batch_attention = self.processor.pad(\n",
    "            attention_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\")\n",
    "    \n",
    "            \n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        with self.processor.as_target_processor():\n",
    "            label_attention_batch = self.processor.pad(\n",
    "                label_attention_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "                \n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels_attention = label_attention_batch[\"input_ids\"].masked_fill(labels_batch.input_ids.eq(101), 0).masked_fill(labels_batch.input_ids.eq(102), 0)\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        \n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        batch[\"label_attention_masks\"] = labels_attention\n",
    "        batch[\"attention_mask\"] = batch_attention[\"input_values\"]\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5405a62-6cc2-4291-b105-4f2f9c7aaa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor,BertTokenizer\n",
    "from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC, AutoModel,AutoProcessor\n",
    "\n",
    "from datasets import load_dataset,Dataset,Audio\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "WAV2VEC2_ARGS.BATCH_SIZE = 2\n",
    "class SpeechRecognitionDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, WAV2VEC2_ARGS: WAV2VEC2_ARGS, num_workers):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.batch_size = WAV2VEC2_ARGS.BATCH_SIZE\n",
    "        self.num_workers = num_workers\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(hparams.wav2vec2_model)\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        #self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(WAV2VEC2_ARGS.MODEL_BACKBONE)\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "        #self.feature_extractor = AutoFeatureExtractor.from_pretrained(hparams.wav2vec2_model)\n",
    "        self.processor = Wav2Vec2Processor(feature_extractor=self.feature_extractor, tokenizer=self.bert_tokenizer)\n",
    "        #self.processor = AutoProcessor(feature_extractor=self.feature_extractor, tokenizer=self.bert_tokenizer)\n",
    "        #self.processor = WhisperProcessor(feature_extractor=self.feature_extractor, tokenizer=self.bert_tokenizer)\n",
    "        self.data_collator = DataCollatorCTCWithPadding(processor=self.processor, padding=True)\n",
    "        self.cpu_count = 4\n",
    "        print(f\"CPU Count = {self.cpu_count}\")\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        train_df = pd.read_csv(WAV2VEC2_ARGS.TRAIN_FILE_PATH).head(10)\n",
    "        validation_df = pd.read_csv(WAV2VEC2_ARGS.TEST_FILE_PATH).head(10)\n",
    "        if stage == 'fit' or stage is None:\n",
    "            print(\"In Stage = Fit\")\n",
    "            train_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(train_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(train_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "            self.train_dataset = train_dataset.map(self.prepare_dataset,remove_columns = train_dataset.column_names)\n",
    "            \n",
    "            val_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(validation_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(validation_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "            self.val_dataset = val_dataset.map(self.prepare_dataset,remove_columns = val_dataset.column_names)\n",
    "\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            print(\"In Stage = Test\")\n",
    "            test_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(validation_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(validation_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "            test_dataset = val_dataset.map(self.prepare_dataset,remove_columns = val_dataset.column_names)\n",
    "            self.test_dataset = test_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        print(\"entering train data loader\")\n",
    "        return DataLoader(\n",
    "            self.train_dataset.with_format(\"torch\"), \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn = self.data_collator\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        print(\"entering val data loader\")\n",
    "        return DataLoader(\n",
    "            self.val_dataset.with_format(\"torch\"), \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn = self.data_collator\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        print(\"entering test data loader\")\n",
    "        return DataLoader(\n",
    "            self.test_dataset.with_format(\"torch\"), \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn = self.data_collator\n",
    "        )\n",
    "\n",
    "    def prepare_dataset(self,batch):\n",
    "        audio = batch[\"audio\"]\n",
    "        # batched output is \"un-batched\" to ensure mapping is correct\n",
    "        batch[\"input_values\"] = self.processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "        batch[\"attention_mask\"] = self.processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).attention_mask[0]\n",
    "        batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "        with self.processor.as_target_processor():\n",
    "            batch[\"labels\"] = self.processor(batch[\"transcription\"]).input_ids\n",
    "            batch[\"label_attention_mask\"] = self.processor(batch[\"transcription\"]).attention_mask\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "raw",
   "id": "75fe7b63-806e-46e7-8007-6223196529a5",
   "metadata": {},
   "source": [
    "data = SpeechRecognitionDataModule(WAV2VEC2_ARGS, num_workers = 0)\n",
    "data.setup()\n",
    "data.val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44155934-62d0-4943-afb7-9891bbd11a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.val_dataset[0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d90ec0e4-4a82-4f5b-a545-14a2be55dd00",
   "metadata": {},
   "source": [
    "print(next(iter(data.val_dataloader())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c3d9597-6db9-48ff-b8db-4fe73bef6ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Model, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC, AutoModel\n",
    "\n",
    "class Wav2SeqModel(LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.batch_size = hparams.batch_size\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.wav2vec2 = AutoModelForCTC.from_pretrained(hparams.wav2vec2_model,ctc_zero_infinity=True,ctc_loss_reduction=\"mean\")\n",
    "        self.wav2vec2.eval()\n",
    "        #self.wav2vec2 = AutoModel.from_pretrained(hparams.wav2vec2_model)\n",
    "        #self.wav2vec2.eval()\n",
    "        print(self.wav2vec2.config)\n",
    "        self.seq2seq = AutoModelForCausalLM.from_pretrained(hparams.lm_model)\n",
    "        self.seq2seq.config.is_decoder = True\n",
    "        self.seq2seq.add_cross_attention = True\n",
    "        print(self.seq2seq.config)\n",
    "\n",
    "    def forward(self, audio, attention_mask, labels, label_attention_mask):\n",
    "        #print(\"entering forward step\")\n",
    "        encoder_outputs = self.wav2vec2(audio[0],\n",
    "                                        attention_mask=attention_mask, #Made a change here by adding a [0]\n",
    "                                        output_hidden_states=True,\n",
    "                                        output_attentions=True)\n",
    "        encoder_hidden_states = encoder_outputs[0]  \n",
    "        print(f\"encoder_hidden_states={encoder_hidden_states}\")\n",
    "        encoder_attention_mask = self.wav2vec2._get_feature_vector_attention_mask(\n",
    "                encoder_hidden_states.shape[1], attention_mask\n",
    "            )\n",
    "        print(f\"[PRIOR]decoder_input_ids={labels}\")\n",
    "        print(f\"[PRIOR]decoder_attention_masks={label_attention_mask}\")\n",
    "        decoder_input_ids = self.shift_tokens_right(labels, 0, 101)\n",
    "        decoder_attention_masks = self.shift_tokens_right_mask(label_attention_mask)\n",
    "        print(f\"[AFTER]decoder_input_ids={decoder_input_ids}\")\n",
    "        print(f\"[AFTER]decoder_attention_masks={decoder_attention_masks}\")\n",
    "        decoder_outputs = self.seq2seq(input_ids=decoder_input_ids,\n",
    "                                       attention_mask = decoder_attention_masks,\n",
    "                                       encoder_hidden_states=encoder_hidden_states,\n",
    "                                      encoder_attention_mask=encoder_attention_mask) #,attention_mask = x.attentions, decoder_input_ids=predicted_ids\n",
    "        return decoder_outputs\n",
    "\n",
    "    def generate(self, audio):\n",
    "        encoder_outputs = self.wav2vec2(audio[0],output_hidden_states=True,output_attentions=True)\n",
    "        encoder_hidden_states = encoder_outputs[0]\n",
    "        bos_ids = (\n",
    "            torch.ones(\n",
    "                (encoder_hidden_states.size()[0], 1),\n",
    "                dtype=torch.long,\n",
    "                device=self.seq2seq.device,\n",
    "            )\n",
    "            * self.seq2seq.config.pad_token_id\n",
    "        )\n",
    "        return self.seq2seq.generate(\n",
    "            input_ids=bos_ids,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        #print(\"entering training step\")\n",
    "        audio = batch[\"input_values\"].unsqueeze(0)\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        label_attention_mask = batch[\"label_attention_masks\"].unsqueeze(0)\n",
    "        #print(f\"audio={audio}, attention_mask = {attention_mask}, label_attention_mask = {label_attention_mask}\")\n",
    "        labels = batch[\"labels\"]\n",
    "        labels = labels.reshape(self.batch_size,-1)\n",
    "        logits = self(audio,attention_mask,labels,label_attention_mask).logits\n",
    "        logits = logits.reshape(-1, self.batch_size, self.seq2seq.config.vocab_size)\n",
    "        input_lengths = torch.full(size=(self.batch_size,), fill_value=logits.shape[0], dtype=torch.long)\n",
    "        target_lengths = torch.full(size=(self.batch_size,), fill_value=labels.shape[0], dtype=torch.long)\n",
    "        # ctc_loss =  nn.CTCLoss(blank=0)\n",
    "        # loss = ctc_loss(logits,labels,input_lengths,target_lengths)\n",
    "        ce_loss = nn.CrossEntropyLoss()\n",
    "        loss = ce_loss(logits, labels)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch,batch_idx):\n",
    "        #print(\"entering validation step\")\n",
    "        audio = batch[\"input_values\"].unsqueeze(0)\n",
    "        labels = batch[\"labels\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        label_attention_mask = batch[\"label_attention_masks\"].unsqueeze(0)\n",
    "        #print(f\"audio={audio}, attention_mask = {attention_mask}, label_attention_mask = {label_attention_mask}\")\n",
    "        labels = labels.reshape(self.batch_size,-1)\n",
    "        logits = self(audio,attention_mask,labels,label_attention_mask).logits\n",
    "        logits = logits.reshape(-1, self.batch_size, self.seq2seq.config.vocab_size)\n",
    "        input_lengths = torch.full(size=(self.batch_size,), fill_value=logits.shape[0], dtype=torch.long)\n",
    "        target_lengths = torch.full(size=(self.batch_size,), fill_value=labels.shape[0], dtype=torch.long)\n",
    "        # ctc_loss =  nn.CTCLoss(blank=0)\n",
    "        # loss = ctc_loss(logits,labels,input_lengths,target_lengths)\n",
    "        ce_loss = nn.CrossEntropyLoss()\n",
    "        loss = ce_loss(logits, labels)\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        label_decoded = labels.type(torch.int64).tolist()\n",
    "        #print(f\"labels = {labels}\")\n",
    "        #print(f\"predicted ids = {predicted_ids}\")\n",
    "        print(f\"original text = {self.bert_tokenizer.decode(label_decoded[0])},{self.bert_tokenizer.decode(label_decoded[1])}\")\n",
    "        predicted_text = predicted_ids.type(torch.int64)\n",
    "        print(f\"Predicted text = {self.bert_tokenizer.decode(predicted_text[:,0].flatten().tolist())},{self.bert_tokenizer.decode(predicted_text[:,-1].flatten().tolist())}\")\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=hparams.learning_rate)\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
    "        \"\"\"\n",
    "        Shift input ids one token to the right.\n",
    "        \"\"\"\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "        if decoder_start_token_id is None:\n",
    "            raise ValueError(\"Make sure to set the decoder_start_token_id attribute of the model's configuration.\")\n",
    "        shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "    \n",
    "        if pad_token_id is None:\n",
    "            raise ValueError(\"Make sure to set the pad_token_id attribute of the model's configuration.\")\n",
    "        # replace possible -100 values in labels by `pad_token_id`\n",
    "        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "    \n",
    "        return shifted_input_ids\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_tokens_right_mask(input_ids: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Shift input ids one token to the right.\n",
    "        \"\"\"\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[:,:, 1:] = input_ids[:,:, :-1].clone()\n",
    "        shifted_input_ids[:,:, 0] = 0\n",
    "        return shifted_input_ids.squeeze()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1365d597-7bd4-420e-8077-b021455357df",
   "metadata": {},
   "source": [
    "test_data = SpeechRecognitionDataModule(WAV2VEC2_ARGS, num_workers = 0)\n",
    "test_data.setup()\n",
    "print(next(iter(test_data.val_dataloader())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d93bcca1-79eb-4f51-9dd3-368e192fcfcd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-large-960h-lv60-self\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_attn_dim\": null,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"mean\",\n",
      "  \"ctc_zero_infinity\": true,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "CPU Count = 4\n",
      "In Stage = Fit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73608bda21c24162a14f86a6aa8ec4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a7901d2fd34512b5e950dd6cb6eb32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type            | Params\n",
      "---------------------------------------------\n",
      "0 | wav2vec2 | Wav2Vec2ForCTC  | 315 M \n",
      "1 | seq2seq  | BertLMHeadModel | 109 M \n",
      "---------------------------------------------\n",
      "420 M     Trainable params\n",
      "4.2 M     Non-trainable params\n",
      "424 M     Total params\n",
      "1,699.943 Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entering val data loader\n",
      "encoder_hidden_states=tensor([[[ 13.2059,  -9.3881,  -9.4763,  ...,  -3.9759,  -4.3190,  -3.3699],\n",
      "         [ 13.2005,  -9.3863,  -9.4740,  ...,  -3.9772,  -4.3162,  -3.3681],\n",
      "         [ 13.1804,  -9.3830,  -9.4700,  ...,  -3.9760,  -4.3148,  -3.3615],\n",
      "         ...,\n",
      "         [ 13.1026,  -9.4301,  -9.5170,  ...,  -3.9986,  -4.3407,  -3.3799],\n",
      "         [ 13.1090,  -9.4243,  -9.5105,  ...,  -3.9988,  -4.3382,  -3.3929],\n",
      "         [  5.7097, -35.3043, -35.1321,  ..., -12.3908, -18.1052, -13.1409]],\n",
      "\n",
      "        [[ 12.8892, -10.3671, -10.4393,  ...,  -4.1289,  -4.5885,  -3.4776],\n",
      "         [ 12.8570, -10.2750, -10.3311,  ...,  -4.1606,  -4.5560,  -3.4659],\n",
      "         [ 12.9040, -10.0648, -10.1415,  ...,  -4.1024,  -4.4938,  -3.4318],\n",
      "         ...,\n",
      "         [  8.4392, -26.1368, -25.6246,  ...,  -4.6106,  -4.3979,  -4.7279],\n",
      "         [  8.4392, -26.1368, -25.6246,  ...,  -4.6106,  -4.3979,  -4.7279],\n",
      "         [  8.4392, -26.1368, -25.6246,  ...,  -4.6106,  -4.3979,  -4.7279]]],\n",
      "       device='cuda:0')\n",
      "[PRIOR]decoder_input_ids=tensor([[  101,  2061,  4553,  2013,  2115, 12051,   102,  -100,  -100,  -100,\n",
      "          -100],\n",
      "        [  101,  1045,  1005,  2310,  2042,  4198,  2000,  1996,  2157,  2240,\n",
      "           102]], device='cuda:0')\n",
      "[PRIOR]decoder_attention_masks=tensor([[[0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "         [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]], device='cuda:0')\n",
      "[AFTER]decoder_input_ids=tensor([[  101,   101,  2061,  4553,  2013,  2115, 12051,   102,     0,     0,\n",
      "             0],\n",
      "        [  101,   101,  1045,  1005,  2310,  2042,  4198,  2000,  1996,  2157,\n",
      "          2240]], device='cuda:0')\n",
      "[AFTER]decoder_attention_masks=tensor([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "original text = [CLS] so learn from your mistakes [SEP] [UNK] [UNK] [UNK] [UNK],[CLS] i've been connected to the right line [SEP]\n",
      "Predicted text = many so now many and and the'been the right,for learn your so and \" as'' the the\n",
      "encoder_hidden_states=tensor([[[ 13.0357,  -9.6098,  -9.6965,  ...,  -4.0207,  -4.3701,  -3.4082],\n",
      "         [ 12.9981,  -9.6541,  -9.7400,  ...,  -4.0414,  -4.3810,  -3.4130],\n",
      "         [ 12.8417, -10.1472, -10.2223,  ...,  -4.1523,  -4.5257,  -3.4529],\n",
      "         ...,\n",
      "         [ 12.7111, -10.7931, -10.8472,  ...,  -4.4000,  -4.9217,  -3.7329],\n",
      "         [ 12.7125, -10.8392, -10.8920,  ...,  -4.4163,  -4.9400,  -3.7443],\n",
      "         [ 12.7137, -10.9200, -10.9708,  ...,  -4.4422,  -4.9770,  -3.7703]],\n",
      "\n",
      "        [[ 13.0248,  -9.5613,  -9.6468,  ...,  -3.9963,  -4.3500,  -3.3806],\n",
      "         [ 12.9903,  -9.6214,  -9.7080,  ...,  -4.0014,  -4.3690,  -3.3925],\n",
      "         [ 12.9440,  -9.6421,  -9.7207,  ...,  -4.0134,  -4.3861,  -3.4035],\n",
      "         ...,\n",
      "         [ 10.7352, -21.4841, -21.3626,  ...,  -5.9832,  -7.5187,  -2.9838],\n",
      "         [ 11.3716, -16.2923, -16.2665,  ...,  -5.1255,  -6.2335,  -2.7358],\n",
      "         [  5.8083, -35.1162, -34.9816,  ..., -12.1939, -17.9562, -13.0928]]],\n",
      "       device='cuda:0')\n",
      "[PRIOR]decoder_input_ids=tensor([[  101,  1996,  3606,  2000,  2022,  2179,   102,  -100,  -100,  -100,\n",
      "          -100,  -100],\n",
      "        [  101,  2002,  2056,  1996,  2126,  2026, 16558,  5657,  2159, 12342,\n",
      "          2094,   102]], device='cuda:0')\n",
      "[PRIOR]decoder_attention_masks=tensor([[[0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]], device='cuda:0')\n",
      "[AFTER]decoder_input_ids=tensor([[  101,   101,  1996,  3606,  2000,  2022,  2179,   102,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,   101,  2002,  2056,  1996,  2126,  2026, 16558,  5657,  2159,\n",
      "         12342,  2094]], device='cuda:0')\n",
      "[AFTER]decoder_attention_masks=tensor([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "original text = [CLS] the truth to be found [SEP] [UNK] [UNK] [UNK] [UNK] [UNK],[CLS] he said the way myblue eyes shined [SEP]\n",
      "Predicted text = or or often often as as it, it itlay shine,or or as as as as it, waybl bard\n",
      "entering train data loader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65858337a17e4ccb8d9d661fa179f362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_hidden_states=tensor([[[ 13.2333, -23.8555, -23.4841,  ...,  -5.3658,  -3.4472,  -3.1685],\n",
      "         [ 11.5298, -24.3932, -24.4742,  ...,  -6.3774,  -2.7487,  -4.8845],\n",
      "         [  8.7940, -24.5847, -25.0222,  ...,  -1.9890,   6.8930,  -3.9203],\n",
      "         ...,\n",
      "         [  7.0538, -25.2091, -24.7490,  ...,  -6.3622,  -6.9245,  -4.9849],\n",
      "         [  8.2973, -25.0508, -24.5228,  ...,  -7.2009,  -7.5637,  -5.4144],\n",
      "         [  8.1107, -25.3663, -25.2478,  ...,  -5.9740,  -6.1398,  -4.5526]],\n",
      "\n",
      "        [[  6.8999, -24.7396, -24.5651,  ...,  -4.4368,  -4.5482,  -3.6329],\n",
      "         [  7.9876, -25.0050, -24.7086,  ...,  -5.6266,  -7.1464,  -4.0452],\n",
      "         [  8.3215, -24.8860, -24.4427,  ...,  -4.2104,  -7.6929,  -3.4054],\n",
      "         ...,\n",
      "         [  9.9505, -23.0232, -23.2926,  ...,  -7.0951,  -8.0425,  -2.5369],\n",
      "         [  9.0284, -27.3691, -27.0113,  ...,  -7.3537,  -9.2589,  -7.4480],\n",
      "         [  7.3017, -25.9519, -25.6586,  ...,  -6.9050,  -6.5107,  -7.1333]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "[PRIOR]decoder_input_ids=tensor([[ 101, 2157, 2055, 2085, 1045, 1005, 1049, 5595, 5595,  102],\n",
      "        [ 101, 2292, 2033, 2448, 2026, 3093, 2083, 2115, 2606,  102]],\n",
      "       device='cuda:0')\n",
      "[PRIOR]decoder_attention_masks=tensor([[[0, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "         [0, 1, 1, 1, 1, 1, 1, 1, 1, 0]]], device='cuda:0')\n",
      "[AFTER]decoder_input_ids=tensor([[ 101,  101, 2157, 2055, 2085, 1045, 1005, 1049, 5595, 5595],\n",
      "        [ 101,  101, 2292, 2033, 2448, 2026, 3093, 2083, 2115, 2606]],\n",
      "       device='cuda:0')\n",
      "[AFTER]decoder_attention_masks=tensor([[0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 31.74 GiB total capacity; 2.73 GiB already allocated; 1.38 MiB free; 2.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m Wav2SeqModel(hparams)\n\u001b[1;32m     11\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mSpeechRecognitionDataModule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWAV2VEC2_ARGS\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 608\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[38;5;241m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    643\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    646\u001b[0m     ckpt_path,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    648\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    649\u001b[0m )\n\u001b[0;32m--> 650\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[0;32m-> 1112\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1191\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1214\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1214\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:267\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher\u001b[38;5;241m.\u001b[39msetup(dataloader, batch_to_device\u001b[38;5;241m=\u001b[39mbatch_to_device)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:213\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_started()\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 213\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# update non-plateau LR schedulers\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# update epoch-interval ones only when we are at the end of training epoch\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88\u001b[0m, in \u001b[0;36mTrainingBatchLoop.advance\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m     85\u001b[0m     optimizers \u001b[38;5;241m=\u001b[39m _get_active_optimizers(\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39moptimizers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39moptimizer_frequencies, kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     87\u001b[0m     )\n\u001b[0;32m---> 88\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_loop\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:202\u001b[0m, in \u001b[0;36mOptimizerLoop.advance\u001b[0;34m(self, optimizers, kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madvance\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizers: List[Tuple[\u001b[38;5;28mint\u001b[39m, Optimizer]], kwargs: OrderedDict) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_kwargs(kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hiddens)\n\u001b[0;32m--> 202\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_optimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim_progress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_position\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;66;03m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001b[39;00m\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;66;03m# would be skipped otherwise\u001b[39;00m\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_idx] \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39masdict()\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:249\u001b[0m, in \u001b[0;36mOptimizerLoop._run_optimization\u001b[0;34m(self, kwargs, optimizer)\u001b[0m\n\u001b[1;32m    241\u001b[0m         closure()\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# the `batch_idx` is optional with inter-batch parallelism\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# if no result, user decided to skip optimization\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m# otherwise update running loss + reset accumulated loss\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;66;03m# TODO: find proper way to handle updating running loss\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:370\u001b[0m, in \u001b[0;36mOptimizerLoop._optimizer_step\u001b[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    362\u001b[0m     rank_zero_deprecation(\n\u001b[1;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe NVIDIA/apex AMP implementation has been deprecated upstream. Consequently, its integration inside\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m PyTorch Lightning has been deprecated in v1.9.0 and will be removed in v2.0.0.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m return True.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m     )\n\u001b[1;32m    369\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing_native_amp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprecision_plugin, MixedPrecisionPlugin)\n\u001b[0;32m--> 370\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_tpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTPUAccelerator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_lbfgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_lbfgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1356\u001b[0m, in \u001b[0;36mTrainer._call_lightning_module_hook\u001b[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1353\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m   1355\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1356\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/core/module.py:1754\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_lbfgs)\u001b[0m\n\u001b[1;32m   1675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1676\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1677\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1683\u001b[0m     using_lbfgs: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1684\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1685\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;124;03m    Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;124;03m    each optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1752\u001b[0m \n\u001b[1;32m   1753\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1754\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:169\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:234\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:119\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[0;34m(self, optimizer, model, optimizer_idx, closure, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    118\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, optimizer_idx, closure)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/torch/optim/adam.py:132\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    129\u001b[0m     state_steps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     adam(\n\u001b[1;32m    142\u001b[0m         params_with_grad,\n\u001b[1;32m    143\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/torch/optim/adam.py:94\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m     92\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# Maintains max of all exp. moving avg. of sq. grad. values\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 31.74 GiB total capacity; 2.73 GiB already allocated; 1.38 MiB free; 2.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pytorch_lightning as pl\n",
    "hparams = argparse.Namespace()\n",
    "hparams.wav2vec2_model = 'facebook/wav2vec2-large-960h-lv60-self' #'openai/whisper-base'# #'openai/whisper-base'\n",
    "hparams.lm_model = 'bert-base-uncased' #'facebook/bart-large'\n",
    "hparams.vocab_size = 30000\n",
    "hparams.learning_rate = 0.00001\n",
    "hparams.batch_size = WAV2VEC2_ARGS.BATCH_SIZE\n",
    "\n",
    "model = Wav2SeqModel(hparams)\n",
    "trainer = Trainer(max_epochs=10,devices=1, accelerator=\"gpu\")\n",
    "trainer.fit(model,SpeechRecognitionDataModule(WAV2VEC2_ARGS,num_workers=4))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3fa8d2a-177c-40b8-b166-108a04b7c4ae",
   "metadata": {},
   "source": [
    "import librosa \n",
    "def create_audio_tensor(audio_path):\n",
    "    audio, sample_rate = librosa.load(audio_path)\n",
    "    audio_tensor = torch.from_numpy(audio).float()\n",
    "    audio_tensor = audio_tensor.unsqueeze(0)\n",
    "    return audio_tensor\n",
    "audio_tensor = create_audio_tensor(\"notebooks/separated/mdx_extra/test_clip/vocals.wav\")\n",
    "print(audio_tensor.unsqueeze(0).shape)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_hat = model.generate(audio_tensor.unsqueeze(0))\n",
    "torch.argmax(y_hat, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac5538f-c4f3-4c19-a729-26f38aff7b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f92b482-a046-4c0f-bcd3-2ca40b2fc33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_tensor = next(iter(test_data.val_dataloader()))[\"input_values\"]\n",
    "audio_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9938cd6-0d0e-4914-89cd-9772ac41a9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_hat = model.generate(audio_tensor.unsqueeze(0))\n",
    "torch.argmax(y_hat, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3838261-9da4-46a1-8a35-543a359e81cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import librosa\n",
    "# import torch\n",
    "# from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "# def create_audio_tensor(audio_path):\n",
    "#     audio, sample_rate = librosa.load(audio_path)\n",
    "#     audio_tensor = torch.from_numpy(audio).float()\n",
    "#     audio_tensor = audio_tensor.unsqueeze(0)\n",
    "#     return audio_tensor\n",
    "\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "# audio_tensor = create_audio_tensor(\"notebooks/separated/mdx_extra/test_clip/vocals.wav\")\n",
    "logits = model(audio_tensor.unsqueeze(0),torch.Tensor(18, 11,  0,  0,  0,  0,  0,  7,  0,  0,  0,  0,  6,  0,  0,  0,  4,  4,\n",
    "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  5,  5,  0,  0,  0,\n",
    "          0, 15, 15,  0,  0,  0,  0,  0,  0, 12,  0,  0,  0,  4,  4,  0,  0,  0,\n",
    "         14,  0,  0,  0,  0, 10,  0,  0, 12, 12,  0,  0,  0,  0,  0, 19,  0,  0,\n",
    "          0,  0,  0,  0,  0,  0,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
    "          0,  0,  0,  0,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  6, 11,\n",
    "          0,  0,  5,  0,  0,  0,  4,  4,  0,  0,  0, 24,  0,  0,  0,  0,  0,  0,\n",
    "          0,  0,  0,  5,  0,  0,  0,  0,  0,  9,  0,  0,  0,  0,  4,  4,  4,  0,\n",
    "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
    "          0,  0,  0))\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "print(audio_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c2fb4-ab29-433a-ab4f-5e895a1aaf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e40856-59d1-4d5e-84c3-ab64b528f23c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96624e0-53a7-47c0-bb31-07da5ad31208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4686fb-f6f8-4b08-af94-a9d18762a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.LogSoftmax(dim=1)\n",
    "input = torch.randn(2, 3)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa479c4-9660-4ee0-b8f1-7cabac8d3480",
   "metadata": {},
   "outputs": [],
   "source": [
    "input,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b60aad4-07ca-422c-a376-fab2cd8cbdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor,BertTokenizer\n",
    "from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC, AutoModel,AutoProcessor\n",
    "\n",
    "from datasets import load_dataset,Dataset,Audio\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "class SpeechRecognitionDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, WAV2VEC2_ARGS: WAV2VEC2_ARGS, num_workers):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.batch_size = WAV2VEC2_ARGS.BATCH_SIZE\n",
    "        self.num_workers = num_workers\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(hparams.wav2vec2_model)\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        #self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(WAV2VEC2_ARGS.MODEL_BACKBONE)\n",
    "        #self.feature_extractor = AutoFeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(hparams.wav2vec2_model)\n",
    "        #self.processor = Wav2Vec2Processor(feature_extractor=self.feature_extractor, tokenizer=self.tokenizer)\n",
    "        #self.processor = AutoProcessor(feature_extractor=self.feature_extractor, tokenizer=self.bert_tokenizer)\n",
    "        self.processor = WhisperProcessor(feature_extractor=self.feature_extractor, tokenizer=self.bert_tokenizer)\n",
    "        self.data_collator = DataCollatorCTCWithPadding(processor=self.processor, padding=True)\n",
    "        self.cpu_count = 4\n",
    "        print(f\"CPU Count = {self.cpu_count}\")\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        train_df = pd.read_csv(WAV2VEC2_ARGS.TRAIN_FILE_PATH).head(10)\n",
    "        validation_df = pd.read_csv(WAV2VEC2_ARGS.TEST_FILE_PATH).head(10)\n",
    "        if stage == 'fit' or stage is None:\n",
    "            print(\"In Stage = Fit\")\n",
    "            train_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(train_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(train_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "            self.train_dataset = train_dataset.map(self.prepare_dataset,remove_columns = train_dataset.column_names)\n",
    "            \n",
    "            val_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(validation_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(validation_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "            self.val_dataset = val_dataset.map(self.prepare_dataset,remove_columns = val_dataset.column_names)\n",
    "\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            print(\"In Stage = Test\")\n",
    "            test_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(validation_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(validation_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "            test_dataset = val_dataset.map(self.prepare_dataset,remove_columns = val_dataset.column_names)\n",
    "            self.test_dataset = test_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        print(\"entering train data loader\")\n",
    "        return DataLoader(\n",
    "            self.train_dataset.with_format(\"torch\"), \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn = self.data_collator\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        print(\"entering val data loader\")\n",
    "        return DataLoader(\n",
    "            self.val_dataset.with_format(\"torch\"), \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn = self.data_collator\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        print(\"entering test data loader\")\n",
    "        return DataLoader(\n",
    "            self.test_dataset.with_format(\"torch\"), \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn = self.data_collator\n",
    "        )\n",
    "\n",
    "    def prepare_dataset(self,batch):\n",
    "        audio = batch[\"audio\"]\n",
    "        # batched output is \"un-batched\" to ensure mapping is correct\n",
    "        batch[\"input_values\"] = self.processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "        batch[\"attention_mask\"] = self.processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).attention_mask[0]\n",
    "        batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "        with self.processor.as_target_processor():\n",
    "            batch[\"labels\"] = self.processor(batch[\"transcription\"]).input_ids\n",
    "            batch[\"label_attention_mask\"] = self.processor(batch[\"transcription\"]).attention_mask\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a286c7f4-b4f8-4849-a121-7b7337b701c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Model, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC, AutoModel\n",
    "\n",
    "class Wav2SeqModel(LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.batch_size = hparams.batch_size\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        #self.wav2vec2 = AutoModelForCTC.from_pretrained(hparams.wav2vec2_model,ctc_zero_infinity=True,ctc_loss_reduction=\"mean\")\n",
    "        #self.wav2vec2.freeze_feature_encoder()\n",
    "        self.wav2vec2 = AutoModel.from_pretrained(hparams.wav2vec2_model)\n",
    "        self.wav2vec2.eval()\n",
    "        print(self.wav2vec2.config)\n",
    "        self.seq2seq = AutoModelForCausalLM.from_pretrained(hparams.lm_model)\n",
    "        self.seq2seq.config.is_decoder = True\n",
    "        self.seq2seq.add_cross_attention = True\n",
    "        print(self.seq2seq.config)\n",
    "\n",
    "    def forward(self, audio, attention_mask, labels, label_attention_mask):\n",
    "        #print(\"entering forward step\")\n",
    "        encoder_outputs = self.wav2vec2(audio[0],\n",
    "                                        attention_mask=attention_mask, #Made a change here by adding a [0]\n",
    "                                        output_hidden_states=True,\n",
    "                                        output_attentions=True)\n",
    "        encoder_hidden_states = encoder_outputs[0]  \n",
    "        print(f\"encoder_hidden_states={encoder_hidden_states}\")\n",
    "        encoder_attention_mask = self.wav2vec2._get_feature_vector_attention_mask(\n",
    "                encoder_hidden_states.shape[1], attention_mask\n",
    "            )\n",
    "        #print(f\"[PRIOR]decoder_input_ids={labels}\")\n",
    "        #print(f\"[PRIOR]decoder_attention_masks={label_attention_mask}\")\n",
    "        decoder_input_ids = self.shift_tokens_right(labels, 0, 101)\n",
    "        decoder_attention_masks = self.shift_tokens_right_mask(label_attention_mask)\n",
    "        #print(f\"[AFTER]decoder_input_ids={decoder_input_ids}\")\n",
    "        #print(f\"[AFTER]decoder_attention_masks={decoder_attention_masks}\")\n",
    "        decoder_outputs = self.seq2seq(input_ids=decoder_input_ids,\n",
    "                                       attention_mask = decoder_attention_masks,\n",
    "                                       encoder_hidden_states=encoder_hidden_states,\n",
    "                                      encoder_attention_mask=encoder_attention_mask) #,attention_mask = x.attentions, decoder_input_ids=predicted_ids\n",
    "        return decoder_outputs\n",
    "\n",
    "    def generate(self, audio):\n",
    "        encoder_outputs = self.wav2vec2(audio[0],output_hidden_states=True,output_attentions=True)\n",
    "        encoder_hidden_states = encoder_outputs[0]\n",
    "        bos_ids = (\n",
    "            torch.ones(\n",
    "                (encoder_hidden_states.size()[0], 1),\n",
    "                dtype=torch.long,\n",
    "                device=self.seq2seq.device,\n",
    "            )\n",
    "            * self.seq2seq.config.pad_token_id\n",
    "        )\n",
    "        return self.seq2seq.generate(\n",
    "            input_ids=bos_ids,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        #print(\"entering training step\")\n",
    "        audio = batch[\"input_values\"].unsqueeze(0)\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        label_attention_mask = batch[\"label_attention_masks\"].unsqueeze(0)\n",
    "        #print(f\"audio={audio}, attention_mask = {attention_mask}, label_attention_mask = {label_attention_mask}\")\n",
    "        labels = batch[\"labels\"]\n",
    "        labels = labels.reshape(self.batch_size,-1)\n",
    "        logits = self(audio,attention_mask,labels,label_attention_mask).logits\n",
    "        logits = logits.reshape(-1, self.batch_size, self.seq2seq.config.vocab_size)\n",
    "        input_lengths = torch.full(size=(self.batch_size,), fill_value=logits.shape[0], dtype=torch.long)\n",
    "        target_lengths = torch.full(size=(self.batch_size,), fill_value=labels.shape[0], dtype=torch.long)\n",
    "        ctc_loss =  nn.CTCLoss(blank=0)\n",
    "        loss = ctc_loss(logits,labels,input_lengths,target_lengths)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch,batch_idx):\n",
    "        #print(\"entering validation step\")\n",
    "        audio = batch[\"input_values\"].unsqueeze(0)\n",
    "        labels = batch[\"labels\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        label_attention_mask = batch[\"label_attention_masks\"].unsqueeze(0)\n",
    "        #print(f\"audio={audio}, attention_mask = {attention_mask}, label_attention_mask = {label_attention_mask}\")\n",
    "        labels = labels.reshape(self.batch_size,-1)\n",
    "        logits = self(audio,attention_mask,labels,label_attention_mask).logits\n",
    "        logits = logits.reshape(-1, self.batch_size, self.seq2seq.config.vocab_size)\n",
    "        input_lengths = torch.full(size=(self.batch_size,), fill_value=logits.shape[0], dtype=torch.long)\n",
    "        target_lengths = torch.full(size=(self.batch_size,), fill_value=labels.shape[0], dtype=torch.long)\n",
    "        ctc_loss =  nn.CTCLoss(blank=0)\n",
    "        loss = ctc_loss(logits,labels,input_lengths,target_lengths)\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        label_decoded = labels.type(torch.int64).tolist()\n",
    "        #print(f\"labels = {labels}\")\n",
    "        #print(f\"predicted ids = {predicted_ids}\")\n",
    "        print(f\"original text = {self.bert_tokenizer.decode(label_decoded[0])},{self.bert_tokenizer.decode(label_decoded[1])}\")\n",
    "        predicted_text = predicted_ids.type(torch.int64)\n",
    "        print(f\"Predicted text = {self.bert_tokenizer.decode(predicted_text[:,0].flatten().tolist())},{self.bert_tokenizer.decode(predicted_text[:,-1].flatten().tolist())}\")\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=hparams.learning_rate)\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
    "        \"\"\"\n",
    "        Shift input ids one token to the right.\n",
    "        \"\"\"\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "        if decoder_start_token_id is None:\n",
    "            raise ValueError(\"Make sure to set the decoder_start_token_id attribute of the model's configuration.\")\n",
    "        shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "    \n",
    "        if pad_token_id is None:\n",
    "            raise ValueError(\"Make sure to set the pad_token_id attribute of the model's configuration.\")\n",
    "        # replace possible -100 values in labels by `pad_token_id`\n",
    "        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "    \n",
    "        return shifted_input_ids\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_tokens_right_mask(input_ids: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Shift input ids one token to the right.\n",
    "        \"\"\"\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[:,:, 1:] = input_ids[:,:, :-1].clone()\n",
    "        shifted_input_ids[:,:, 0] = 0\n",
    "        return shifted_input_ids.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41800a8c-b400-4f13-acc7-af7a56bbfa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(WAV2VEC2_ARGS.TRAIN_FILE_PATH)\n",
    "validation_df = pd.read_csv(WAV2VEC2_ARGS.TEST_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497b1e3a-6e1a-4d93-9f48-a970bf08eea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df.transcription_capitalized.str.len()>200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806eb51a-1af0-4b4b-a02d-f26c16beed6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93f879b6-a836-4798-86ed-6bf40d44fc81",
   "metadata": {},
   "source": [
    "# Readying the codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9aa9d25-0b49-480f-87fd-a7a1439cea0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgreeshmasmenon\u001b[0m (\u001b[33msongslyricstranscription\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230821_082425-porh3ojt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20transfer%20learning/runs/porh3ojt' target=\"_blank\">earthy-vortex-99</a></strong> to <a href='https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20transfer%20learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20transfer%20learning' target=\"_blank\">https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20transfer%20learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20transfer%20learning/runs/porh3ojt' target=\"_blank\">https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20transfer%20learning/runs/porh3ojt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TRAIN_FILE_PATH\": \"/scratch/users/gmenon/train_song_metadata_en_demucs_cleaned_filtered_095.csv\",\n",
      "    \"TEST_FILE_PATH\": \"/scratch/users/gmenon/validation_song_metadata_en_demucs_cleaned_filtered_005.csv\",\n",
      "    \"MODEL_BACKBONE\": \"facebook/wav2vec2-large-960h-lv60-self\",\n",
      "    \"BATCH_SIZE\": 2,\n",
      "    \"NUM_EPOCHS\": 15,\n",
      "    \"MODEL_SAVE_PATH\": \"/scratch/users/gmenon//model_artefacts/wav2vec2_demucs_en_large-960h-lv60-self_freeze_unfreeze_15epochs_adamw.pt\",\n",
      "    \"FINETUNE_STRATEGY\": [\n",
      "        \"freeze_unfreeze\",\n",
      "        10\n",
      "    ],\n",
      "    \"LR_SCHEDULER\": \"reduce_on_plateau_schedule\"\n",
      "}\n",
      "Namespace(batch_size=2, learning_rate=1e-06, lm_model='bert-base-uncased', vocab_size=30000, wav2vec2_model='facebook/wav2vec2-large-960h-lv60-self')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-large-960h-lv60-self\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_attn_dim\": null,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "CPU Count = 4\n",
      "In Stage = Fit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db694eacfd7546dc8e72fb4297e2d096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': '/scratch/users/gmenon/wav_clips/separated/htdemucs/01cef35811fd4a3fa63a3ab8bba5430c/vocals.wav', 'array': array([-0.01242626, -0.06679466, -0.00535162, ...,  0.19485795,\n",
      "        0.20158702,  0.        ]), 'sampling_rate': 16000}, 'transcription': \"RIGHT ABOUT NOW I'M FIFTY FIFTY\", 'input_values': array([-0.37839139, -0.9591157 , -0.3028252 , ...,  1.83567   ,\n",
      "        1.9075451 , -0.24566302], dtype=float32), 'attention_mask': array([1, 1, 1, ..., 1, 1, 1], dtype=int32), 'input_length': 39159, 'labels': [101, 155, 23413, 18784, 16151, 2346, 16830, 24819, 2924, 146, 112, 150, 143, 15499, 16880, 143, 15499, 16880, 102], 'label_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'audio': {'path': '/scratch/users/gmenon/wav_clips/separated/htdemucs/0c04880a6eaf4e559fd9b20594eb74cd/vocals.wav', 'array': array([-0.00312309, -0.0268747 , -0.046181  , ...,  0.05217864,\n",
      "        0.04038712,  0.        ]), 'sampling_rate': 16000}, 'transcription': 'LET ME RUN MY FINGERS THROUGH YOUR HAIR', 'input_values': array([-0.04593911, -0.4045467 , -0.696038  , ...,  0.7890201 ,\n",
      "        0.61098874,  0.0012141 ], dtype=float32), 'attention_mask': array([1, 1, 1, ..., 1, 1, 1], dtype=int32), 'input_length': 66205, 'labels': [101, 149, 11943, 22157, 155, 27370, 150, 3663, 143, 15740, 9637, 1708, 157, 3048, 21564, 2591, 2349, 3048, 19141, 2069, 145, 1592, 18172, 102], 'label_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'audio': {'path': '/scratch/users/gmenon/wav_clips/separated/htdemucs/01a7c7f2e6db4c68abfbc1218dc9ef76/vocals.wav', 'array': array([ 0.01353866,  0.00440393, -0.04988923, ...,  0.0555356 ,\n",
      "        0.080953  ,  0.        ]), 'sampling_rate': 16000}, 'transcription': \"BUT YOU'RE YESTERDAY'S CHILD TO ME\", 'input_values': array([ 0.1281477 ,  0.040337  , -0.48157454, ...,  0.53185767,\n",
      "        0.7761912 , -0.00199725], dtype=float32), 'attention_mask': array([1, 1, 1, ..., 1, 1, 1], dtype=int32), 'input_length': 46780, 'labels': [101, 139, 16830, 19141, 112, 155, 2036, 162, 9919, 12880, 23354, 1592, 3663, 112, 156, 24890, 17656, 2137, 16972, 22157, 102], 'label_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'audio': {'path': '/scratch/users/gmenon/wav_clips/separated/htdemucs/0c8c027ad9374db6b9e6c12dc873be7b/vocals.wav', 'array': array([ 0.00052718,  0.00067053,  0.00012973, ..., -0.00186653,\n",
      "       -0.00116307,  0.        ]), 'sampling_rate': 16000}, 'transcription': 'AND WHISPER SO SOFTLY', 'input_values': array([ 0.00856515,  0.01073111,  0.00255971, ..., -0.02760304,\n",
      "       -0.01697393,  0.00059959], dtype=float32), 'attention_mask': array([1, 1, 1, ..., 1, 1, 1], dtype=int32), 'input_length': 32935, 'labels': [101, 16716, 160, 3048, 6258, 2101, 9637, 156, 2346, 156, 2346, 26321, 2162, 3663, 102], 'label_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'audio': {'path': '/scratch/users/gmenon/wav_clips/separated/htdemucs/08558a8941c04a559a3ea48793ac1a34/vocals.wav', 'array': array([-0.00957847, -0.03610604, -0.04754353, ...,  0.00834633,\n",
      "        0.00938783,  0.        ]), 'sampling_rate': 16000}, 'transcription': 'HAVE A LAUGH AT THE RAT RACE', 'input_values': array([-0.06932838, -0.24880685, -0.32618988, ...,  0.05194601,\n",
      "        0.0589925 , -0.00452304], dtype=float32), 'attention_mask': array([1, 1, 1, ..., 1, 1, 1], dtype=int32), 'input_length': 32610, 'labels': [101, 145, 26390, 2036, 138, 10722, 2591, 2349, 3048, 13020, 7462, 26547, 1942, 26547, 10954, 102], 'label_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'audio': {'path': '/scratch/users/gmenon/wav_clips/separated/htdemucs/07929e956a9745fa96e91730d8540cd1/vocals.wav', 'array': array([-1.10489207e-04, -7.11857065e-05,  2.02913770e-06, ...,\n",
      "       -3.43970569e-05, -5.12741208e-05,  0.00000000e+00]), 'sampling_rate': 16000}, 'transcription': 'AAAA HAAA HAAA', 'input_values': array([-0.17427056, -0.06104483,  0.14987285, ...,  0.04493609,\n",
      "       -0.00368344,  0.14402731], dtype=float32), 'attention_mask': array([1, 1, 1, ..., 1, 1, 1], dtype=int32), 'input_length': 43248, 'labels': [101, 13807, 1592, 145, 11189, 1592, 145, 11189, 1592, 102], 'label_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'audio': {'path': '/scratch/users/gmenon/wav_clips/separated/htdemucs/002fe3610f1b41b59d28546201635043/vocals.wav', 'array': array([ 0.00026922,  0.00069345,  0.00059003, ..., -0.0002898 ,\n",
      "       -0.00055549,  0.        ]), 'sampling_rate': 16000}, 'transcription': \"BUT THAT'S NOT HOW IT USED TO BE\", 'input_values': array([ 0.00308637,  0.00618155,  0.00542703, ..., -0.00099212,\n",
      "       -0.00293056,  0.0011222 ], dtype=float32), 'attention_mask': array([1, 1, 1, ..., 1, 1, 1], dtype=int32), 'input_length': 56114, 'labels': [101, 139, 16830, 157, 11612, 1942, 112, 156, 24819, 1942, 145, 17056, 9686, 1646, 10069, 16972, 139, 2036, 102], 'label_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'audio': {'path': '/scratch/users/gmenon/wav_clips/separated/htdemucs/0a4e93435d0746d39ea0814667d6fa3a/vocals.wav', 'array': array([-0.0389038 , -0.04655774,  0.03475165, ...,  0.22686306,\n",
      "        0.17466137,  0.        ]), 'sampling_rate': 16000}, 'transcription': \"AND YOU'VE SEEN IT ALL BEFORE BUT THE WOLF'S OUTSIDE YOUR DOOR\", 'input_values': array([-0.2939282 , -0.35308152,  0.27531672, ...,  1.7600464 ,\n",
      "        1.3566065 ,  0.00673916], dtype=float32), 'attention_mask': array([1, 1, 1, ..., 1, 1, 1], dtype=int32), 'input_length': 48328, 'labels': [101, 16716, 19141, 112, 159, 2036, 12342, 11680, 9686, 18589, 2162, 139, 14663, 9565, 2036, 139, 16830, 7462, 160, 13901, 2271, 112, 156, 152, 16830, 13882, 20427, 19141, 2069, 141, 2346, 9565, 102], 'label_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'audio': {'path': '/scratch/users/gmenon/wav_clips/separated/htdemucs/0bde4da0cf2842028bf7d1a712dec1c5/vocals.wav', 'array': array([ 0.00056127,  0.00239139,  0.00253299, ..., -0.07850006,\n",
      "       -0.08049925,  0.        ]), 'sampling_rate': 16000}, 'transcription': 'AND TALK IS CHEAP AWHEN THE STORY IS GOOD', 'input_values': array([ 1.3003699e-02,  5.4385010e-02,  5.7586595e-02, ...,\n",
      "       -1.7746736e+00, -1.8198777e+00,  3.1257488e-04], dtype=float32), 'attention_mask': array([1, 1, 1, ..., 1, 1, 1], dtype=int32), 'input_length': 43758, 'labels': [101, 16716, 157, 12507, 2428, 19432, 24890, 12420, 2101, 138, 2924, 3048, 11680, 7462, 23676, 9565, 3663, 19432, 27157, 15609, 102], 'label_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'audio': {'path': '/scratch/users/gmenon/wav_clips/separated/htdemucs/0851f1ca1594443d897d4a32b4eab788/vocals.wav', 'array': array([ 0.00032071, -0.00098128, -0.00218035, ..., -0.00251653,\n",
      "       -0.00220199,  0.        ]), 'sampling_rate': 16000}, 'transcription': 'TURNED YOU INTO SOMEONE NEW', 'input_values': array([ 0.00669078, -0.01386142, -0.03278906, ..., -0.03809577,\n",
      "       -0.03313072,  0.00162831], dtype=float32), 'attention_mask': array([1, 1, 1, ..., 1, 1, 1], dtype=int32), 'input_length': 36611, 'labels': [101, 157, 19556, 22680, 2137, 19141, 15969, 18082, 156, 13041, 2036, 11414, 2036, 26546, 2924, 102], 'label_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a53ae701e04cd7a42a80fb723c8a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': '/scratch/users/gmenon/wav_clips/separated/htdemucs/0aa835068aa64515b2158c2f0b6513b7/vocals.wav', 'array': array([ 0.00768129,  0.00689538, -0.0072176 , ...,  0.00820587,\n",
      "       -0.03958851,  0.        ]), 'sampling_rate': 16000}, 'transcription': 'SO LEARN FROM YOUR MISTAKES', 'input_values': array([ 0.0553176 ,  0.04940544, -0.05676297, ...,  0.05926391,\n",
      "       -0.30028123, -0.00246681], dtype=float32), 'attention_mask': array([1, 1, 1, ..., 1, 1, 1], dtype=int32), 'input_length': 76080, 'labels': [101, 156, 2346, 149, 12420, 2069, 2249, 143, 21564, 2107, 19141, 2069, 26574, 9272, 1592, 22441, 1708, 102], 'label_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'audio': {'path': '/scratch/users/gmenon/wav_clips/separated/htdemucs/089cd84d662e4c50954cfa46c07cae14/vocals.wav', 'array': array([ 8.71017910e-05,  1.32896865e-04, -3.25404108e-05, ...,\n",
      "       -3.55621916e-04, -2.33990228e-04,  0.00000000e+00]), 'sampling_rate': 16000}, 'transcription': \"I'VE BEEN CONNECTED TO THE RIGHT LINE\", 'input_values': array([ 0.00778199,  0.00865428,  0.00550306, ..., -0.00065094,\n",
      "        0.00166588,  0.00612289], dtype=float32), 'attention_mask': array([1, 1, 1, ..., 1, 1, 1], dtype=int32), 'input_length': 44151, 'labels': [101, 146, 112, 159, 2036, 139, 27073, 2249, 18732, 2249, 22680, 16647, 10069, 16972, 7462, 155, 23413, 18784, 149, 11607, 2036, 102], 'label_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'audio': {'path': '/scratch/users/gmenon/wav_clips/separated/htdemucs/127cc2aaff9a44fab21b2a0c4f5c3e5d/vocals.wav', 'array': array([-0.00048735, -0.00017505,  0.00117214, ..., -0.01783411,\n",
      "       -0.01409711,  0.        ]), 'sampling_rate': 16000}, 'transcription': 'THE TRUTH TO BE FOUND', 'input_values': array([-0.00202202,  0.00097077,  0.0138808 , ..., -0.16825496,\n",
      "       -0.13244353,  0.00264824], dtype=float32), 'attention_mask': array([1, 1, 1, ..., 1, 1, 1], dtype=int32), 'input_length': 42693, 'labels': [101, 7462, 157, 2069, 16830, 3048, 16972, 139, 2036, 143, 2346, 27370, 2137, 102], 'label_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'audio': {'path': '/scratch/users/gmenon/wav_clips/separated/htdemucs/18e03c3980014046a3504ef42fe81ae8/vocals.wav', 'array': array([0.00051718, 0.00063564, 0.00065174, ..., 0.01805303, 0.01632161,\n",
      "       0.        ]), 'sampling_rate': 16000}, 'transcription': 'HE SAID THE WAY MYBLUE EYES SHINED', 'input_values': array([ 0.00266595,  0.00341666,  0.00351871, ...,  0.11379667,\n",
      "        0.10282407, -0.00061161], dtype=float32), 'attention_mask': array([1, 1, 1, ..., 1, 1, 1], dtype=int32), 'input_length': 46322, 'labels': [101, 145, 2036, 13411, 9949, 7462, 22751, 3663, 150, 3663, 13360, 24846, 142, 3663, 9919, 17730, 11607, 10069, 102], 'label_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'audio': {'path': '/scratch/users/gmenon/wav_clips/separated/htdemucs/07c8b8c0b0424c1e87389392695a551b/vocals.wav', 'array': array([ 0.00089497,  0.00205238,  0.00402898, ..., -0.00495676,\n",
      "       -0.00487107,  0.        ]), 'sampling_rate': 16000}, 'transcription': 'YOU LEAVE ME ONCE AGAIN HOME ALONE', 'input_values': array([ 0.02511287,  0.035881  ,  0.05427045, ..., -0.02932927,\n",
      "       -0.02853212,  0.01678643], dtype=float32), 'attention_mask': array([1, 1, 1, ..., 1, 1, 1], dtype=int32), 'input_length': 42672, 'labels': [101, 19141, 149, 12420, 17145, 22157, 21748, 10954, 14731, 1592, 11607, 145, 13041, 2036, 18589, 11414, 2036, 102], 'label_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'audio': {'path': '/scratch/users/gmenon/wav_clips/separated/htdemucs/0cb795c9b6e64a4abab5553c2b2ed3e0/vocals.wav', 'array': array([ 0.00407792,  0.00723517,  0.0076816 , ..., -0.02896231,\n",
      "       -0.03239755,  0.        ]), 'sampling_rate': 16000}, 'transcription': 'WHILE THEY ARE IN COMMEND', 'input_values': array([ 0.02720711,  0.05045969,  0.05374754, ..., -0.21612814,\n",
      "       -0.24142803, -0.00282603], dtype=float32), 'attention_mask': array([1, 1, 1, ..., 1, 1, 1], dtype=int32), 'input_length': 43887, 'labels': [101, 160, 3048, 17656, 2036, 7462, 3663, 22133, 2036, 15969, 18732, 25290, 11680, 2137, 102], 'label_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'audio': {'path': '/scratch/users/gmenon/wav_clips/separated/htdemucs/0655395364614218be31548245853ca0/vocals.wav', 'array': array([-0.09609721, -0.26857331, -0.35068995, ...,  0.17164174,\n",
      "        0.16529259,  0.        ]), 'sampling_rate': 16000}, 'transcription': 'A LIFE ALL MINE', 'input_values': array([-4.1982284e-01, -1.1709135e+00, -1.5285110e+00, ...,\n",
      "        7.4611378e-01,  7.1846485e-01, -1.3433518e-03], dtype=float32), 'attention_mask': array([1, 1, 1, ..., 1, 1, 1], dtype=int32), 'input_length': 75164, 'labels': [101, 138, 149, 15499, 2036, 18589, 2162, 26574, 22680, 102], 'label_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'audio': {'path': '/scratch/users/gmenon/wav_clips/separated/htdemucs/113bb605eb324aeb8db046b4193424a2/vocals.wav', 'array': array([-0.02348703,  0.02917787,  0.03286819, ..., -0.00478536,\n",
      "       -0.00230002,  0.        ]), 'sampling_rate': 16000}, 'transcription': 'SO I NEVER WENT BACK', 'input_values': array([-0.16703744,  0.26465353,  0.29490292, ..., -0.01374093,\n",
      "        0.00663123,  0.02548434], dtype=float32), 'attention_mask': array([1, 1, 1, ..., 1, 1, 1], dtype=int32), 'input_length': 42288, 'labels': [101, 156, 2346, 146, 26546, 17145, 2069, 160, 11680, 1942, 12465, 1658, 2428, 102], 'label_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'audio': {'path': '/scratch/users/gmenon/wav_clips/separated/htdemucs/00ecd4b71c094b959ae492ec138bb555/vocals.wav', 'array': array([0.00085518, 0.00132938, 0.00094412, ..., 0.00092911, 0.00053253,\n",
      "       0.        ]), 'sampling_rate': 16000}, 'transcription': 'IN YOU I TASTE GOD', 'input_values': array([-0.10882185,  0.70078903,  0.04303571, ...,  0.01740086,\n",
      "       -0.6596727 , -1.5688527 ], dtype=float32), 'attention_mask': array([1, 1, 1, ..., 1, 1, 1], dtype=int32), 'input_length': 50825, 'labels': [101, 15969, 19141, 146, 157, 10719, 12880, 27157, 2137, 102], 'label_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'audio': {'path': '/scratch/users/gmenon/wav_clips/separated/htdemucs/0b5fa72c054e44598ec8aefb76ff855e/vocals.wav', 'array': array([0.00246025, 0.00450533, 0.00380508, ..., 0.00518884, 0.00082541,\n",
      "       0.        ]), 'sampling_rate': 16000}, 'transcription': 'COULD STAY A WHILE', 'input_values': array([0.01700313, 0.03074011, 0.02603648, ..., 0.03533131, 0.00602176,\n",
      "       0.00047739], dtype=float32), 'attention_mask': array([1, 1, 1, ..., 1, 1, 1], dtype=int32), 'input_length': 39441, 'labels': [101, 18732, 2591, 20521, 23676, 1592, 3663, 138, 160, 3048, 17656, 2036, 102], 'label_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type            | Params\n",
      "---------------------------------------------\n",
      "0 | wav2vec2 | Wav2Vec2ForCTC  | 315 M \n",
      "1 | seq2seq  | BertLMHeadModel | 109 M \n",
      "---------------------------------------------\n",
      "420 M     Trainable params\n",
      "4.2 M     Non-trainable params\n",
      "424 M     Total params\n",
      "1,699.943 Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fac940d4bc843098acc5ed423736334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entering val data loader\n",
      "{'input_values': tensor([[-0.0020,  0.0010,  0.0139,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0027,  0.0034,  0.0035,  ...,  0.1138,  0.1028, -0.0006]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  101,  7462,   157,  2069, 16830,  3048, 16972,   139,  2036,   143,\n",
      "          2346, 27370,  2137,   102,  -100,  -100,  -100,  -100,  -100],\n",
      "        [  101,   145,  2036, 13411,  9949,  7462, 22751,  3663,   150,  3663,\n",
      "         13360, 24846,   142,  3663,  9919, 17730, 11607, 10069,   102]]), 'label_attention_masks': tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}{'input_values': tensor([[ 0.0251,  0.0359,  0.0543,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0272,  0.0505,  0.0537,  ..., -0.2161, -0.2414, -0.0028]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  101, 19141,   149, 12420, 17145, 22157, 21748, 10954, 14731,  1592,\n",
      "         11607,   145, 13041,  2036, 18589, 11414,  2036,   102],\n",
      "        [  101,   160,  3048, 17656,  2036,  7462,  3663, 22133,  2036, 15969,\n",
      "         18732, 25290, 11680,  2137,   102,  -100,  -100,  -100]]), 'label_attention_masks': tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}\n",
      "{'input_values': tensor([[ 0.0553,  0.0494, -0.0568,  ...,  0.0593, -0.3003, -0.0025],\n",
      "        [ 0.0078,  0.0087,  0.0055,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  101,   156,  2346,   149, 12420,  2069,  2249,   143, 21564,  2107,\n",
      "         19141,  2069, 26574,  9272,  1592, 22441,  1708,   102,  -100,  -100,\n",
      "          -100,  -100],\n",
      "        [  101,   146,   112,   159,  2036,   139, 27073,  2249, 18732,  2249,\n",
      "         22680, 16647, 10069, 16972,  7462,   155, 23413, 18784,   149, 11607,\n",
      "          2036,   102]]), 'label_attention_masks': tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}\n",
      "\n",
      "{'input_values': tensor([[-0.1088,  0.7008,  0.0430,  ...,  0.0174, -0.6597, -1.5689],\n",
      "        [ 0.0170,  0.0307,  0.0260,  ...,  0.0000,  0.0000,  0.0000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  101, 15969, 19141,   146,   157, 10719, 12880, 27157,  2137,   102,\n",
      "          -100,  -100,  -100],\n",
      "        [  101, 18732,  2591, 20521, 23676,  1592,  3663,   138,   160,  3048,\n",
      "         17656,  2036,   102]]), 'label_attention_masks': tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}\n",
      "{'input_values': tensor([[-4.1982e-01, -1.1709e+00, -1.5285e+00,  ...,  7.4611e-01,\n",
      "          7.1846e-01, -1.3434e-03],\n",
      "        [-1.6704e-01,  2.6465e-01,  2.9490e-01,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  101,   138,   149, 15499,  2036, 18589,  2162, 26574, 22680,   102,\n",
      "          -100,  -100,  -100,  -100],\n",
      "        [  101,   156,  2346,   146, 26546, 17145,  2069,   160, 11680,  1942,\n",
      "         12465,  1658,  2428,   102]]), 'label_attention_masks': tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}\n",
      "[[101, 156, 2346, 149, 12420, 2069, 2249, 143, 21564, 2107, 19141, 2069, 26574, 9272, 1592, 22441, 1708, 102, -100, -100, -100, -100], [101, 146, 112, 159, 2036, 139, 27073, 2249, 18732, 2249, 22680, 16647, 10069, 16972, 7462, 155, 23413, 18784, 149, 11607, 2036, 102]]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'transcription'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39m chdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/users/gmenon/workspace/songsLyricsGenerator/src\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lyrics_finetune\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmir_constants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingArgs, WAV2VEC2_ARGS\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/src/training/lyrics_finetune.py:328\u001b[0m\n\u001b[1;32m    325\u001b[0m hparams\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-6\u001b[39m\n\u001b[1;32m    326\u001b[0m hparams\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 328\u001b[0m model,trainer \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/src/training/lyrics_finetune.py:317\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(hparams)\u001b[0m\n\u001b[1;32m    315\u001b[0m model \u001b[38;5;241m=\u001b[39m Wav2SeqModel(hparams)\n\u001b[1;32m    316\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger\u001b[38;5;241m=\u001b[39mwandb_logger)\n\u001b[0;32m--> 317\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mSpeechRecognitionDataModule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWAV2VEC2_ARGS\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, trainer\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 608\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[38;5;241m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    643\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    646\u001b[0m     ckpt_path,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    648\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    649\u001b[0m )\n\u001b[0;32m--> 650\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[0;32m-> 1112\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1191\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1204\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_training_routine()\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;66;03m# enable train mode\u001b[39;00m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1276\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m-> 1276\u001b[0m     \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:152\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_dataloaders \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    151\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataloader_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataloader_idx\n\u001b[0;32m--> 152\u001b[0m dl_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_max_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs\u001b[38;5;241m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:137\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_started()\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# lightning module methods\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:234\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The evaluation step (validation_step or test_step depending on the trainer's state).\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m    the outputs of the step\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    233\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 234\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1494\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1494\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mval_step_context():\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, ValidationStep)\n\u001b[0;32m--> 390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/src/training/lyrics_finetune.py:276\u001b[0m, in \u001b[0;36mWav2SeqModel.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28mprint\u001b[39m(label_decoded)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m#print(f\"labels = {labels}\")\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m#print(f\"predicted ids = {predicted_ids}\")\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal transcript = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranscription\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal text = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert_tokenizer\u001b[38;5;241m.\u001b[39mdecode(labels)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert_tokenizer\u001b[38;5;241m.\u001b[39mdecode(labels)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    278\u001b[0m predicted_text \u001b[38;5;241m=\u001b[39m predicted_ids\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mint32)\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/transformers/feature_extraction_utils.py:86\u001b[0m, in \u001b[0;36mBatchFeature.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mIf the key is a string, returns the value of the dict associated to `key` ('input_values', 'attention_mask',\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03metc.).\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndexing with integers is not available when using Python based feature extractors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'transcription'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os. chdir('/home/users/gmenon/workspace/songsLyricsGenerator/src')\n",
    "from training import lyrics_finetune\n",
    "from constants.mir_constants import TrainingArgs, WAV2VEC2_ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565f829-bc48-41db-a714-42028b889662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "hparams = argparse.Namespace()\n",
    "hparams.wav2vec2_model = 'facebook/wav2vec2-large-960h-lv60-self'\n",
    "hparams.lm_model = 'bert-base-uncased' #'facebook/bart-large'\n",
    "hparams.vocab_size = 40000\n",
    "hparams.learning_rate = 1e-6\n",
    "hparams.batch_size = 2\n",
    "\n",
    "model,trainer = lyrics_finetune.run(hparams=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1943c2e9-59c8-46f5-a681-9c96ec6776f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544d3245-2648-4434-baf5-2249682358e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = trainer.datamodule\n",
    "# data.setup()\n",
    "# import torch\n",
    "# torch.Tensor(data.val_dataset['input_values'][0]).unsqueeze(0)\n",
    "# model.generate(torch.Tensor(data.val_dataset['input_values'][0]).unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd01a88-f5e5-467d-b7fe-b019deb2b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.Tensor([6506, 3808,1998, 1996,6506,3808,2058,2058,1012])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2911ae-4a22-4957-b03f-4ba66dfcd017",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a92ef7-b8db-4cbd-90f0-3e991c895322",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [101, 2054, 2842, 2023, 2071, 1005, 2310, 2042,  102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d53c214-9bd2-4bc1-a664-f5efb642ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d42a160-2f59-4890-9040-95d0d430c41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edeeb59-e4fb-4458-9072-4995164f53b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c818fd82-a604-428d-8407-291640c1fea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603d0b8-5ec5-4b92-8d23-ce783df5b183",
   "metadata": {},
   "outputs": [],
   "source": [
    "b  = [6506, 3808, 2003,1998,2042,3808,7316,1999,1012]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4a8aab-5759-49ab-8769-4d6973e21bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_2D_list = [[5, 10, 15, 20],\n",
    "                   [25, 30, 35, 40],\n",
    "                   [45, 50, 55, 60]]\n",
    "list_to_tensor = torch.tensor(example_2D_list)\n",
    "print(\"Our New 2D Tensor from 2D List is: \", list_to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc4b9c-8c94-4c66-b9e4-344d1a6a0c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b95338-4fc7-46bc-b7fc-3f8fa3ef4bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = torch.Tensor([[6506, 3808],\n",
    "        [3808, 3808],\n",
    "        [2003, 1998],\n",
    "        [1998, 1996],\n",
    "        [2042, 6506],\n",
    "        [3808, 3808],\n",
    "        [7316, 2058],\n",
    "        [1999, 2058],\n",
    "        [1012, 1012]]).type(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6578baf2-b5f9-4889-9c1f-96498a615ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = tensors[:,:1].flatten().tolist()\n",
    "list2 = tensors[:,-1].flatten().tolist()\n",
    "list1, list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216744cf-ab2d-42fb-bbaf-e215c03a8602",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345721b-6fb6-4d85-a7bb-97ac07da7e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffe275d-62f2-4ed3-8483-8c0d612068bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.Tensor([[ 101, 2054, 2842, 2023, 2071, 1005, 2310, 2042,  102],\n",
    "        [ 101, 2562, 2115, 2192, 1999, 3067,  102, -100, -100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644480ec-0dd4-493b-b237-92fb3112d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4364896-4d8a-4f68-82a9-53dfff78945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe00d13-4a16-4d87-b099-d9523c4b2a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541180e7-ee15-4c38-9b11-5a19d615e4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3af1ef-24f3-4655-ae66-3d76994c8809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54566220-e690-4eba-a75f-557845db3bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f292ed0a-68b9-4164-9e50-cf3e3a2b6598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9880857-0b09-4be1-be14-5e279a0e9ed1",
   "metadata": {},
   "source": [
    "## TESTING HUGGINGFACE's IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3892980d-3369-4945-82ea-7631c6a86e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02cbdd8cd0dd4d5cae737fe52edf394f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()rocessor_config.json:   0%|          | 0.00/212 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0683f20b284a4c6e8c5059d85f01183c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()okenizer_config.json:   0%|          | 0.00/839 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43d1db188b94b40991377af777b7d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()lve/main/config.json:   0%|          | 0.00/6.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07cf039cd024953a8d70c7983a05d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb6365af83e4b22a9faffa6e3c41dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()cial_tokens_map.json:   0%|          | 0.00/717 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86348b943d104324bfd2968d7e9578fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/3.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the decoder: <class 'transformers.models.mbart.modeling_mbart.MBartForCausalLM'> is overwritten by shared decoder config: MBartConfig {\n",
      "  \"_name_or_path\": \"/home/suraj/projects/mbart-50/hf_models/mbart-50-large-one-to-many/\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_cross_attention\": true,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"MBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"MBart50Tokenizer\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250054\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf650f474fe4a509205e38d14f57161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()neration_config.json:   0%|          | 0.00/294 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6e5536050d47f99714c4cd15474798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f9501562594fa8815cad3381e64892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8646a0d770046899f4ed78ab1dea3aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070fd417cce443b3b82be9a25ee0e249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import SpeechEncoderDecoderModel, AutoProcessor\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-xls-r-300m-en-to-15\")\n",
    "model = SpeechEncoderDecoderModel.from_pretrained(\"facebook/wav2vec2-xls-r-300m-en-to-15\")\n",
    "\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\").input_values\n",
    "# Inference: Translate English speech to German\n",
    "generated = model.generate(input_values)\n",
    "decoded = processor.batch_decode(generated, skip_special_tokens=True)[0]\n",
    "decoded\n",
    "\n",
    "# Training: Train model on English transcription\n",
    "labels = processor(text=ds[0][\"text\"], return_tensors=\"pt\").input_ids\n",
    "\n",
    "loss = model(input_values, labels=labels).loss\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfb6c4c2-6498-4e6e-bc35-99954667b3f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoFeatureExtractor, SpeechEncoderDecoderModel\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      4\u001b[0m encoder_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/wav2vec2-base-960h\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# acoustic model encoder\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1039\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/transformers/utils/import_utils.py:1090\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1089\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m-> 1090\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/transformers/utils/import_utils.py:1089\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1087\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1089\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/transformers/utils/import_utils.py:1099\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1102\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1103\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py:20\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\" Classes to support Speech-Encoder-Text-Decoder architectures\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional, Tuple, Union\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEntropyLoss\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/torch/__init__.py:1239\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m special \u001b[38;5;28;01mas\u001b[39;00m special\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackcompat\u001b[39;00m\n\u001b[0;32m-> 1239\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m onnx \u001b[38;5;28;01mas\u001b[39;00m onnx\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jit \u001b[38;5;28;01mas\u001b[39;00m jit\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg \u001b[38;5;28;01mas\u001b[39;00m linalg\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/torch/onnx/__init__.py:12\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _onnx \u001b[38;5;28;01mas\u001b[39;00m _C_onnx\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_onnx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     _CAFFE2_ATEN_FALLBACK,\n\u001b[1;32m      7\u001b[0m     OperatorExportTypes,\n\u001b[1;32m      8\u001b[0m     TensorProtoDataType,\n\u001b[1;32m      9\u001b[0m     TrainingMode,\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# usort:skip. Keep the order instead of sorting lexicographically\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     _deprecation,\n\u001b[1;32m     14\u001b[0m     errors,\n\u001b[1;32m     15\u001b[0m     symbolic_caffe2,\n\u001b[1;32m     16\u001b[0m     symbolic_helper,\n\u001b[1;32m     17\u001b[0m     symbolic_opset7,\n\u001b[1;32m     18\u001b[0m     symbolic_opset8,\n\u001b[1;32m     19\u001b[0m     symbolic_opset9,\n\u001b[1;32m     20\u001b[0m     symbolic_opset10,\n\u001b[1;32m     21\u001b[0m     symbolic_opset11,\n\u001b[1;32m     22\u001b[0m     symbolic_opset12,\n\u001b[1;32m     23\u001b[0m     symbolic_opset13,\n\u001b[1;32m     24\u001b[0m     symbolic_opset14,\n\u001b[1;32m     25\u001b[0m     symbolic_opset15,\n\u001b[1;32m     26\u001b[0m     symbolic_opset16,\n\u001b[1;32m     27\u001b[0m     symbolic_opset17,\n\u001b[1;32m     28\u001b[0m     symbolic_opset18,\n\u001b[1;32m     29\u001b[0m     utils,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# TODO(After 1.13 release): Remove the deprecated SymbolicContext\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exporter_states\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExportTypes, SymbolicContext\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:779\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:874\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:973\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoFeatureExtractor, SpeechEncoderDecoderModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "encoder_id = \"facebook/wav2vec2-base-960h\"  # acoustic model encoder\n",
    "decoder_id = \"bert-base-uncased\"  # text decoder\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(encoder_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(decoder_id)\n",
    "# Combine pre-trained encoder and pre-trained decoder to form a Seq2Seq model\n",
    "model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_id, decoder_id)\n",
    "\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# load an audio input and pre-process (normalise mean/std to 0/1)\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "input_values = feature_extractor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\").input_values\n",
    "\n",
    "# load its corresponding transcription and tokenize to generate labels\n",
    "labels = tokenizer(ds[0][\"text\"], return_tensors=\"pt\").input_ids\n",
    "\n",
    "# the forward function automatically creates the correct decoder_input_ids\n",
    "loss = model(input_values).loss\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3899ce1-5707-4381-9981-ce9c3646f20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.cls_token_id,tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09855416-6f50-4a4b-8667-d311a544a20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0386, 0.0337, 0.0322,  ..., 0.0070, 0.0095, 0.0169]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8949546b-5e7d-479e-818f-0431b00a1e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 12525, 27565,  2121,  2003,  1996, 20121,  1997,  1996,  2690,\n",
       "          4280,  1998,  2057,  2024,  5580,  2000,  6160,  2010,  8036,   102]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b88539-fb4f-437a-b7b0-1abc3696f966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from datasets.features import Audio\n",
    "from transformers import SpeechEncoderDecoderModel, Wav2Vec2Processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56bf4512-0a15-4407-ad72-cb7071af0f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac3a1103ea14b8eb1b49cb4713b3a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()rocessor_config.json:   0%|          | 0.00/256 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5a03abb04b4b888b5496790ee47d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()okenizer_config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfcd489889fd4c0c9560c186774cf21b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb2994764eb4ce4ab2cc4b93c99437e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d95986b247474e52a2975b27e865a95d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()cial_tokens_map.json:   0%|          | 0.00/880 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e1727652bf440c8a97529fc37fd01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()lve/main/config.json:   0%|          | 0.00/6.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "307aa4dd1b9f41efb1929be55863c252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/3.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "LANG_ID = \"ru\"\n",
    "MODEL_ID = \"bond005/wav2vec2-mbart50-ru\"\n",
    "SAMPLES = 30\n",
    "\n",
    "num_processes = 4\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "model = SpeechEncoderDecoderModel.from_pretrained(MODEL_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9964f298-8e2b-4302-8772-c83fa20e8f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c016606486c14ac5a0acc637972ba3cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/26.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b94de7be5e9456cb8481bbcb888871c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/174k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1439e9747404491394c4fad6be7872cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/62.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/gmenon/.cache/huggingface/modules/datasets_modules/datasets/common_voice/220833898d6a60c50f621126e51fb22eb2dfe5244392c70dccd8e6e2f055f4bf/common_voice.py:634: FutureWarning: \n",
      "            This version of the Common Voice dataset is deprecated.\n",
      "            You can download the latest one with\n",
      "            >>> load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\")\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de234c79ac9d4487903eeab9ed96268d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67bad9ece9074608a2d66295f5cab06e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/15481 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d7439786024e38bae9ad1306037c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/8007 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114efcd104f84cbcb3c58c09952ee244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/7963 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4677fc03ab2346b785324c50a9737442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating other split:   0%|          | 0/10247 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2320bc10414f91a8c262bd70068091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validated split:   0%|          | 0/74256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0ec7f9c4b84bc48415260a50974b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating invalidated split:   0%|          | 0/3056 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "test_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n",
    "\n",
    "if test_dataset.features['audio'].sampling_rate != 16_000:\n",
    "    test_dataset = test_dataset.cast_column(\n",
    "        'audio',\n",
    "        Audio(sampling_rate=16_000)\n",
    "    )\n",
    "\n",
    "audio_data = [test_dataset[i]['audio']['array'] for i in range(SAMPLES)]\n",
    "\n",
    "processed = processor(audio_data, sampling_rate=16_000,\n",
    "                      return_tensors=\"pt\", padding='longest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "067f1f7b-a934-4c63-b188-de0bfbd7d15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:      .\n",
      "Prediction:     .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:      .\n",
      "Prediction:      .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:   .\n",
      "Prediction:  .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:         .\n",
      "Prediction:        .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:        .\n",
      "Prediction:       .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:       .\n",
      "Prediction:      .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:    ,   .\n",
      "Prediction:    ,   .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:       .\n",
      "Prediction:      .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:       .\n",
      "Prediction:      .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:   .\n",
      "Prediction:  .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:            .\n",
      "Prediction:           .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:         .\n",
      "Prediction:        .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:              .\n",
      "Prediction:              .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:         .\n",
      "Prediction:     ,    .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:    .\n",
      "Prediction:   ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:       .\n",
      "Prediction:       .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:   ,    .\n",
      "Prediction: , ,    .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:      .\n",
      "Prediction:     .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:  ,  ,  , .\n",
      "Prediction:   ,   ?   ?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:       .\n",
      "Prediction:      .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:         .\n",
      "Prediction:        .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:       ?\n",
      "Prediction:      ?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:   ,     .\n",
      "Prediction:  ,     .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:        .\n",
      "Prediction:       .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:   Microsoft          .\n",
      "Prediction:    ,         .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:    ,       .\n",
      "Prediction:   ,       .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:            .\n",
      "Prediction:    ,     .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:         .\n",
      "Prediction:        .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:   ,     .\n",
      "Prediction:       .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reference:           .\n",
      "Prediction:          .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    predicted_ids = model.generate(**processed)\n",
    "\n",
    "predicted_sentences = processor.batch_decode(\n",
    "    predicted_ids,\n",
    "    num_processes=num_processes,\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for i, predicted_sentence in enumerate(predicted_sentences):\n",
    "        print(\"-\" * 100)\n",
    "        print(\"Reference: \", test_dataset[i][\"sentence\"])\n",
    "        print(\"Prediction:\", predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ef0a005-9e5a-488b-a9a0-6d57153ad408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b64793a-efaa-407c-94ea-80ece2282045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Folder</th>\n",
       "      <th>consolidated_file_path</th>\n",
       "      <th>Length</th>\n",
       "      <th>transcription</th>\n",
       "      <th>transcription_capitalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0aa835068aa64515b2158c2f0b6513b7</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>4.754943</td>\n",
       "      <td>so learn from your mistakes</td>\n",
       "      <td>SO LEARN FROM YOUR MISTAKES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>089cd84d662e4c50954cfa46c07cae14</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.759388</td>\n",
       "      <td>i've been connected to the right line</td>\n",
       "      <td>I'VE BEEN CONNECTED TO THE RIGHT LINE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>127cc2aaff9a44fab21b2a0c4f5c3e5d</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.668254</td>\n",
       "      <td>the truth to be found</td>\n",
       "      <td>THE TRUTH TO BE FOUND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18e03c3980014046a3504ef42fe81ae8</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.895079</td>\n",
       "      <td>he said the way myblue eyes shined</td>\n",
       "      <td>HE SAID THE WAY MYBLUE EYES SHINED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07c8b8c0b0424c1e87389392695a551b</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.666939</td>\n",
       "      <td>you leave me once again home alone</td>\n",
       "      <td>YOU LEAVE ME ONCE AGAIN HOME ALONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0cb795c9b6e64a4abab5553c2b2ed3e0</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.742880</td>\n",
       "      <td>while they are in commend</td>\n",
       "      <td>WHILE THEY ARE IN COMMEND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0655395364614218be31548245853ca0</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>4.697710</td>\n",
       "      <td>a life all mine</td>\n",
       "      <td>A LIFE ALL MINE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>113bb605eb324aeb8db046b4193424a2</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.642948</td>\n",
       "      <td>so i never went back</td>\n",
       "      <td>SO I NEVER WENT BACK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00ecd4b71c094b959ae492ec138bb555</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>3.176508</td>\n",
       "      <td>in you i taste god</td>\n",
       "      <td>IN YOU I TASTE GOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0b5fa72c054e44598ec8aefb76ff855e</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.465011</td>\n",
       "      <td>could stay a while</td>\n",
       "      <td>COULD STAY A WHILE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Folder  \\\n",
       "0  0aa835068aa64515b2158c2f0b6513b7   \n",
       "1  089cd84d662e4c50954cfa46c07cae14   \n",
       "2  127cc2aaff9a44fab21b2a0c4f5c3e5d   \n",
       "3  18e03c3980014046a3504ef42fe81ae8   \n",
       "4  07c8b8c0b0424c1e87389392695a551b   \n",
       "5  0cb795c9b6e64a4abab5553c2b2ed3e0   \n",
       "6  0655395364614218be31548245853ca0   \n",
       "7  113bb605eb324aeb8db046b4193424a2   \n",
       "8  00ecd4b71c094b959ae492ec138bb555   \n",
       "9  0b5fa72c054e44598ec8aefb76ff855e   \n",
       "\n",
       "                              consolidated_file_path    Length  \\\n",
       "0  /scratch/users/gmenon/wav_clips/separated/htde...  4.754943   \n",
       "1  /scratch/users/gmenon/wav_clips/separated/htde...  2.759388   \n",
       "2  /scratch/users/gmenon/wav_clips/separated/htde...  2.668254   \n",
       "3  /scratch/users/gmenon/wav_clips/separated/htde...  2.895079   \n",
       "4  /scratch/users/gmenon/wav_clips/separated/htde...  2.666939   \n",
       "5  /scratch/users/gmenon/wav_clips/separated/htde...  2.742880   \n",
       "6  /scratch/users/gmenon/wav_clips/separated/htde...  4.697710   \n",
       "7  /scratch/users/gmenon/wav_clips/separated/htde...  2.642948   \n",
       "8  /scratch/users/gmenon/wav_clips/separated/htde...  3.176508   \n",
       "9  /scratch/users/gmenon/wav_clips/separated/htde...  2.465011   \n",
       "\n",
       "                           transcription  \\\n",
       "0            so learn from your mistakes   \n",
       "1  i've been connected to the right line   \n",
       "2                  the truth to be found   \n",
       "3     he said the way myblue eyes shined   \n",
       "4     you leave me once again home alone   \n",
       "5              while they are in commend   \n",
       "6                        a life all mine   \n",
       "7                   so i never went back   \n",
       "8                     in you i taste god   \n",
       "9                     could stay a while   \n",
       "\n",
       "               transcription_capitalized  \n",
       "0            SO LEARN FROM YOUR MISTAKES  \n",
       "1  I'VE BEEN CONNECTED TO THE RIGHT LINE  \n",
       "2                  THE TRUTH TO BE FOUND  \n",
       "3     HE SAID THE WAY MYBLUE EYES SHINED  \n",
       "4     YOU LEAVE ME ONCE AGAIN HOME ALONE  \n",
       "5              WHILE THEY ARE IN COMMEND  \n",
       "6                        A LIFE ALL MINE  \n",
       "7                   SO I NEVER WENT BACK  \n",
       "8                     IN YOU I TASTE GOD  \n",
       "9                     COULD STAY A WHILE  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df = pd.read_csv(WAV2VEC2_ARGS.TEST_FILE_PATH).head(10)\n",
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5a069d0-7df7-454a-b738-8756510565c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor, BertTokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f95a8161-b007-4da7-bb81-3a4c103eab4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('so learn from your mistakes', [101, 2061, 4553, 2013, 2115, 12051, 102]),\n",
       " (\"i've been connected to the right line\",\n",
       "  [101, 1045, 1005, 2310, 2042, 4198, 2000, 1996, 2157, 2240, 102]),\n",
       " ('the truth to be found', [101, 1996, 3606, 2000, 2022, 2179, 102]),\n",
       " ('he said the way myblue eyes shined',\n",
       "  [101, 2002, 2056, 1996, 2126, 2026, 16558, 5657, 2159, 12342, 2094, 102]),\n",
       " ('you leave me once again home alone',\n",
       "  [101, 2017, 2681, 2033, 2320, 2153, 2188, 2894, 102]),\n",
       " ('while they are in commend',\n",
       "  [101, 2096, 2027, 2024, 1999, 4012, 3549, 2094, 102]),\n",
       " ('a life all mine', [101, 1037, 2166, 2035, 3067, 102]),\n",
       " ('so i never went back', [101, 2061, 1045, 2196, 2253, 2067, 102]),\n",
       " ('in you i taste god', [101, 1999, 2017, 1045, 5510, 2643, 102]),\n",
       " ('could stay a while', [101, 2071, 2994, 1037, 2096, 102])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a for a in zip(list(validation_df[\"transcription\"]),bert_tokenizer(list(validation_df[\"transcription\"])).input_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1a216e7-276c-45cc-91f7-401946441a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he said the way myblue eyes shined'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.decode(bert_tokenizer(list(validation_df[\"transcription\"])).input_ids[3],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec7d0394-1291-433f-935c-fbe850890b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'so learn from your mistakes'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(validation_df[\"transcription\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d4025-cde5-48d9-ac08-294a5ec63ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
