{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46358ffa-ac47-48c6-917e-54d7918483d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os. chdir('/home/users/gmenon/workspace/songsLyricsGenerator/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3852a9e9-9f11-40e7-be68-d372a242db5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_lightning import LightningModule\n",
    "import pytorch_lightning as pl\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Model\n",
    "from constants.mir_constants import TrainingArgs, WAV2VEC2_ARGS\n",
    "from training import lyrics_finetune\n",
    "from dataclasses import dataclass, asdict\n",
    "import json\n",
    "import argparse\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23a1124d-49d5-4576-b8f2-f775c7239d84",
   "metadata": {},
   "source": [
    "from flash.audio import SpeechRecognitionData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b62468-9929-43e3-8aeb-7daaeb74b47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(asdict(WAV2VEC2_ARGS), indent = 4))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "690c412e-6b90-431e-98d9-1dfac1e94f98",
   "metadata": {},
   "source": [
    "datamodule = SpeechRecognitionData.from_csv(\"consolidated_file_path\",\n",
    "                                                         \"transcription_capitalized\",\n",
    "                                                         train_file=WAV2VEC2_ARGS.TRAIN_FILE_PATH,\n",
    "                                                         test_file=WAV2VEC2_ARGS.TEST_FILE_PATH,\n",
    "                                                         batch_size=WAV2VEC2_ARGS.BATCH_SIZE\n",
    "                                                         )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0809f82-32b0-4995-bdb0-08779a8c0211",
   "metadata": {},
   "source": [
    "datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831d6096-d0e4-4d39-8828-57e3a4a208cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flash\n",
    "import torchaudio\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# class SpeechDataModule(pl.LightningDataModule):\n",
    "#     def __init__(self, args: WAV2VEC2_ARGS):\n",
    "#         super().__init__()\n",
    "#         self.batch_size = 2\n",
    "\n",
    "#     def prepare_data(self):\n",
    "#         # Download and extract the audio files and transcripts, if necessary.\n",
    "#         pass\n",
    "\n",
    "#     def setup(self, stage=None):\n",
    "#         # Create the dataset\n",
    "#         # datamodule = \n",
    "#         self.dataset = SpeechRecognitionData.from_csv(\"consolidated_file_path\",\n",
    "#                                                          \"transcription_capitalized\",\n",
    "#                                                          train_file=WAV2VEC2_ARGS.TRAIN_FILE_PATH,\n",
    "#                                                          test_file=WAV2VEC2_ARGS.TEST_FILE_PATH,\n",
    "#                                                          batch_size=2\n",
    "#                                                          )\n",
    "\n",
    "#         # Create the dataloader\n",
    "#         self.train_dataloader = self.dataset.train_dataloader\n",
    "#         # torch.utils.data.DataLoader(\n",
    "#         #     self.dataset,\n",
    "#         #     batch_size=self.batch_size,\n",
    "#         #     shuffle=True,\n",
    "#         #     num_workers=4,\n",
    "#         # )\n",
    "#         self.val_dataloader = self.dataset.val_dataloader\n",
    "#         # torch.utils.data.DataLoader(\n",
    "#         #     self.dataset,\n",
    "#         #     batch_size=self.batch_size,\n",
    "#         #     shuffle=False,\n",
    "#         #     num_workers=4,\n",
    "#         # )\n",
    "\n",
    "#     def train_dataloader(self):\n",
    "#         return self.train_dataloader\n",
    "\n",
    "#     def val_dataloader(self):\n",
    "#         return self.val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3354031-3fb6-46ee-9fed-c3c06d23348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DALIDataset(pl.LightningDataModule):\n",
    "#     def __init__(self, batch_size: int = 4,\n",
    "#                   train_path :Optional[str] = None,\n",
    "#                     validation_path: Optional[str] = None,\n",
    "#                       model_backbone: pl.LightningModule = None,\n",
    "#                       args: TrainingArgs = WAV2VEC2_ARGS\n",
    "#                       ):\n",
    "        \n",
    "#         super().__init__()\n",
    "#         self.train_path = train_path if train_path is not None else args.TRAIN_FILE_PATH\n",
    "#         self.validation_path = validation_path if validation_path is not None else args.TEST_FILE_PATH\n",
    "#         self.model_backbone = model_backbone if model_backbone is not None else args.MODEL_BACKBONE\n",
    "\n",
    "#         def prepare_data(self):\n",
    "#             pass\n",
    "        \n",
    "#         def setup(self):\n",
    "#             train_df = pd.read_csv(WAV2VEC2_ARGS.TRAIN_FILE_PATH) \n",
    "#             validation_df = pd.read_csv(WAV2VEC2_ARGS.TEST_FILE_PATH)\n",
    "#             songs_metadata = pd.concat([train_df,validation_df], ignore_index = True)\n",
    "#             audio_dataset = Dataset.from_dict(\n",
    "#                 {\"audio\": list(songs_metadata[\"file_name\"]),\n",
    "#                  \"transcription\": list(songs_metadata[\"transcription\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "#             audio_dataset[\"transcription\"] = audio_dataset[\"transcription\"] = re.sub(WAV2VEC2_ARGS.CHARS_TO_REMOVE_FROM_TRANSCRIPTS, '', audio_dataset[\"transcription\"]).upper()\n",
    "#             audio_dataset = audio_dataset.train_test_split(test_size=0.2, shuffle=True)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a239b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n",
    "from datasets import load_dataset,Dataset,Audio\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# class SpeechRecognitionDataset(Dataset):\n",
    "#     def __init__(self, dataset,args:TrainingArgs,processor):\n",
    "#         self.model_backbone = args.MODEL_BACKBONE\n",
    "#         self.dataset = dataset\n",
    "#         self.processor = processor\n",
    "#         self.tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(self.model_backbone)\n",
    "#         self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(self.model_backbone)\n",
    "#         self.processor = Wav2Vec2Processor(feature_extractor=self.feature_extractor, tokenizer=self.tokenizer)\n",
    "        \n",
    "#     def __getitem__(self, index):\n",
    "#         audio = self.dataset[index][\"audio\"][0]\n",
    "#         transcription = self.dataset[index][\"transcription\"][0]\n",
    "\n",
    "#         input_values = self.processor(audio[\"array\"], sampling_rate=16_000).input_values[0]\n",
    "#         with self.processor.as_target_processor():\n",
    "#             print(\"Entering the label encoder\")\n",
    "#             labels = self.processor(transcription,return_tensors = 'pt').input_ids\n",
    "#        # attention_mask = inputs.attention_mask.squeeze()\n",
    "#         return input_values, labels\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8e8b187-08b6-4e27-b3ec-7e1da0ad5ead",
   "metadata": {},
   "source": [
    "class SpeechRecognitionDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, WAV2VEC2_ARGS: WAV2VEC2_ARGS, num_workers):\n",
    "        super().__init__()\n",
    "        self.batch_size = WAV2VEC2_ARGS.BATCH_SIZE\n",
    "        self.num_workers = num_workers\n",
    "        self.tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(WAV2VEC2_ARGS.MODEL_BACKBONE)\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(WAV2VEC2_ARGS.MODEL_BACKBONE)\n",
    "        self.processor = Wav2Vec2Processor(feature_extractor=self.feature_extractor, tokenizer=self.tokenizer)\n",
    "        self.cpu_count = 1\n",
    "        print(f\"CPU Count = {self.cpu_count}\")\n",
    "        self.padding: Union[bool, str] = True\n",
    "        self.max_length: Optional[int] = None\n",
    "        self.max_length_labels: Optional[int] = None\n",
    "        self.pad_to_multiple_of: Optional[int] = None\n",
    "        self.pad_to_multiple_of_labels: Optional[int] = None\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        train_df = pd.read_csv(WAV2VEC2_ARGS.TRAIN_FILE_PATH).head(10)\n",
    "        validation_df = pd.read_csv(WAV2VEC2_ARGS.TEST_FILE_PATH).head(10)\n",
    "        #songs_metadata = pd.concat([train_df,validation_df], ignore_index = True)\n",
    "        if stage == 'fit' or stage is None:\n",
    "            print(\"In Stage = Fit\")\n",
    "            train_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(train_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(train_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "            \n",
    "            val_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(validation_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(validation_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "            \n",
    "            train_dataset = train_dataset.map(self.transform_huggingface_dataset, num_proc = self.cpu_count)\n",
    "            train_dataset =train_dataset.remove_columns(['audio','transcription','input_length'])\n",
    "            self.train_dataset = train_dataset.map(self.data_collate, num_proc = self.cpu_count)\n",
    "\n",
    "            \n",
    "            val_dataset = val_dataset.map(self.transform_huggingface_dataset, num_proc = self.cpu_count,)\n",
    "            val_dataset =val_dataset.remove_columns(['audio','transcription','input_length'])\n",
    "            self.val_dataset = val_dataset.map(self.data_collate, num_proc = self.cpu_count)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            print(\"In Stage = Test\")\n",
    "            val_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(validation_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(validation_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "          #  val_dataset[\"transcription\"] = val_dataset[\"transcription\"] = re.sub(WAV2VEC2_ARGS.CHARS_TO_REMOVE_FROM_TRANSCRIPTS, '', val_dataset[\"transcription\"]).upper()\n",
    "            val_dataset = val_dataset.map(self.transform_huggingface_dataset, num_proc = self.cpu_count)\n",
    "            #self.test_dataset = SpeechRecognitionDataset(val_dataset,WAV2VEC2_ARGS,self.processor)\n",
    "            self.test_dataset = val_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        print(\"entering train data loader\")\n",
    "        return DataLoader(\n",
    "            self.train_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        print(\"entering val data loader\")\n",
    "        return DataLoader(\n",
    "            self.val_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        print(\"entering test data loader\")\n",
    "        return DataLoader(\n",
    "            self.test_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "    \n",
    "    def transform_huggingface_dataset(self, batch: Dataset) -> Dataset:\n",
    "        audio = batch[\"audio\"]\n",
    "        batch[\"input_values\"] = self.processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "        batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "        with self.processor.as_target_processor():\n",
    "            batch[\"labels\"] = self.processor(batch[\"transcription\"]).input_ids\n",
    "        return batch\n",
    "\n",
    "    def data_collate(self, batch: Dataset) :\n",
    "        input_features = [{\"input_values\": batch[\"input_values\"]}]\n",
    "        label_features = [{\"input_ids\": batch[\"labels\"]}]\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )      \n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ed33833-367d-48be-a594-389fa4aef1d8",
   "metadata": {},
   "source": [
    "data = SpeechRecognitionDataModule(WAV2VEC2_ARGS, num_workers = 0)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d7bd241-6c23-48c0-a557-fd3cd663add3",
   "metadata": {},
   "source": [
    "data.setup()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "64526ed0-9e12-4065-b4d3-4e484d573e92",
   "metadata": {},
   "source": [
    "next(iter(data.val_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bc4a02-0e0b-4462-bb7b-79e70cbec785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96306ffe-8df9-4e84-a994-d7d92f614a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Union, List, Dict\n",
    "# import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "# from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "\n",
    "# class DataCollatorCTCWithPadding:\n",
    "#     \"\"\"\n",
    "#     Data collator that will dynamically pad the inputs received.\n",
    "#     Args:\n",
    "#         processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "#             The processor used for proccessing the data.\n",
    "#         padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "#             Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "#             among:\n",
    "#             * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "#               sequence if provided).\n",
    "#             * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "#               maximum acceptable input length for the model if that argument is not provided.\n",
    "#             * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "#               different lengths).\n",
    "#         max_length (:obj:`int`, `optional`):\n",
    "#             Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "#         max_length_labels (:obj:`int`, `optional`):\n",
    "#             Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "#         pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "#             If set will pad the sequence to a multiple of the provided value.\n",
    "#             This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "#             7.5 (Volta).\n",
    "#     \"\"\"\n",
    "\n",
    "#     processor: Wav2Vec2Processor\n",
    "#     padding: Union[bool, str] = True\n",
    "#     max_length: Optional[int] = None\n",
    "#     max_length_labels: Optional[int] = None\n",
    "#     pad_to_multiple_of: Optional[int] = None\n",
    "#     pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "#     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "#         # split inputs and labels since they have to be of different lengths and need\n",
    "#         # different padding methods\n",
    "#         input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "#         label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "#         batch = self.processor.pad(\n",
    "#             input_features,\n",
    "#             padding=self.padding,\n",
    "#             max_length=self.max_length,\n",
    "#             pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "#             return_tensors=\"pt\",\n",
    "#         )\n",
    "#         with self.processor.as_target_processor():\n",
    "#             labels_batch = self.processor.pad(\n",
    "#                 label_features,\n",
    "#                 padding=self.padding,\n",
    "#                 max_length=self.max_length_labels,\n",
    "#                 pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "#                 return_tensors=\"pt\",\n",
    "#             )\n",
    "\n",
    "#         # replace padding with -100 to ignore loss correctly\n",
    "#         labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "#         batch[\"labels\"] = labels\n",
    "\n",
    "#         return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cdad31-2268-4085-a6d4-6f86eb7558c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer, BertModel\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "# text = \"Replace me by any text you'd like.\"\n",
    "# encoded_input = tokenizer(text, return_tensors='pt')\n",
    "# output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c217f6c-0f28-48c6-92b0-0ca5755253b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Dict, List\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        label_attention_features =[{\"input_ids\": feature[\"label_attention_mask\"]} for feature in features]\n",
    "        attention_features =[{\"input_values\": feature[\"attention_mask\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\")\n",
    "\n",
    "        batch_attention = self.processor.pad(\n",
    "            attention_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\")\n",
    "    \n",
    "            \n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        with self.processor.as_target_processor():\n",
    "            label_attention_batch = self.processor.pad(\n",
    "                label_attention_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "                \n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels_attention = label_attention_batch[\"input_ids\"].masked_fill(labels_batch.input_ids.eq(101), 0).masked_fill(labels_batch.input_ids.eq(102), 0)\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        \n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        batch[\"label_attention_masks\"] = labels_attention\n",
    "        batch[\"attention_mask\"] = batch_attention[\"input_values\"]\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5405a62-6cc2-4291-b105-4f2f9c7aaa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor,BertTokenizer\n",
    "from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC, AutoModel,AutoProcessor\n",
    "\n",
    "from datasets import load_dataset,Dataset,Audio\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "WAV2VEC2_ARGS.BATCH_SIZE = 2\n",
    "class SpeechRecognitionDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, WAV2VEC2_ARGS: WAV2VEC2_ARGS, num_workers):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.batch_size = WAV2VEC2_ARGS.BATCH_SIZE\n",
    "        self.num_workers = num_workers\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(hparams.wav2vec2_model)\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        #self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(WAV2VEC2_ARGS.MODEL_BACKBONE)\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "        #self.feature_extractor = AutoFeatureExtractor.from_pretrained(hparams.wav2vec2_model)\n",
    "        self.processor = Wav2Vec2Processor(feature_extractor=self.feature_extractor, tokenizer=self.bert_tokenizer)\n",
    "        #self.processor = AutoProcessor(feature_extractor=self.feature_extractor, tokenizer=self.bert_tokenizer)\n",
    "        #self.processor = WhisperProcessor(feature_extractor=self.feature_extractor, tokenizer=self.bert_tokenizer)\n",
    "        self.data_collator = DataCollatorCTCWithPadding(processor=self.processor, padding=True)\n",
    "        self.cpu_count = 4\n",
    "        print(f\"CPU Count = {self.cpu_count}\")\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        train_df = pd.read_csv(WAV2VEC2_ARGS.TRAIN_FILE_PATH).head(10)\n",
    "        validation_df = pd.read_csv(WAV2VEC2_ARGS.TEST_FILE_PATH).head(10)\n",
    "        if stage == 'fit' or stage is None:\n",
    "            print(\"In Stage = Fit\")\n",
    "            train_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(train_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(train_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "            self.train_dataset = train_dataset.map(self.prepare_dataset,remove_columns = train_dataset.column_names)\n",
    "            \n",
    "            val_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(validation_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(validation_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "            self.val_dataset = val_dataset.map(self.prepare_dataset,remove_columns = val_dataset.column_names)\n",
    "\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            print(\"In Stage = Test\")\n",
    "            test_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(validation_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(validation_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "            test_dataset = val_dataset.map(self.prepare_dataset,remove_columns = val_dataset.column_names)\n",
    "            self.test_dataset = test_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        print(\"entering train data loader\")\n",
    "        return DataLoader(\n",
    "            self.train_dataset.with_format(\"torch\"), \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn = self.data_collator\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        print(\"entering val data loader\")\n",
    "        return DataLoader(\n",
    "            self.val_dataset.with_format(\"torch\"), \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn = self.data_collator\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        print(\"entering test data loader\")\n",
    "        return DataLoader(\n",
    "            self.test_dataset.with_format(\"torch\"), \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn = self.data_collator\n",
    "        )\n",
    "\n",
    "    def prepare_dataset(self,batch):\n",
    "        audio = batch[\"audio\"]\n",
    "        # batched output is \"un-batched\" to ensure mapping is correct\n",
    "        batch[\"input_values\"] = self.processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "        batch[\"attention_mask\"] = self.processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).attention_mask[0]\n",
    "        batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "        with self.processor.as_target_processor():\n",
    "            batch[\"labels\"] = self.processor(batch[\"transcription\"]).input_ids\n",
    "            batch[\"label_attention_mask\"] = self.processor(batch[\"transcription\"]).attention_mask\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "raw",
   "id": "75fe7b63-806e-46e7-8007-6223196529a5",
   "metadata": {},
   "source": [
    "data = SpeechRecognitionDataModule(WAV2VEC2_ARGS, num_workers = 0)\n",
    "data.setup()\n",
    "data.val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44155934-62d0-4943-afb7-9891bbd11a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.val_dataset[0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d90ec0e4-4a82-4f5b-a545-14a2be55dd00",
   "metadata": {},
   "source": [
    "print(next(iter(data.val_dataloader())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3d9597-6db9-48ff-b8db-4fe73bef6ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Model, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC, AutoModel\n",
    "\n",
    "class Wav2SeqModel(LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.batch_size = hparams.batch_size\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.wav2vec2 = AutoModelForCTC.from_pretrained(hparams.wav2vec2_model,ctc_zero_infinity=True,ctc_loss_reduction=\"mean\")\n",
    "        self.wav2vec2.eval()\n",
    "        #self.wav2vec2 = AutoModel.from_pretrained(hparams.wav2vec2_model)\n",
    "        #self.wav2vec2.eval()\n",
    "        print(self.wav2vec2.config)\n",
    "        self.seq2seq = AutoModelForCausalLM.from_pretrained(hparams.lm_model)\n",
    "        self.seq2seq.config.is_decoder = True\n",
    "        self.seq2seq.add_cross_attention = True\n",
    "        print(self.seq2seq.config)\n",
    "\n",
    "    def forward(self, audio, attention_mask, labels, label_attention_mask):\n",
    "        #print(\"entering forward step\")\n",
    "        encoder_outputs = self.wav2vec2(audio[0],\n",
    "                                        attention_mask=attention_mask, #Made a change here by adding a [0]\n",
    "                                        output_hidden_states=True,\n",
    "                                        output_attentions=True)\n",
    "        encoder_hidden_states = encoder_outputs[0]  \n",
    "        print(f\"encoder_hidden_states={encoder_hidden_states}\")\n",
    "        encoder_attention_mask = self.wav2vec2._get_feature_vector_attention_mask(\n",
    "                encoder_hidden_states.shape[1], attention_mask\n",
    "            )\n",
    "        print(f\"[PRIOR]decoder_input_ids={labels}\")\n",
    "        print(f\"[PRIOR]decoder_attention_masks={label_attention_mask}\")\n",
    "        decoder_input_ids = self.shift_tokens_right(labels, 0, 101)\n",
    "        decoder_attention_masks = self.shift_tokens_right_mask(label_attention_mask)\n",
    "        print(f\"[AFTER]decoder_input_ids={decoder_input_ids}\")\n",
    "        print(f\"[AFTER]decoder_attention_masks={decoder_attention_masks}\")\n",
    "        decoder_outputs = self.seq2seq(input_ids=decoder_input_ids,\n",
    "                                       attention_mask = decoder_attention_masks,\n",
    "                                       encoder_hidden_states=encoder_hidden_states,\n",
    "                                      encoder_attention_mask=encoder_attention_mask) #,attention_mask = x.attentions, decoder_input_ids=predicted_ids\n",
    "        return decoder_outputs\n",
    "\n",
    "    def generate(self, audio):\n",
    "        encoder_outputs = self.wav2vec2(audio[0],output_hidden_states=True,output_attentions=True)\n",
    "        encoder_hidden_states = encoder_outputs[0]\n",
    "        bos_ids = (\n",
    "            torch.ones(\n",
    "                (encoder_hidden_states.size()[0], 1),\n",
    "                dtype=torch.long,\n",
    "                device=self.seq2seq.device,\n",
    "            )\n",
    "            * self.seq2seq.config.pad_token_id\n",
    "        )\n",
    "        return self.seq2seq.generate(\n",
    "            input_ids=bos_ids,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        #print(\"entering training step\")\n",
    "        audio = batch[\"input_values\"].unsqueeze(0)\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        label_attention_mask = batch[\"label_attention_masks\"].unsqueeze(0)\n",
    "        #print(f\"audio={audio}, attention_mask = {attention_mask}, label_attention_mask = {label_attention_mask}\")\n",
    "        labels = batch[\"labels\"]\n",
    "        labels = labels.reshape(self.batch_size,-1)\n",
    "        logits = self(audio,attention_mask,labels,label_attention_mask).logits\n",
    "        logits = logits.reshape(-1, self.batch_size, self.seq2seq.config.vocab_size)\n",
    "        input_lengths = torch.full(size=(self.batch_size,), fill_value=logits.shape[0], dtype=torch.long)\n",
    "        target_lengths = torch.full(size=(self.batch_size,), fill_value=labels.shape[0], dtype=torch.long)\n",
    "        # ctc_loss =  nn.CTCLoss(blank=0)\n",
    "        # loss = ctc_loss(logits,labels,input_lengths,target_lengths)\n",
    "        ce_loss = nn.CrossEntropyLoss()\n",
    "        loss = ce_loss(logits, labels)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch,batch_idx):\n",
    "        #print(\"entering validation step\")\n",
    "        audio = batch[\"input_values\"].unsqueeze(0)\n",
    "        labels = batch[\"labels\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        label_attention_mask = batch[\"label_attention_masks\"].unsqueeze(0)\n",
    "        #print(f\"audio={audio}, attention_mask = {attention_mask}, label_attention_mask = {label_attention_mask}\")\n",
    "        labels = labels.reshape(self.batch_size,-1)\n",
    "        logits = self(audio,attention_mask,labels,label_attention_mask).logits\n",
    "        logits = logits.reshape(-1, self.batch_size, self.seq2seq.config.vocab_size)\n",
    "        input_lengths = torch.full(size=(self.batch_size,), fill_value=logits.shape[0], dtype=torch.long)\n",
    "        target_lengths = torch.full(size=(self.batch_size,), fill_value=labels.shape[0], dtype=torch.long)\n",
    "        # ctc_loss =  nn.CTCLoss(blank=0)\n",
    "        # loss = ctc_loss(logits,labels,input_lengths,target_lengths)\n",
    "        ce_loss = nn.CrossEntropyLoss()\n",
    "        loss = ce_loss(logits, labels)\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        label_decoded = labels.type(torch.int64).tolist()\n",
    "        #print(f\"labels = {labels}\")\n",
    "        #print(f\"predicted ids = {predicted_ids}\")\n",
    "        print(f\"original text = {self.bert_tokenizer.decode(label_decoded[0])},{self.bert_tokenizer.decode(label_decoded[1])}\")\n",
    "        predicted_text = predicted_ids.type(torch.int64)\n",
    "        print(f\"Predicted text = {self.bert_tokenizer.decode(predicted_text[:,0].flatten().tolist())},{self.bert_tokenizer.decode(predicted_text[:,-1].flatten().tolist())}\")\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=hparams.learning_rate)\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
    "        \"\"\"\n",
    "        Shift input ids one token to the right.\n",
    "        \"\"\"\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "        if decoder_start_token_id is None:\n",
    "            raise ValueError(\"Make sure to set the decoder_start_token_id attribute of the model's configuration.\")\n",
    "        shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "    \n",
    "        if pad_token_id is None:\n",
    "            raise ValueError(\"Make sure to set the pad_token_id attribute of the model's configuration.\")\n",
    "        # replace possible -100 values in labels by `pad_token_id`\n",
    "        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "    \n",
    "        return shifted_input_ids\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_tokens_right_mask(input_ids: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Shift input ids one token to the right.\n",
    "        \"\"\"\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[:,:, 1:] = input_ids[:,:, :-1].clone()\n",
    "        shifted_input_ids[:,:, 0] = 0\n",
    "        return shifted_input_ids.squeeze()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1365d597-7bd4-420e-8077-b021455357df",
   "metadata": {},
   "source": [
    "test_data = SpeechRecognitionDataModule(WAV2VEC2_ARGS, num_workers = 0)\n",
    "test_data.setup()\n",
    "print(next(iter(test_data.val_dataloader())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93bcca1-79eb-4f51-9dd3-368e192fcfcd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pytorch_lightning as pl\n",
    "hparams = argparse.Namespace()\n",
    "hparams.wav2vec2_model = 'facebook/wav2vec2-large-960h-lv60-self' #'openai/whisper-base'# #'openai/whisper-base'\n",
    "hparams.lm_model = 'bert-base-uncased' #'facebook/bart-large'\n",
    "hparams.vocab_size = 30000\n",
    "hparams.learning_rate = 0.00001\n",
    "hparams.batch_size = WAV2VEC2_ARGS.BATCH_SIZE\n",
    "\n",
    "model = Wav2SeqModel(hparams)\n",
    "trainer = Trainer(max_epochs=10,devices=1, accelerator=\"gpu\")\n",
    "trainer.fit(model,SpeechRecognitionDataModule(WAV2VEC2_ARGS,num_workers=4))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3fa8d2a-177c-40b8-b166-108a04b7c4ae",
   "metadata": {},
   "source": [
    "import librosa \n",
    "def create_audio_tensor(audio_path):\n",
    "    audio, sample_rate = librosa.load(audio_path)\n",
    "    audio_tensor = torch.from_numpy(audio).float()\n",
    "    audio_tensor = audio_tensor.unsqueeze(0)\n",
    "    return audio_tensor\n",
    "audio_tensor = create_audio_tensor(\"notebooks/separated/mdx_extra/test_clip/vocals.wav\")\n",
    "print(audio_tensor.unsqueeze(0).shape)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_hat = model.generate(audio_tensor.unsqueeze(0))\n",
    "torch.argmax(y_hat, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac5538f-c4f3-4c19-a729-26f38aff7b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f92b482-a046-4c0f-bcd3-2ca40b2fc33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_tensor = next(iter(test_data.val_dataloader()))[\"input_values\"]\n",
    "audio_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9938cd6-0d0e-4914-89cd-9772ac41a9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_hat = model.generate(audio_tensor.unsqueeze(0))\n",
    "torch.argmax(y_hat, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3838261-9da4-46a1-8a35-543a359e81cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import librosa\n",
    "# import torch\n",
    "# from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "# def create_audio_tensor(audio_path):\n",
    "#     audio, sample_rate = librosa.load(audio_path)\n",
    "#     audio_tensor = torch.from_numpy(audio).float()\n",
    "#     audio_tensor = audio_tensor.unsqueeze(0)\n",
    "#     return audio_tensor\n",
    "\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "# audio_tensor = create_audio_tensor(\"notebooks/separated/mdx_extra/test_clip/vocals.wav\")\n",
    "logits = model(audio_tensor.unsqueeze(0),torch.Tensor(18, 11,  0,  0,  0,  0,  0,  7,  0,  0,  0,  0,  6,  0,  0,  0,  4,  4,\n",
    "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  5,  5,  0,  0,  0,\n",
    "          0, 15, 15,  0,  0,  0,  0,  0,  0, 12,  0,  0,  0,  4,  4,  0,  0,  0,\n",
    "         14,  0,  0,  0,  0, 10,  0,  0, 12, 12,  0,  0,  0,  0,  0, 19,  0,  0,\n",
    "          0,  0,  0,  0,  0,  0,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
    "          0,  0,  0,  0,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  6, 11,\n",
    "          0,  0,  5,  0,  0,  0,  4,  4,  0,  0,  0, 24,  0,  0,  0,  0,  0,  0,\n",
    "          0,  0,  0,  5,  0,  0,  0,  0,  0,  9,  0,  0,  0,  0,  4,  4,  4,  0,\n",
    "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
    "          0,  0,  0))\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "print(audio_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c2fb4-ab29-433a-ab4f-5e895a1aaf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e40856-59d1-4d5e-84c3-ab64b528f23c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96624e0-53a7-47c0-bb31-07da5ad31208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4686fb-f6f8-4b08-af94-a9d18762a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.LogSoftmax(dim=1)\n",
    "input = torch.randn(2, 3)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa479c4-9660-4ee0-b8f1-7cabac8d3480",
   "metadata": {},
   "outputs": [],
   "source": [
    "input,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b60aad4-07ca-422c-a376-fab2cd8cbdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor,BertTokenizer\n",
    "from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC, AutoModel,AutoProcessor\n",
    "\n",
    "from datasets import load_dataset,Dataset,Audio\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "class SpeechRecognitionDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, WAV2VEC2_ARGS: WAV2VEC2_ARGS, num_workers):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.batch_size = WAV2VEC2_ARGS.BATCH_SIZE\n",
    "        self.num_workers = num_workers\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(hparams.wav2vec2_model)\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        #self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(WAV2VEC2_ARGS.MODEL_BACKBONE)\n",
    "        #self.feature_extractor = AutoFeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(hparams.wav2vec2_model)\n",
    "        #self.processor = Wav2Vec2Processor(feature_extractor=self.feature_extractor, tokenizer=self.tokenizer)\n",
    "        #self.processor = AutoProcessor(feature_extractor=self.feature_extractor, tokenizer=self.bert_tokenizer)\n",
    "        self.processor = WhisperProcessor(feature_extractor=self.feature_extractor, tokenizer=self.bert_tokenizer)\n",
    "        self.data_collator = DataCollatorCTCWithPadding(processor=self.processor, padding=True)\n",
    "        self.cpu_count = 4\n",
    "        print(f\"CPU Count = {self.cpu_count}\")\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        train_df = pd.read_csv(WAV2VEC2_ARGS.TRAIN_FILE_PATH).head(10)\n",
    "        validation_df = pd.read_csv(WAV2VEC2_ARGS.TEST_FILE_PATH).head(10)\n",
    "        if stage == 'fit' or stage is None:\n",
    "            print(\"In Stage = Fit\")\n",
    "            train_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(train_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(train_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "            self.train_dataset = train_dataset.map(self.prepare_dataset,remove_columns = train_dataset.column_names)\n",
    "            \n",
    "            val_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(validation_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(validation_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "            self.val_dataset = val_dataset.map(self.prepare_dataset,remove_columns = val_dataset.column_names)\n",
    "\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            print(\"In Stage = Test\")\n",
    "            test_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(validation_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(validation_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "            test_dataset = val_dataset.map(self.prepare_dataset,remove_columns = val_dataset.column_names)\n",
    "            self.test_dataset = test_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        print(\"entering train data loader\")\n",
    "        return DataLoader(\n",
    "            self.train_dataset.with_format(\"torch\"), \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn = self.data_collator\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        print(\"entering val data loader\")\n",
    "        return DataLoader(\n",
    "            self.val_dataset.with_format(\"torch\"), \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn = self.data_collator\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        print(\"entering test data loader\")\n",
    "        return DataLoader(\n",
    "            self.test_dataset.with_format(\"torch\"), \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn = self.data_collator\n",
    "        )\n",
    "\n",
    "    def prepare_dataset(self,batch):\n",
    "        audio = batch[\"audio\"]\n",
    "        # batched output is \"un-batched\" to ensure mapping is correct\n",
    "        batch[\"input_values\"] = self.processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "        batch[\"attention_mask\"] = self.processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).attention_mask[0]\n",
    "        batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "        with self.processor.as_target_processor():\n",
    "            batch[\"labels\"] = self.processor(batch[\"transcription\"]).input_ids\n",
    "            batch[\"label_attention_mask\"] = self.processor(batch[\"transcription\"]).attention_mask\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a286c7f4-b4f8-4849-a121-7b7337b701c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Model, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC, AutoModel\n",
    "\n",
    "class Wav2SeqModel(LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.batch_size = hparams.batch_size\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        #self.wav2vec2 = AutoModelForCTC.from_pretrained(hparams.wav2vec2_model,ctc_zero_infinity=True,ctc_loss_reduction=\"mean\")\n",
    "        #self.wav2vec2.freeze_feature_encoder()\n",
    "        self.wav2vec2 = AutoModel.from_pretrained(hparams.wav2vec2_model)\n",
    "        self.wav2vec2.eval()\n",
    "        print(self.wav2vec2.config)\n",
    "        self.seq2seq = AutoModelForCausalLM.from_pretrained(hparams.lm_model)\n",
    "        self.seq2seq.config.is_decoder = True\n",
    "        self.seq2seq.add_cross_attention = True\n",
    "        print(self.seq2seq.config)\n",
    "\n",
    "    def forward(self, audio, attention_mask, labels, label_attention_mask):\n",
    "        #print(\"entering forward step\")\n",
    "        encoder_outputs = self.wav2vec2(audio[0],\n",
    "                                        attention_mask=attention_mask, #Made a change here by adding a [0]\n",
    "                                        output_hidden_states=True,\n",
    "                                        output_attentions=True)\n",
    "        encoder_hidden_states = encoder_outputs[0]  \n",
    "        print(f\"encoder_hidden_states={encoder_hidden_states}\")\n",
    "        encoder_attention_mask = self.wav2vec2._get_feature_vector_attention_mask(\n",
    "                encoder_hidden_states.shape[1], attention_mask\n",
    "            )\n",
    "        #print(f\"[PRIOR]decoder_input_ids={labels}\")\n",
    "        #print(f\"[PRIOR]decoder_attention_masks={label_attention_mask}\")\n",
    "        decoder_input_ids = self.shift_tokens_right(labels, 0, 101)\n",
    "        decoder_attention_masks = self.shift_tokens_right_mask(label_attention_mask)\n",
    "        #print(f\"[AFTER]decoder_input_ids={decoder_input_ids}\")\n",
    "        #print(f\"[AFTER]decoder_attention_masks={decoder_attention_masks}\")\n",
    "        decoder_outputs = self.seq2seq(input_ids=decoder_input_ids,\n",
    "                                       attention_mask = decoder_attention_masks,\n",
    "                                       encoder_hidden_states=encoder_hidden_states,\n",
    "                                      encoder_attention_mask=encoder_attention_mask) #,attention_mask = x.attentions, decoder_input_ids=predicted_ids\n",
    "        return decoder_outputs\n",
    "\n",
    "    def generate(self, audio):\n",
    "        encoder_outputs = self.wav2vec2(audio[0],output_hidden_states=True,output_attentions=True)\n",
    "        encoder_hidden_states = encoder_outputs[0]\n",
    "        bos_ids = (\n",
    "            torch.ones(\n",
    "                (encoder_hidden_states.size()[0], 1),\n",
    "                dtype=torch.long,\n",
    "                device=self.seq2seq.device,\n",
    "            )\n",
    "            * self.seq2seq.config.pad_token_id\n",
    "        )\n",
    "        return self.seq2seq.generate(\n",
    "            input_ids=bos_ids,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        #print(\"entering training step\")\n",
    "        audio = batch[\"input_values\"].unsqueeze(0)\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        label_attention_mask = batch[\"label_attention_masks\"].unsqueeze(0)\n",
    "        #print(f\"audio={audio}, attention_mask = {attention_mask}, label_attention_mask = {label_attention_mask}\")\n",
    "        labels = batch[\"labels\"]\n",
    "        labels = labels.reshape(self.batch_size,-1)\n",
    "        logits = self(audio,attention_mask,labels,label_attention_mask).logits\n",
    "        logits = logits.reshape(-1, self.batch_size, self.seq2seq.config.vocab_size)\n",
    "        input_lengths = torch.full(size=(self.batch_size,), fill_value=logits.shape[0], dtype=torch.long)\n",
    "        target_lengths = torch.full(size=(self.batch_size,), fill_value=labels.shape[0], dtype=torch.long)\n",
    "        ctc_loss =  nn.CTCLoss(blank=0)\n",
    "        loss = ctc_loss(logits,labels,input_lengths,target_lengths)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch,batch_idx):\n",
    "        #print(\"entering validation step\")\n",
    "        audio = batch[\"input_values\"].unsqueeze(0)\n",
    "        labels = batch[\"labels\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        label_attention_mask = batch[\"label_attention_masks\"].unsqueeze(0)\n",
    "        #print(f\"audio={audio}, attention_mask = {attention_mask}, label_attention_mask = {label_attention_mask}\")\n",
    "        labels = labels.reshape(self.batch_size,-1)\n",
    "        logits = self(audio,attention_mask,labels,label_attention_mask).logits\n",
    "        logits = logits.reshape(-1, self.batch_size, self.seq2seq.config.vocab_size)\n",
    "        input_lengths = torch.full(size=(self.batch_size,), fill_value=logits.shape[0], dtype=torch.long)\n",
    "        target_lengths = torch.full(size=(self.batch_size,), fill_value=labels.shape[0], dtype=torch.long)\n",
    "        ctc_loss =  nn.CTCLoss(blank=0)\n",
    "        loss = ctc_loss(logits,labels,input_lengths,target_lengths)\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        label_decoded = labels.type(torch.int64).tolist()\n",
    "        #print(f\"labels = {labels}\")\n",
    "        #print(f\"predicted ids = {predicted_ids}\")\n",
    "        print(f\"original text = {self.bert_tokenizer.decode(label_decoded[0])},{self.bert_tokenizer.decode(label_decoded[1])}\")\n",
    "        predicted_text = predicted_ids.type(torch.int64)\n",
    "        print(f\"Predicted text = {self.bert_tokenizer.decode(predicted_text[:,0].flatten().tolist())},{self.bert_tokenizer.decode(predicted_text[:,-1].flatten().tolist())}\")\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=hparams.learning_rate)\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
    "        \"\"\"\n",
    "        Shift input ids one token to the right.\n",
    "        \"\"\"\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "        if decoder_start_token_id is None:\n",
    "            raise ValueError(\"Make sure to set the decoder_start_token_id attribute of the model's configuration.\")\n",
    "        shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "    \n",
    "        if pad_token_id is None:\n",
    "            raise ValueError(\"Make sure to set the pad_token_id attribute of the model's configuration.\")\n",
    "        # replace possible -100 values in labels by `pad_token_id`\n",
    "        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "    \n",
    "        return shifted_input_ids\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_tokens_right_mask(input_ids: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Shift input ids one token to the right.\n",
    "        \"\"\"\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[:,:, 1:] = input_ids[:,:, :-1].clone()\n",
    "        shifted_input_ids[:,:, 0] = 0\n",
    "        return shifted_input_ids.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41800a8c-b400-4f13-acc7-af7a56bbfa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(WAV2VEC2_ARGS.TRAIN_FILE_PATH)\n",
    "validation_df = pd.read_csv(WAV2VEC2_ARGS.TEST_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497b1e3a-6e1a-4d93-9f48-a970bf08eea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df.transcription_capitalized.str.len()>200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806eb51a-1af0-4b4b-a02d-f26c16beed6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93f879b6-a836-4798-86ed-6bf40d44fc81",
   "metadata": {},
   "source": [
    "# Readying the codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aa9d25-0b49-480f-87fd-a7a1439cea0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgreeshmasmenon\u001b[0m (\u001b[33msongslyricstranscription\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230822_035929-vhnvnqsx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20transfer%20learning/runs/vhnvnqsx' target=\"_blank\">flowing-moon-126</a></strong> to <a href='https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20transfer%20learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20transfer%20learning' target=\"_blank\">https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20transfer%20learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20transfer%20learning/runs/vhnvnqsx' target=\"_blank\">https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20transfer%20learning/runs/vhnvnqsx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TRAIN_FILE_PATH\": \"/scratch/users/gmenon/train_song_metadata_en_demucs_cleaned_filtered_095.csv\",\n",
      "    \"TEST_FILE_PATH\": \"/scratch/users/gmenon/validation_song_metadata_en_demucs_cleaned_filtered_005.csv\",\n",
      "    \"MODEL_BACKBONE\": \"facebook/wav2vec2-large-960h-lv60-self\",\n",
      "    \"BATCH_SIZE\": 2,\n",
      "    \"NUM_EPOCHS\": 15,\n",
      "    \"MODEL_SAVE_PATH\": \"/scratch/users/gmenon//model_artefacts/wav2vec2_demucs_en_large-960h-lv60-self_freeze_unfreeze_15epochs_adamw.pt\",\n",
      "    \"FINETUNE_STRATEGY\": [\n",
      "        \"freeze_unfreeze\",\n",
      "        10\n",
      "    ],\n",
      "    \"LR_SCHEDULER\": \"reduce_on_plateau_schedule\"\n",
      "}\n",
      "Namespace(batch_size=2, learning_rate=0.001, lm_model='bert-base-uncased', vocab_size=40000, wav2vec2_model='facebook/wav2vec2-large-960h-lv60-self')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "Using 'facebook/wav2vec2-large-960h-lv60-self' provided by Hugging Face/transformers (https://github.com/huggingface/transformers).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Using 'facebook/wav2vec2-large-960h-lv60-self' provided by Hugging Face/transformers (https://github.com/huggingface/transformers).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"facebook/wav2vec2-large-960h-lv60-self\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_attn_dim\": null,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "CPU Count = 4\n",
      "In Stage = Fit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1cebfd9596d42cf8d6b241b75a5bb08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609bf5fb5c594f749bda66de4c38e244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type            | Params\n",
      "---------------------------------------------\n",
      "0 | wav2vec2 | Wav2Vec2ForCTC  | 315 M \n",
      "1 | seq2seq  | BertLMHeadModel | 109 M \n",
      "---------------------------------------------\n",
      "424 M     Trainable params\n",
      "0         Non-trainable params\n",
      "424 M     Total params\n",
      "1,699.943 Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering Optimization Step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entering val data loader\n",
      "original text = [CLS] so learn from your mistakes [SEP] [PAD] [PAD] [PAD] [PAD],[CLS] i've been connected to the right line [SEP]\n",
      "Predicted text = \" learn your \" and and to'' the line,so now so and and the the been to right.\n",
      "original text = [CLS] the truth to be found [SEP] [PAD] [PAD] [PAD] [PAD] [PAD],[CLS] he said the way myblue eyes shined [SEP]\n",
      "Predicted text = and the be to as as as and waybl andd,the the seen as as as and, alllay bar )\n",
      "entering train data loader\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dfe093cacf948bab13d57f087b6c7ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text = [CLS] so learn from your mistakes [SEP] [PAD] [PAD] [PAD] [PAD],[CLS] i've been connected to the right line [SEP]\n",
      "Predicted text = [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS],[CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "original text = [CLS] the truth to be found [SEP] [PAD] [PAD] [PAD] [PAD] [PAD],[CLS] he said the way myblue eyes shined [SEP]\n",
      "Predicted text = [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS],[CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "original text = [CLS] you leave me once again home alone [SEP],[CLS] while they are in commend [SEP]\n",
      "Predicted text = [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS],[CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "original text = [CLS] a life all mine [SEP] [PAD],[CLS] so i never went back [SEP]\n",
      "Predicted text = [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS],[CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "original text = [CLS] in you i taste god [SEP],[CLS] could stay a while [SEP] [PAD]\n",
      "Predicted text = [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS],[CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text = [CLS] so learn from your mistakes [SEP] [PAD] [PAD] [PAD] [PAD],[CLS] i've been connected to the right line [SEP]\n",
      "Predicted text = [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS],[CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "original text = [CLS] the truth to be found [SEP] [PAD] [PAD] [PAD] [PAD] [PAD],[CLS] he said the way myblue eyes shined [SEP]\n",
      "Predicted text = [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS],[CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "original text = [CLS] you leave me once again home alone [SEP],[CLS] while they are in commend [SEP]\n",
      "Predicted text = [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS],[CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "original text = [CLS] a life all mine [SEP] [PAD],[CLS] so i never went back [SEP]\n",
      "Predicted text = [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS],[CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "original text = [CLS] in you i taste god [SEP],[CLS] could stay a while [SEP] [PAD]\n",
      "Predicted text = [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS],[CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text = [CLS] so learn from your mistakes [SEP] [PAD] [PAD] [PAD] [PAD],[CLS] i've been connected to the right line [SEP]\n",
      "Predicted text = [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS],[CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "original text = [CLS] the truth to be found [SEP] [PAD] [PAD] [PAD] [PAD] [PAD],[CLS] he said the way myblue eyes shined [SEP]\n",
      "Predicted text = [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS],[CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "original text = [CLS] you leave me once again home alone [SEP],[CLS] while they are in commend [SEP]\n",
      "Predicted text = [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS],[CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "original text = [CLS] a life all mine [SEP] [PAD],[CLS] so i never went back [SEP]\n",
      "Predicted text = [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS],[CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "original text = [CLS] in you i taste god [SEP],[CLS] could stay a while [SEP] [PAD]\n",
      "Predicted text = [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS],[CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text = [CLS] so learn from your mistakes [SEP] [PAD] [PAD] [PAD] [PAD],[CLS] i've been connected to the right line [SEP]\n",
      "Predicted text = [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS],[CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "original text = [CLS] the truth to be found [SEP] [PAD] [PAD] [PAD] [PAD] [PAD],[CLS] he said the way myblue eyes shined [SEP]\n",
      "Predicted text = [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS],[CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "original text = [CLS] you leave me once again home alone [SEP],[CLS] while they are in commend [SEP]\n",
      "Predicted text = [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS],[CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "original text = [CLS] a life all mine [SEP] [PAD],[CLS] so i never went back [SEP]\n",
      "Predicted text = [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS],[CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "original text = [CLS] in you i taste god [SEP],[CLS] could stay a while [SEP] [PAD]\n",
      "Predicted text = [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS],[CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text = [CLS] so learn from your mistakes [SEP] [PAD] [PAD] [PAD] [PAD],[CLS] i've been connected to the right line [SEP]\n",
      "Predicted text = [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS],[CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "original text = [CLS] the truth to be found [SEP] [PAD] [PAD] [PAD] [PAD] [PAD],[CLS] he said the way myblue eyes shined [SEP]\n",
      "Predicted text = [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS],[CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "original text = [CLS] you leave me once again home alone [SEP],[CLS] while they are in commend [SEP]\n",
      "Predicted text = [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS],[CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "original text = [CLS] a life all mine [SEP] [PAD],[CLS] so i never went back [SEP]\n",
      "Predicted text = [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS],[CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n",
      "original text = [CLS] in you i taste god [SEP],[CLS] could stay a while [SEP] [PAD]\n",
      "Predicted text = [CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS],[CLS] [CLS] [CLS] [CLS] [CLS] [CLS] [CLS]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os. chdir('/home/users/gmenon/workspace/songsLyricsGenerator/src')\n",
    "from training import lyrics_finetune\n",
    "from constants.mir_constants import TrainingArgs, WAV2VEC2_ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565f829-bc48-41db-a714-42028b889662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "hparams = argparse.Namespace()\n",
    "hparams.wav2vec2_model = 'facebook/wav2vec2-large-960h-lv60-self'\n",
    "hparams.lm_model = 'bert-base-uncased' #'facebook/bart-large'\n",
    "hparams.vocab_size = 40000\n",
    "hparams.learning_rate = 1e-6\n",
    "hparams.batch_size = 2\n",
    "\n",
    "model,trainer = lyrics_finetune.run(hparams=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1943c2e9-59c8-46f5-a681-9c96ec6776f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544d3245-2648-4434-baf5-2249682358e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = trainer.datamodule\n",
    "# data.setup()\n",
    "# import torch\n",
    "# torch.Tensor(data.val_dataset['input_values'][0]).unsqueeze(0)\n",
    "# model.generate(torch.Tensor(data.val_dataset['input_values'][0]).unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd01a88-f5e5-467d-b7fe-b019deb2b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.Tensor([6506, 3808,1998, 1996,6506,3808,2058,2058,1012])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2911ae-4a22-4957-b03f-4ba66dfcd017",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a92ef7-b8db-4cbd-90f0-3e991c895322",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [101, 2054, 2842, 2023, 2071, 1005, 2310, 2042,  102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d53c214-9bd2-4bc1-a664-f5efb642ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d42a160-2f59-4890-9040-95d0d430c41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edeeb59-e4fb-4458-9072-4995164f53b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c818fd82-a604-428d-8407-291640c1fea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603d0b8-5ec5-4b92-8d23-ce783df5b183",
   "metadata": {},
   "outputs": [],
   "source": [
    "b  = [6506, 3808, 2003,1998,2042,3808,7316,1999,1012]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4a8aab-5759-49ab-8769-4d6973e21bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_2D_list = [[5, 10, 15, 20],\n",
    "                   [25, 30, 35, 40],\n",
    "                   [45, 50, 55, 60]]\n",
    "list_to_tensor = torch.tensor(example_2D_list)\n",
    "print(\"Our New 2D Tensor from 2D List is: \", list_to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc4b9c-8c94-4c66-b9e4-344d1a6a0c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b95338-4fc7-46bc-b7fc-3f8fa3ef4bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = torch.Tensor([[6506, 3808],\n",
    "        [3808, 3808],\n",
    "        [2003, 1998],\n",
    "        [1998, 1996],\n",
    "        [2042, 6506],\n",
    "        [3808, 3808],\n",
    "        [7316, 2058],\n",
    "        [1999, 2058],\n",
    "        [1012, 1012]]).type(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6578baf2-b5f9-4889-9c1f-96498a615ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = tensors[:,:1].flatten().tolist()\n",
    "list2 = tensors[:,-1].flatten().tolist()\n",
    "list1, list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216744cf-ab2d-42fb-bbaf-e215c03a8602",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345721b-6fb6-4d85-a7bb-97ac07da7e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffe275d-62f2-4ed3-8483-8c0d612068bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.Tensor([[ 101, 2054, 2842, 2023, 2071, 1005, 2310, 2042,  102],\n",
    "        [ 101, 2562, 2115, 2192, 1999, 3067,  102, -100, -100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644480ec-0dd4-493b-b237-92fb3112d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4364896-4d8a-4f68-82a9-53dfff78945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe00d13-4a16-4d87-b099-d9523c4b2a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541180e7-ee15-4c38-9b11-5a19d615e4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3af1ef-24f3-4655-ae66-3d76994c8809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54566220-e690-4eba-a75f-557845db3bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f292ed0a-68b9-4164-9e50-cf3e3a2b6598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9880857-0b09-4be1-be14-5e279a0e9ed1",
   "metadata": {},
   "source": [
    "## TESTING HUGGINGFACE's IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3892980d-3369-4945-82ea-7631c6a86e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechEncoderDecoderModel, AutoProcessor\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-xls-r-300m-en-to-15\")\n",
    "model = SpeechEncoderDecoderModel.from_pretrained(\"facebook/wav2vec2-xls-r-300m-en-to-15\")\n",
    "\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\").input_values\n",
    "# Inference: Translate English speech to German\n",
    "generated = model.generate(input_values)\n",
    "decoded = processor.batch_decode(generated, skip_special_tokens=True)[0]\n",
    "decoded\n",
    "\n",
    "# Training: Train model on English transcription\n",
    "labels = processor(text=ds[0][\"text\"], return_tensors=\"pt\").input_ids\n",
    "\n",
    "loss = model(input_values, labels=labels).loss\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb6c4c2-6498-4e6e-bc35-99954667b3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoFeatureExtractor, SpeechEncoderDecoderModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "encoder_id = \"facebook/wav2vec2-base-960h\"  # acoustic model encoder\n",
    "decoder_id = \"bert-base-uncased\"  # text decoder\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(encoder_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(decoder_id)\n",
    "# Combine pre-trained encoder and pre-trained decoder to form a Seq2Seq model\n",
    "model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_id, decoder_id)\n",
    "\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# load an audio input and pre-process (normalise mean/std to 0/1)\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "input_values = feature_extractor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\").input_values\n",
    "\n",
    "# load its corresponding transcription and tokenize to generate labels\n",
    "labels = tokenizer(ds[0][\"text\"], return_tensors=\"pt\").input_ids\n",
    "\n",
    "# the forward function automatically creates the correct decoder_input_ids\n",
    "loss = model(input_values).loss\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3899ce1-5707-4381-9981-ce9c3646f20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.cls_token_id,tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09855416-6f50-4a4b-8667-d311a544a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8949546b-5e7d-479e-818f-0431b00a1e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b88539-fb4f-437a-b7b0-1abc3696f966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from datasets.features import Audio\n",
    "from transformers import SpeechEncoderDecoderModel, Wav2Vec2Processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bf4512-0a15-4407-ad72-cb7071af0f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LANG_ID = \"ru\"\n",
    "MODEL_ID = \"bond005/wav2vec2-mbart50-ru\"\n",
    "SAMPLES = 30\n",
    "\n",
    "num_processes = 4\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "model = SpeechEncoderDecoderModel.from_pretrained(MODEL_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9964f298-8e2b-4302-8772-c83fa20e8f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n",
    "\n",
    "if test_dataset.features['audio'].sampling_rate != 16_000:\n",
    "    test_dataset = test_dataset.cast_column(\n",
    "        'audio',\n",
    "        Audio(sampling_rate=16_000)\n",
    "    )\n",
    "\n",
    "audio_data = [test_dataset[i]['audio']['array'] for i in range(SAMPLES)]\n",
    "\n",
    "processed = processor(audio_data, sampling_rate=16_000,\n",
    "                      return_tensors=\"pt\", padding='longest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067f1f7b-a934-4c63-b188-de0bfbd7d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    predicted_ids = model.generate(**processed)\n",
    "\n",
    "predicted_sentences = processor.batch_decode(\n",
    "    predicted_ids,\n",
    "    num_processes=num_processes,\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for i, predicted_sentence in enumerate(predicted_sentences):\n",
    "        print(\"-\" * 100)\n",
    "        print(\"Reference: \", test_dataset[i][\"sentence\"])\n",
    "        print(\"Prediction:\", predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ef0a005-9e5a-488b-a9a0-6d57153ad408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2babd045-1df9-42e6-aa6e-afde7e55d8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Folder</th>\n",
       "      <th>consolidated_file_path</th>\n",
       "      <th>Length</th>\n",
       "      <th>transcription</th>\n",
       "      <th>transcription_capitalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01cef35811fd4a3fa63a3ab8bba5430c</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.447392</td>\n",
       "      <td>right about now i'm fifty fifty</td>\n",
       "      <td>RIGHT ABOUT NOW I'M FIFTY FIFTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0c04880a6eaf4e559fd9b20594eb74cd</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>4.137755</td>\n",
       "      <td>let me run my fingers through your hair</td>\n",
       "      <td>LET ME RUN MY FINGERS THROUGH YOUR HAIR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01a7c7f2e6db4c68abfbc1218dc9ef76</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.923696</td>\n",
       "      <td>but you're yesterday's child to me</td>\n",
       "      <td>BUT YOU'RE YESTERDAY'S CHILD TO ME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0c8c027ad9374db6b9e6c12dc873be7b</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.058390</td>\n",
       "      <td>and whisper so softly</td>\n",
       "      <td>AND WHISPER SO SOFTLY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>08558a8941c04a559a3ea48793ac1a34</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.038073</td>\n",
       "      <td>have a laugh at the rat race</td>\n",
       "      <td>HAVE A LAUGH AT THE RAT RACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>07929e956a9745fa96e91730d8540cd1</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.702948</td>\n",
       "      <td>aaaa haaa haaa</td>\n",
       "      <td>AAAA HAAA HAAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>002fe3610f1b41b59d28546201635043</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>3.507075</td>\n",
       "      <td>but that's not how it used to be</td>\n",
       "      <td>BUT THAT'S NOT HOW IT USED TO BE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0a4e93435d0746d39ea0814667d6fa3a</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>3.020454</td>\n",
       "      <td>and you've seen it all before but the wolf's o...</td>\n",
       "      <td>AND YOU'VE SEEN IT ALL BEFORE BUT THE WOLF'S O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0bde4da0cf2842028bf7d1a712dec1c5</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.734830</td>\n",
       "      <td>and talk is cheap awhen the story is good</td>\n",
       "      <td>AND TALK IS CHEAP AWHEN THE STORY IS GOOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0851f1ca1594443d897d4a32b4eab788</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.288141</td>\n",
       "      <td>turned you into someone new</td>\n",
       "      <td>TURNED YOU INTO SOMEONE NEW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Folder  \\\n",
       "0  01cef35811fd4a3fa63a3ab8bba5430c   \n",
       "1  0c04880a6eaf4e559fd9b20594eb74cd   \n",
       "2  01a7c7f2e6db4c68abfbc1218dc9ef76   \n",
       "3  0c8c027ad9374db6b9e6c12dc873be7b   \n",
       "4  08558a8941c04a559a3ea48793ac1a34   \n",
       "5  07929e956a9745fa96e91730d8540cd1   \n",
       "6  002fe3610f1b41b59d28546201635043   \n",
       "7  0a4e93435d0746d39ea0814667d6fa3a   \n",
       "8  0bde4da0cf2842028bf7d1a712dec1c5   \n",
       "9  0851f1ca1594443d897d4a32b4eab788   \n",
       "\n",
       "                              consolidated_file_path    Length  \\\n",
       "0  /scratch/users/gmenon/wav_clips/separated/htde...  2.447392   \n",
       "1  /scratch/users/gmenon/wav_clips/separated/htde...  4.137755   \n",
       "2  /scratch/users/gmenon/wav_clips/separated/htde...  2.923696   \n",
       "3  /scratch/users/gmenon/wav_clips/separated/htde...  2.058390   \n",
       "4  /scratch/users/gmenon/wav_clips/separated/htde...  2.038073   \n",
       "5  /scratch/users/gmenon/wav_clips/separated/htde...  2.702948   \n",
       "6  /scratch/users/gmenon/wav_clips/separated/htde...  3.507075   \n",
       "7  /scratch/users/gmenon/wav_clips/separated/htde...  3.020454   \n",
       "8  /scratch/users/gmenon/wav_clips/separated/htde...  2.734830   \n",
       "9  /scratch/users/gmenon/wav_clips/separated/htde...  2.288141   \n",
       "\n",
       "                                       transcription  \\\n",
       "0                    right about now i'm fifty fifty   \n",
       "1            let me run my fingers through your hair   \n",
       "2                 but you're yesterday's child to me   \n",
       "3                              and whisper so softly   \n",
       "4                       have a laugh at the rat race   \n",
       "5                                     aaaa haaa haaa   \n",
       "6                   but that's not how it used to be   \n",
       "7  and you've seen it all before but the wolf's o...   \n",
       "8          and talk is cheap awhen the story is good   \n",
       "9                        turned you into someone new   \n",
       "\n",
       "                           transcription_capitalized  \n",
       "0                    RIGHT ABOUT NOW I'M FIFTY FIFTY  \n",
       "1            LET ME RUN MY FINGERS THROUGH YOUR HAIR  \n",
       "2                 BUT YOU'RE YESTERDAY'S CHILD TO ME  \n",
       "3                              AND WHISPER SO SOFTLY  \n",
       "4                       HAVE A LAUGH AT THE RAT RACE  \n",
       "5                                     AAAA HAAA HAAA  \n",
       "6                   BUT THAT'S NOT HOW IT USED TO BE  \n",
       "7  AND YOU'VE SEEN IT ALL BEFORE BUT THE WOLF'S O...  \n",
       "8          AND TALK IS CHEAP AWHEN THE STORY IS GOOD  \n",
       "9                        TURNED YOU INTO SOMEONE NEW  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(WAV2VEC2_ARGS.TRAIN_FILE_PATH).head(10)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b64793a-efaa-407c-94ea-80ece2282045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Folder</th>\n",
       "      <th>consolidated_file_path</th>\n",
       "      <th>Length</th>\n",
       "      <th>transcription</th>\n",
       "      <th>transcription_capitalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0aa835068aa64515b2158c2f0b6513b7</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>4.754943</td>\n",
       "      <td>so learn from your mistakes</td>\n",
       "      <td>SO LEARN FROM YOUR MISTAKES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>089cd84d662e4c50954cfa46c07cae14</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.759388</td>\n",
       "      <td>i've been connected to the right line</td>\n",
       "      <td>I'VE BEEN CONNECTED TO THE RIGHT LINE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>127cc2aaff9a44fab21b2a0c4f5c3e5d</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.668254</td>\n",
       "      <td>the truth to be found</td>\n",
       "      <td>THE TRUTH TO BE FOUND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18e03c3980014046a3504ef42fe81ae8</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.895079</td>\n",
       "      <td>he said the way myblue eyes shined</td>\n",
       "      <td>HE SAID THE WAY MYBLUE EYES SHINED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07c8b8c0b0424c1e87389392695a551b</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.666939</td>\n",
       "      <td>you leave me once again home alone</td>\n",
       "      <td>YOU LEAVE ME ONCE AGAIN HOME ALONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0cb795c9b6e64a4abab5553c2b2ed3e0</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.742880</td>\n",
       "      <td>while they are in commend</td>\n",
       "      <td>WHILE THEY ARE IN COMMEND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0655395364614218be31548245853ca0</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>4.697710</td>\n",
       "      <td>a life all mine</td>\n",
       "      <td>A LIFE ALL MINE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>113bb605eb324aeb8db046b4193424a2</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.642948</td>\n",
       "      <td>so i never went back</td>\n",
       "      <td>SO I NEVER WENT BACK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00ecd4b71c094b959ae492ec138bb555</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>3.176508</td>\n",
       "      <td>in you i taste god</td>\n",
       "      <td>IN YOU I TASTE GOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0b5fa72c054e44598ec8aefb76ff855e</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.465011</td>\n",
       "      <td>could stay a while</td>\n",
       "      <td>COULD STAY A WHILE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Folder  \\\n",
       "0  0aa835068aa64515b2158c2f0b6513b7   \n",
       "1  089cd84d662e4c50954cfa46c07cae14   \n",
       "2  127cc2aaff9a44fab21b2a0c4f5c3e5d   \n",
       "3  18e03c3980014046a3504ef42fe81ae8   \n",
       "4  07c8b8c0b0424c1e87389392695a551b   \n",
       "5  0cb795c9b6e64a4abab5553c2b2ed3e0   \n",
       "6  0655395364614218be31548245853ca0   \n",
       "7  113bb605eb324aeb8db046b4193424a2   \n",
       "8  00ecd4b71c094b959ae492ec138bb555   \n",
       "9  0b5fa72c054e44598ec8aefb76ff855e   \n",
       "\n",
       "                              consolidated_file_path    Length  \\\n",
       "0  /scratch/users/gmenon/wav_clips/separated/htde...  4.754943   \n",
       "1  /scratch/users/gmenon/wav_clips/separated/htde...  2.759388   \n",
       "2  /scratch/users/gmenon/wav_clips/separated/htde...  2.668254   \n",
       "3  /scratch/users/gmenon/wav_clips/separated/htde...  2.895079   \n",
       "4  /scratch/users/gmenon/wav_clips/separated/htde...  2.666939   \n",
       "5  /scratch/users/gmenon/wav_clips/separated/htde...  2.742880   \n",
       "6  /scratch/users/gmenon/wav_clips/separated/htde...  4.697710   \n",
       "7  /scratch/users/gmenon/wav_clips/separated/htde...  2.642948   \n",
       "8  /scratch/users/gmenon/wav_clips/separated/htde...  3.176508   \n",
       "9  /scratch/users/gmenon/wav_clips/separated/htde...  2.465011   \n",
       "\n",
       "                           transcription  \\\n",
       "0            so learn from your mistakes   \n",
       "1  i've been connected to the right line   \n",
       "2                  the truth to be found   \n",
       "3     he said the way myblue eyes shined   \n",
       "4     you leave me once again home alone   \n",
       "5              while they are in commend   \n",
       "6                        a life all mine   \n",
       "7                   so i never went back   \n",
       "8                     in you i taste god   \n",
       "9                     could stay a while   \n",
       "\n",
       "               transcription_capitalized  \n",
       "0            SO LEARN FROM YOUR MISTAKES  \n",
       "1  I'VE BEEN CONNECTED TO THE RIGHT LINE  \n",
       "2                  THE TRUTH TO BE FOUND  \n",
       "3     HE SAID THE WAY MYBLUE EYES SHINED  \n",
       "4     YOU LEAVE ME ONCE AGAIN HOME ALONE  \n",
       "5              WHILE THEY ARE IN COMMEND  \n",
       "6                        A LIFE ALL MINE  \n",
       "7                   SO I NEVER WENT BACK  \n",
       "8                     IN YOU I TASTE GOD  \n",
       "9                     COULD STAY A WHILE  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df = pd.read_csv(WAV2VEC2_ARGS.TEST_FILE_PATH).head(10)\n",
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5a069d0-7df7-454a-b738-8756510565c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor, BertTokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('facebook/wav2vec2-large-960h-lv60-self')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f95a8161-b007-4da7-bb81-3a4c103eab4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([101,\n",
       "   156,\n",
       "   2346,\n",
       "   149,\n",
       "   12420,\n",
       "   2069,\n",
       "   2249,\n",
       "   143,\n",
       "   21564,\n",
       "   2107,\n",
       "   19141,\n",
       "   2069,\n",
       "   26574,\n",
       "   9272,\n",
       "   1592,\n",
       "   22441,\n",
       "   1708,\n",
       "   102],\n",
       "  [101, 1177, 3858, 1121, 1240, 12572, 102]),\n",
       " ([101,\n",
       "   146,\n",
       "   112,\n",
       "   159,\n",
       "   2036,\n",
       "   139,\n",
       "   27073,\n",
       "   2249,\n",
       "   18732,\n",
       "   2249,\n",
       "   22680,\n",
       "   16647,\n",
       "   10069,\n",
       "   16972,\n",
       "   7462,\n",
       "   155,\n",
       "   23413,\n",
       "   18784,\n",
       "   149,\n",
       "   11607,\n",
       "   2036,\n",
       "   102],\n",
       "  [101, 178, 112, 1396, 1151, 3387, 1106, 1103, 1268, 1413, 102]),\n",
       " ([101,\n",
       "   7462,\n",
       "   157,\n",
       "   2069,\n",
       "   16830,\n",
       "   3048,\n",
       "   16972,\n",
       "   139,\n",
       "   2036,\n",
       "   143,\n",
       "   2346,\n",
       "   27370,\n",
       "   2137,\n",
       "   102],\n",
       "  [101, 1103, 3062, 1106, 1129, 1276, 102]),\n",
       " ([101,\n",
       "   145,\n",
       "   2036,\n",
       "   13411,\n",
       "   9949,\n",
       "   7462,\n",
       "   22751,\n",
       "   3663,\n",
       "   150,\n",
       "   3663,\n",
       "   13360,\n",
       "   24846,\n",
       "   142,\n",
       "   3663,\n",
       "   9919,\n",
       "   17730,\n",
       "   11607,\n",
       "   10069,\n",
       "   102],\n",
       "  [101, 1119, 1163, 1103, 1236, 1139, 1830, 19224, 1257, 18978, 1181, 102]),\n",
       " ([101,\n",
       "   19141,\n",
       "   149,\n",
       "   12420,\n",
       "   17145,\n",
       "   22157,\n",
       "   21748,\n",
       "   10954,\n",
       "   14731,\n",
       "   1592,\n",
       "   11607,\n",
       "   145,\n",
       "   13041,\n",
       "   2036,\n",
       "   18589,\n",
       "   11414,\n",
       "   2036,\n",
       "   102],\n",
       "  [101, 1128, 1817, 1143, 1517, 1254, 1313, 2041, 102]),\n",
       " ([101,\n",
       "   160,\n",
       "   3048,\n",
       "   17656,\n",
       "   2036,\n",
       "   7462,\n",
       "   3663,\n",
       "   22133,\n",
       "   2036,\n",
       "   15969,\n",
       "   18732,\n",
       "   25290,\n",
       "   11680,\n",
       "   2137,\n",
       "   102],\n",
       "  [101, 1229, 1152, 1132, 1107, 3254, 2354, 1181, 102]),\n",
       " ([101, 138, 149, 15499, 2036, 18589, 2162, 26574, 22680, 102],\n",
       "  [101, 170, 1297, 1155, 2317, 102]),\n",
       " ([101,\n",
       "   156,\n",
       "   2346,\n",
       "   146,\n",
       "   26546,\n",
       "   17145,\n",
       "   2069,\n",
       "   160,\n",
       "   11680,\n",
       "   1942,\n",
       "   12465,\n",
       "   1658,\n",
       "   2428,\n",
       "   102],\n",
       "  [101, 1177, 178, 1309, 1355, 1171, 102]),\n",
       " ([101, 15969, 19141, 146, 157, 10719, 12880, 27157, 2137, 102],\n",
       "  [101, 1107, 1128, 178, 5080, 5540, 102]),\n",
       " ([101,\n",
       "   18732,\n",
       "   2591,\n",
       "   20521,\n",
       "   23676,\n",
       "   1592,\n",
       "   3663,\n",
       "   138,\n",
       "   160,\n",
       "   3048,\n",
       "   17656,\n",
       "   2036,\n",
       "   102],\n",
       "  [101, 1180, 2215, 170, 1229, 102])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a for a in zip(bert_tokenizer(list(validation_df[\"transcription_capitalized\"])).input_ids,bert_tokenizer(list(validation_df[\"transcription\"])).input_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1a216e7-276c-45cc-91f7-401946441a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he said the way myblue eyes shined'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.decode(bert_tokenizer(list(validation_df[\"transcription\"])).input_ids[3],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec7d0394-1291-433f-935c-fbe850890b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'so learn from your mistakes'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(validation_df[\"transcription\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "559d4025-cde5-48d9-ac08-294a5ec63ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "label = torch.Tensor([101, 155, 23413, 18784, 16151, 2346, 16830, 24819, 2924, 146, 112, 150, 143, 15499, 16880, 143, 15499, 16880, 102])\n",
    "\n",
    "label = torch.Tensor([[ 101, 1037, 2166, 2035, 3067,  102, -100],[ 101, 2061, 1045, 2196, 2253, 2067,  102]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "317b076f-5be1-4e66-b382-5b23bf56cc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor, BertTokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('facebook/wav2vec2-base-960h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dee7e39-0f87-4918-b1d1-b37c65a653b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] so i never went back [SEP]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.decode(label[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b14e73de-3744-45e5-a44b-f10890399af7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wav_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwav_tokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mdecode(label)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wav_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "wav_tokenizer.decode(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b0e4c8-2c5d-4c8c-bad1-54ff6462c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.Tensor([[ 101, 1037, 2166, 2035, 3067,  102, -100],\n",
    "        [ 101, 2061, 1045, 2196, 2253, 2067,  102]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218963a8-63c0-45d4-9679-139ced75b5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(label[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689de184-6d7c-4a51-846a-c3cddb5e338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(label[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035c37ac-55de-43be-9fd1-0d53040b916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = torch.Tensor([[1996, 1996],\n",
    "        [1999, 1996],\n",
    "        [2061, 1996],\n",
    "        [2061, 1000],\n",
    "        [1000, 1998],\n",
    "        [1000, 2030],\n",
    "        [2061, 2061]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211520c9-f5d0-4712-a337-e279d30858e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(predicted.T[0,:],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c065f3e5-eaa6-4aec-9423-ce4a78b8b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "validation_df = pd.read_csv(WAV2VEC2_ARGS.TEST_FILE_PATH).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e9fc80e-dc3b-4a44-8256-06615ad031a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THE TRUTH TO BE FOUND']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(validation_df[validation_df[\"Folder\"] == '127cc2aaff9a44fab21b2a0c4f5c3e5d'][\"transcription_capitalized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f98d289e-a041-4d28-bc86-74c3e7ed3627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[101, 1996, 3606, 2000, 2022, 2179, 102]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer(list(validation_df[validation_df[\"Folder\"] == '127cc2aaff9a44fab21b2a0c4f5c3e5d'][\"transcription_capitalized\"])).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d44d271a-c5d7-4e6f-876d-f11e0ff2fa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    #batch[\"attention_mask\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).attention_mask[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "    batch[\"labels\"] = bert_tokenizer(batch[\"transcription\"]).input_ids\n",
    "    batch[\"label_attention_mask\"] = bert_tokenizer(batch[\"transcription\"]).attention_mask\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07bcd7e1-c776-4cf8-be39-50b0cfb96688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Audio\n",
    "val_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(validation_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(validation_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c6a8104b-8f8b-44eb-ab90-7842b9af0954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6514c59c27fa4320b0d4ac9e1c2322f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_dataset1 = val_dataset.map(prepare_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c485df2f-0cae-4b93-a2ec-32387d553eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_dataset1[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ae5938ce-fe60-404b-abc9-cee7a9dbfd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_features = [{\"input_ids\": feature[\"labels\"]} for feature in val_dataset1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8cee0e86-af23-4f98-969f-be4e9084af5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 101, 1996, 3606, 2000, 2022, 2179,  102]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.pad(\n",
    "            label_features,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3eed0fe2-d3a6-46ff-8de0-0c7515c29d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a965d4d2-c262-447f-9402-3339a2396df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ae1f6e7-a6e0-4292-880f-7b7ff49a0aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da5b74a7-9ce8-444b-a3be-86f495b5c70b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e2603f-4b14-4914-9c05-65bf05bfecac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
