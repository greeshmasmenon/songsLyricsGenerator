{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46358ffa-ac47-48c6-917e-54d7918483d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os. chdir('/home/users/gmenon/workspace/songsLyricsGenerator/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3852a9e9-9f11-40e7-be68-d372a242db5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_lightning import LightningModule\n",
    "import pytorch_lightning as pl\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Model\n",
    "from constants.mir_constants import TrainingArgs, WAV2VEC2_ARGS\n",
    "from training import lyrics_finetune\n",
    "from dataclasses import dataclass, asdict\n",
    "import json\n",
    "import argparse\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23a1124d-49d5-4576-b8f2-f775c7239d84",
   "metadata": {},
   "source": [
    "from flash.audio import SpeechRecognitionData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b62468-9929-43e3-8aeb-7daaeb74b47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(asdict(WAV2VEC2_ARGS), indent = 4))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "690c412e-6b90-431e-98d9-1dfac1e94f98",
   "metadata": {},
   "source": [
    "datamodule = SpeechRecognitionData.from_csv(\"consolidated_file_path\",\n",
    "                                                         \"transcription_capitalized\",\n",
    "                                                         train_file=WAV2VEC2_ARGS.TRAIN_FILE_PATH,\n",
    "                                                         test_file=WAV2VEC2_ARGS.TEST_FILE_PATH,\n",
    "                                                         batch_size=WAV2VEC2_ARGS.BATCH_SIZE\n",
    "                                                         )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0809f82-32b0-4995-bdb0-08779a8c0211",
   "metadata": {},
   "source": [
    "datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831d6096-d0e4-4d39-8828-57e3a4a208cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flash\n",
    "import torchaudio\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# class SpeechDataModule(pl.LightningDataModule):\n",
    "#     def __init__(self, args: WAV2VEC2_ARGS):\n",
    "#         super().__init__()\n",
    "#         self.batch_size = 2\n",
    "\n",
    "#     def prepare_data(self):\n",
    "#         # Download and extract the audio files and transcripts, if necessary.\n",
    "#         pass\n",
    "\n",
    "#     def setup(self, stage=None):\n",
    "#         # Create the dataset\n",
    "#         # datamodule = \n",
    "#         self.dataset = SpeechRecognitionData.from_csv(\"consolidated_file_path\",\n",
    "#                                                          \"transcription_capitalized\",\n",
    "#                                                          train_file=WAV2VEC2_ARGS.TRAIN_FILE_PATH,\n",
    "#                                                          test_file=WAV2VEC2_ARGS.TEST_FILE_PATH,\n",
    "#                                                          batch_size=2\n",
    "#                                                          )\n",
    "\n",
    "#         # Create the dataloader\n",
    "#         self.train_dataloader = self.dataset.train_dataloader\n",
    "#         # torch.utils.data.DataLoader(\n",
    "#         #     self.dataset,\n",
    "#         #     batch_size=self.batch_size,\n",
    "#         #     shuffle=True,\n",
    "#         #     num_workers=4,\n",
    "#         # )\n",
    "#         self.val_dataloader = self.dataset.val_dataloader\n",
    "#         # torch.utils.data.DataLoader(\n",
    "#         #     self.dataset,\n",
    "#         #     batch_size=self.batch_size,\n",
    "#         #     shuffle=False,\n",
    "#         #     num_workers=4,\n",
    "#         # )\n",
    "\n",
    "#     def train_dataloader(self):\n",
    "#         return self.train_dataloader\n",
    "\n",
    "#     def val_dataloader(self):\n",
    "#         return self.val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3354031-3fb6-46ee-9fed-c3c06d23348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DALIDataset(pl.LightningDataModule):\n",
    "#     def __init__(self, batch_size: int = 4,\n",
    "#                   train_path :Optional[str] = None,\n",
    "#                     validation_path: Optional[str] = None,\n",
    "#                       model_backbone: pl.LightningModule = None,\n",
    "#                       args: TrainingArgs = WAV2VEC2_ARGS\n",
    "#                       ):\n",
    "        \n",
    "#         super().__init__()\n",
    "#         self.train_path = train_path if train_path is not None else args.TRAIN_FILE_PATH\n",
    "#         self.validation_path = validation_path if validation_path is not None else args.TEST_FILE_PATH\n",
    "#         self.model_backbone = model_backbone if model_backbone is not None else args.MODEL_BACKBONE\n",
    "\n",
    "#         def prepare_data(self):\n",
    "#             pass\n",
    "        \n",
    "#         def setup(self):\n",
    "#             train_df = pd.read_csv(WAV2VEC2_ARGS.TRAIN_FILE_PATH) \n",
    "#             validation_df = pd.read_csv(WAV2VEC2_ARGS.TEST_FILE_PATH)\n",
    "#             songs_metadata = pd.concat([train_df,validation_df], ignore_index = True)\n",
    "#             audio_dataset = Dataset.from_dict(\n",
    "#                 {\"audio\": list(songs_metadata[\"file_name\"]),\n",
    "#                  \"transcription\": list(songs_metadata[\"transcription\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "#             audio_dataset[\"transcription\"] = audio_dataset[\"transcription\"] = re.sub(WAV2VEC2_ARGS.CHARS_TO_REMOVE_FROM_TRANSCRIPTS, '', audio_dataset[\"transcription\"]).upper()\n",
    "#             audio_dataset = audio_dataset.train_test_split(test_size=0.2, shuffle=True)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a239b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n",
    "from datasets import load_dataset,Dataset,Audio\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# class SpeechRecognitionDataset(Dataset):\n",
    "#     def __init__(self, dataset,args:TrainingArgs,processor):\n",
    "#         self.model_backbone = args.MODEL_BACKBONE\n",
    "#         self.dataset = dataset\n",
    "#         self.processor = processor\n",
    "#         self.tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(self.model_backbone)\n",
    "#         self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(self.model_backbone)\n",
    "#         self.processor = Wav2Vec2Processor(feature_extractor=self.feature_extractor, tokenizer=self.tokenizer)\n",
    "        \n",
    "#     def __getitem__(self, index):\n",
    "#         audio = self.dataset[index][\"audio\"][0]\n",
    "#         transcription = self.dataset[index][\"transcription\"][0]\n",
    "\n",
    "#         input_values = self.processor(audio[\"array\"], sampling_rate=16_000).input_values[0]\n",
    "#         with self.processor.as_target_processor():\n",
    "#             print(\"Entering the label encoder\")\n",
    "#             labels = self.processor(transcription,return_tensors = 'pt').input_ids\n",
    "#        # attention_mask = inputs.attention_mask.squeeze()\n",
    "#         return input_values, labels\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8e8b187-08b6-4e27-b3ec-7e1da0ad5ead",
   "metadata": {},
   "source": [
    "class SpeechRecognitionDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, WAV2VEC2_ARGS: WAV2VEC2_ARGS, num_workers):\n",
    "        super().__init__()\n",
    "        self.batch_size = WAV2VEC2_ARGS.BATCH_SIZE\n",
    "        self.num_workers = num_workers\n",
    "        self.tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(WAV2VEC2_ARGS.MODEL_BACKBONE)\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(WAV2VEC2_ARGS.MODEL_BACKBONE)\n",
    "        self.processor = Wav2Vec2Processor(feature_extractor=self.feature_extractor, tokenizer=self.tokenizer)\n",
    "        self.cpu_count = 1\n",
    "        print(f\"CPU Count = {self.cpu_count}\")\n",
    "        self.padding: Union[bool, str] = True\n",
    "        self.max_length: Optional[int] = None\n",
    "        self.max_length_labels: Optional[int] = None\n",
    "        self.pad_to_multiple_of: Optional[int] = None\n",
    "        self.pad_to_multiple_of_labels: Optional[int] = None\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        train_df = pd.read_csv(WAV2VEC2_ARGS.TRAIN_FILE_PATH).head(10)\n",
    "        validation_df = pd.read_csv(WAV2VEC2_ARGS.TEST_FILE_PATH).head(10)\n",
    "        #songs_metadata = pd.concat([train_df,validation_df], ignore_index = True)\n",
    "        if stage == 'fit' or stage is None:\n",
    "            print(\"In Stage = Fit\")\n",
    "            train_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(train_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(train_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "            \n",
    "            val_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(validation_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(validation_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "            \n",
    "            train_dataset = train_dataset.map(self.transform_huggingface_dataset, num_proc = self.cpu_count)\n",
    "            train_dataset =train_dataset.remove_columns(['audio','transcription','input_length'])\n",
    "            self.train_dataset = train_dataset.map(self.data_collate, num_proc = self.cpu_count)\n",
    "\n",
    "            \n",
    "            val_dataset = val_dataset.map(self.transform_huggingface_dataset, num_proc = self.cpu_count,)\n",
    "            val_dataset =val_dataset.remove_columns(['audio','transcription','input_length'])\n",
    "            self.val_dataset = val_dataset.map(self.data_collate, num_proc = self.cpu_count)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            print(\"In Stage = Test\")\n",
    "            val_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(validation_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(validation_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "          #  val_dataset[\"transcription\"] = val_dataset[\"transcription\"] = re.sub(WAV2VEC2_ARGS.CHARS_TO_REMOVE_FROM_TRANSCRIPTS, '', val_dataset[\"transcription\"]).upper()\n",
    "            val_dataset = val_dataset.map(self.transform_huggingface_dataset, num_proc = self.cpu_count)\n",
    "            #self.test_dataset = SpeechRecognitionDataset(val_dataset,WAV2VEC2_ARGS,self.processor)\n",
    "            self.test_dataset = val_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        print(\"entering train data loader\")\n",
    "        return DataLoader(\n",
    "            self.train_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        print(\"entering val data loader\")\n",
    "        return DataLoader(\n",
    "            self.val_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        print(\"entering test data loader\")\n",
    "        return DataLoader(\n",
    "            self.test_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "    \n",
    "    def transform_huggingface_dataset(self, batch: Dataset) -> Dataset:\n",
    "        audio = batch[\"audio\"]\n",
    "        batch[\"input_values\"] = self.processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "        batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "        with self.processor.as_target_processor():\n",
    "            batch[\"labels\"] = self.processor(batch[\"transcription\"]).input_ids\n",
    "        return batch\n",
    "\n",
    "    def data_collate(self, batch: Dataset) :\n",
    "        input_features = [{\"input_values\": batch[\"input_values\"]}]\n",
    "        label_features = [{\"input_ids\": batch[\"labels\"]}]\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )      \n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ed33833-367d-48be-a594-389fa4aef1d8",
   "metadata": {},
   "source": [
    "data = SpeechRecognitionDataModule(WAV2VEC2_ARGS, num_workers = 0)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d7bd241-6c23-48c0-a557-fd3cd663add3",
   "metadata": {},
   "source": [
    "data.setup()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "64526ed0-9e12-4065-b4d3-4e484d573e92",
   "metadata": {},
   "source": [
    "next(iter(data.val_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bc4a02-0e0b-4462-bb7b-79e70cbec785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96306ffe-8df9-4e84-a994-d7d92f614a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Union, List, Dict\n",
    "# import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "# from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "\n",
    "# class DataCollatorCTCWithPadding:\n",
    "#     \"\"\"\n",
    "#     Data collator that will dynamically pad the inputs received.\n",
    "#     Args:\n",
    "#         processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "#             The processor used for proccessing the data.\n",
    "#         padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "#             Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "#             among:\n",
    "#             * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "#               sequence if provided).\n",
    "#             * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "#               maximum acceptable input length for the model if that argument is not provided.\n",
    "#             * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "#               different lengths).\n",
    "#         max_length (:obj:`int`, `optional`):\n",
    "#             Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "#         max_length_labels (:obj:`int`, `optional`):\n",
    "#             Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "#         pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "#             If set will pad the sequence to a multiple of the provided value.\n",
    "#             This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "#             7.5 (Volta).\n",
    "#     \"\"\"\n",
    "\n",
    "#     processor: Wav2Vec2Processor\n",
    "#     padding: Union[bool, str] = True\n",
    "#     max_length: Optional[int] = None\n",
    "#     max_length_labels: Optional[int] = None\n",
    "#     pad_to_multiple_of: Optional[int] = None\n",
    "#     pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "#     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "#         # split inputs and labels since they have to be of different lengths and need\n",
    "#         # different padding methods\n",
    "#         input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "#         label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "#         batch = self.processor.pad(\n",
    "#             input_features,\n",
    "#             padding=self.padding,\n",
    "#             max_length=self.max_length,\n",
    "#             pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "#             return_tensors=\"pt\",\n",
    "#         )\n",
    "#         with self.processor.as_target_processor():\n",
    "#             labels_batch = self.processor.pad(\n",
    "#                 label_features,\n",
    "#                 padding=self.padding,\n",
    "#                 max_length=self.max_length_labels,\n",
    "#                 pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "#                 return_tensors=\"pt\",\n",
    "#             )\n",
    "\n",
    "#         # replace padding with -100 to ignore loss correctly\n",
    "#         labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "#         batch[\"labels\"] = labels\n",
    "\n",
    "#         return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cdad31-2268-4085-a6d4-6f86eb7558c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer, BertModel\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "# text = \"Replace me by any text you'd like.\"\n",
    "# encoded_input = tokenizer(text, return_tensors='pt')\n",
    "# output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c217f6c-0f28-48c6-92b0-0ca5755253b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Dict, List\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        label_attention_features =[{\"input_ids\": feature[\"label_attention_mask\"]} for feature in features]\n",
    "        attention_features =[{\"input_values\": feature[\"attention_mask\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\")\n",
    "\n",
    "        batch_attention = self.processor.pad(\n",
    "            attention_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\")\n",
    "    \n",
    "            \n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        with self.processor.as_target_processor():\n",
    "            label_attention_batch = self.processor.pad(\n",
    "                label_attention_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "                \n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels_attention = label_attention_batch[\"input_ids\"].masked_fill(labels_batch.input_ids.eq(101), 0).masked_fill(labels_batch.input_ids.eq(102), 0)\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        \n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        batch[\"label_attention_masks\"] = labels_attention\n",
    "        batch[\"attention_mask\"] = batch_attention[\"input_values\"]\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5405a62-6cc2-4291-b105-4f2f9c7aaa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor,BertTokenizer\n",
    "from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC, AutoModel,AutoProcessor\n",
    "\n",
    "from datasets import load_dataset,Dataset,Audio\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "WAV2VEC2_ARGS.BATCH_SIZE = 2\n",
    "class SpeechRecognitionDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, WAV2VEC2_ARGS: WAV2VEC2_ARGS, num_workers):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.batch_size = WAV2VEC2_ARGS.BATCH_SIZE\n",
    "        self.num_workers = num_workers\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(hparams.wav2vec2_model)\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        #self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(WAV2VEC2_ARGS.MODEL_BACKBONE)\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "        #self.feature_extractor = AutoFeatureExtractor.from_pretrained(hparams.wav2vec2_model)\n",
    "        self.processor = Wav2Vec2Processor(feature_extractor=self.feature_extractor, tokenizer=self.bert_tokenizer)\n",
    "        #self.processor = AutoProcessor(feature_extractor=self.feature_extractor, tokenizer=self.bert_tokenizer)\n",
    "        #self.processor = WhisperProcessor(feature_extractor=self.feature_extractor, tokenizer=self.bert_tokenizer)\n",
    "        self.data_collator = DataCollatorCTCWithPadding(processor=self.processor, padding=True)\n",
    "        self.cpu_count = 4\n",
    "        print(f\"CPU Count = {self.cpu_count}\")\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        train_df = pd.read_csv(WAV2VEC2_ARGS.TRAIN_FILE_PATH).head(10)\n",
    "        validation_df = pd.read_csv(WAV2VEC2_ARGS.TEST_FILE_PATH).head(10)\n",
    "        if stage == 'fit' or stage is None:\n",
    "            print(\"In Stage = Fit\")\n",
    "            train_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(train_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(train_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "            self.train_dataset = train_dataset.map(self.prepare_dataset,remove_columns = train_dataset.column_names)\n",
    "            \n",
    "            val_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(validation_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(validation_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "            self.val_dataset = val_dataset.map(self.prepare_dataset,remove_columns = val_dataset.column_names)\n",
    "\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            print(\"In Stage = Test\")\n",
    "            test_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(validation_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(validation_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "            test_dataset = val_dataset.map(self.prepare_dataset,remove_columns = val_dataset.column_names)\n",
    "            self.test_dataset = test_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        print(\"entering train data loader\")\n",
    "        return DataLoader(\n",
    "            self.train_dataset.with_format(\"torch\"), \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn = self.data_collator\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        print(\"entering val data loader\")\n",
    "        return DataLoader(\n",
    "            self.val_dataset.with_format(\"torch\"), \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn = self.data_collator\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        print(\"entering test data loader\")\n",
    "        return DataLoader(\n",
    "            self.test_dataset.with_format(\"torch\"), \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn = self.data_collator\n",
    "        )\n",
    "\n",
    "    def prepare_dataset(self,batch):\n",
    "        audio = batch[\"audio\"]\n",
    "        # batched output is \"un-batched\" to ensure mapping is correct\n",
    "        batch[\"input_values\"] = self.processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "        batch[\"attention_mask\"] = self.processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).attention_mask[0]\n",
    "        batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "        with self.processor.as_target_processor():\n",
    "            batch[\"labels\"] = self.processor(batch[\"transcription\"]).input_ids\n",
    "            batch[\"label_attention_mask\"] = self.processor(batch[\"transcription\"]).attention_mask\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "raw",
   "id": "75fe7b63-806e-46e7-8007-6223196529a5",
   "metadata": {},
   "source": [
    "data = SpeechRecognitionDataModule(WAV2VEC2_ARGS, num_workers = 0)\n",
    "data.setup()\n",
    "data.val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44155934-62d0-4943-afb7-9891bbd11a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.val_dataset[0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d90ec0e4-4a82-4f5b-a545-14a2be55dd00",
   "metadata": {},
   "source": [
    "print(next(iter(data.val_dataloader())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3d9597-6db9-48ff-b8db-4fe73bef6ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Model, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC, AutoModel\n",
    "\n",
    "class Wav2SeqModel(LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.batch_size = hparams.batch_size\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.wav2vec2 = AutoModelForCTC.from_pretrained(hparams.wav2vec2_model,ctc_zero_infinity=True,ctc_loss_reduction=\"mean\")\n",
    "        self.wav2vec2.eval()\n",
    "        #self.wav2vec2 = AutoModel.from_pretrained(hparams.wav2vec2_model)\n",
    "        #self.wav2vec2.eval()\n",
    "        print(self.wav2vec2.config)\n",
    "        self.seq2seq = AutoModelForCausalLM.from_pretrained(hparams.lm_model)\n",
    "        self.seq2seq.config.is_decoder = True\n",
    "        self.seq2seq.add_cross_attention = True\n",
    "        print(self.seq2seq.config)\n",
    "\n",
    "    def forward(self, audio, attention_mask, labels, label_attention_mask):\n",
    "        #print(\"entering forward step\")\n",
    "        encoder_outputs = self.wav2vec2(audio[0],\n",
    "                                        attention_mask=attention_mask, #Made a change here by adding a [0]\n",
    "                                        output_hidden_states=True,\n",
    "                                        output_attentions=True)\n",
    "        encoder_hidden_states = encoder_outputs[0]  \n",
    "        print(f\"encoder_hidden_states={encoder_hidden_states}\")\n",
    "        encoder_attention_mask = self.wav2vec2._get_feature_vector_attention_mask(\n",
    "                encoder_hidden_states.shape[1], attention_mask\n",
    "            )\n",
    "        print(f\"[PRIOR]decoder_input_ids={labels}\")\n",
    "        print(f\"[PRIOR]decoder_attention_masks={label_attention_mask}\")\n",
    "        decoder_input_ids = self.shift_tokens_right(labels, 0, 101)\n",
    "        decoder_attention_masks = self.shift_tokens_right_mask(label_attention_mask)\n",
    "        print(f\"[AFTER]decoder_input_ids={decoder_input_ids}\")\n",
    "        print(f\"[AFTER]decoder_attention_masks={decoder_attention_masks}\")\n",
    "        decoder_outputs = self.seq2seq(input_ids=decoder_input_ids,\n",
    "                                       attention_mask = decoder_attention_masks,\n",
    "                                       encoder_hidden_states=encoder_hidden_states,\n",
    "                                      encoder_attention_mask=encoder_attention_mask) #,attention_mask = x.attentions, decoder_input_ids=predicted_ids\n",
    "        return decoder_outputs\n",
    "\n",
    "    def generate(self, audio):\n",
    "        encoder_outputs = self.wav2vec2(audio[0],output_hidden_states=True,output_attentions=True)\n",
    "        encoder_hidden_states = encoder_outputs[0]\n",
    "        bos_ids = (\n",
    "            torch.ones(\n",
    "                (encoder_hidden_states.size()[0], 1),\n",
    "                dtype=torch.long,\n",
    "                device=self.seq2seq.device,\n",
    "            )\n",
    "            * self.seq2seq.config.pad_token_id\n",
    "        )\n",
    "        return self.seq2seq.generate(\n",
    "            input_ids=bos_ids,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        #print(\"entering training step\")\n",
    "        audio = batch[\"input_values\"].unsqueeze(0)\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        label_attention_mask = batch[\"label_attention_masks\"].unsqueeze(0)\n",
    "        #print(f\"audio={audio}, attention_mask = {attention_mask}, label_attention_mask = {label_attention_mask}\")\n",
    "        labels = batch[\"labels\"]\n",
    "        labels = labels.reshape(self.batch_size,-1)\n",
    "        logits = self(audio,attention_mask,labels,label_attention_mask).logits\n",
    "        logits = logits.reshape(-1, self.batch_size, self.seq2seq.config.vocab_size)\n",
    "        input_lengths = torch.full(size=(self.batch_size,), fill_value=logits.shape[0], dtype=torch.long)\n",
    "        target_lengths = torch.full(size=(self.batch_size,), fill_value=labels.shape[0], dtype=torch.long)\n",
    "        # ctc_loss =  nn.CTCLoss(blank=0)\n",
    "        # loss = ctc_loss(logits,labels,input_lengths,target_lengths)\n",
    "        ce_loss = nn.CrossEntropyLoss()\n",
    "        loss = ce_loss(logits, labels)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch,batch_idx):\n",
    "        #print(\"entering validation step\")\n",
    "        audio = batch[\"input_values\"].unsqueeze(0)\n",
    "        labels = batch[\"labels\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        label_attention_mask = batch[\"label_attention_masks\"].unsqueeze(0)\n",
    "        #print(f\"audio={audio}, attention_mask = {attention_mask}, label_attention_mask = {label_attention_mask}\")\n",
    "        labels = labels.reshape(self.batch_size,-1)\n",
    "        logits = self(audio,attention_mask,labels,label_attention_mask).logits\n",
    "        logits = logits.reshape(-1, self.batch_size, self.seq2seq.config.vocab_size)\n",
    "        input_lengths = torch.full(size=(self.batch_size,), fill_value=logits.shape[0], dtype=torch.long)\n",
    "        target_lengths = torch.full(size=(self.batch_size,), fill_value=labels.shape[0], dtype=torch.long)\n",
    "        # ctc_loss =  nn.CTCLoss(blank=0)\n",
    "        # loss = ctc_loss(logits,labels,input_lengths,target_lengths)\n",
    "        ce_loss = nn.CrossEntropyLoss()\n",
    "        loss = ce_loss(logits, labels)\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        label_decoded = labels.type(torch.int64).tolist()\n",
    "        #print(f\"labels = {labels}\")\n",
    "        #print(f\"predicted ids = {predicted_ids}\")\n",
    "        print(f\"original text = {self.bert_tokenizer.decode(label_decoded[0])},{self.bert_tokenizer.decode(label_decoded[1])}\")\n",
    "        predicted_text = predicted_ids.type(torch.int64)\n",
    "        print(f\"Predicted text = {self.bert_tokenizer.decode(predicted_text[:,0].flatten().tolist())},{self.bert_tokenizer.decode(predicted_text[:,-1].flatten().tolist())}\")\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=hparams.learning_rate)\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
    "        \"\"\"\n",
    "        Shift input ids one token to the right.\n",
    "        \"\"\"\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "        if decoder_start_token_id is None:\n",
    "            raise ValueError(\"Make sure to set the decoder_start_token_id attribute of the model's configuration.\")\n",
    "        shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "    \n",
    "        if pad_token_id is None:\n",
    "            raise ValueError(\"Make sure to set the pad_token_id attribute of the model's configuration.\")\n",
    "        # replace possible -100 values in labels by `pad_token_id`\n",
    "        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "    \n",
    "        return shifted_input_ids\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_tokens_right_mask(input_ids: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Shift input ids one token to the right.\n",
    "        \"\"\"\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[:,:, 1:] = input_ids[:,:, :-1].clone()\n",
    "        shifted_input_ids[:,:, 0] = 0\n",
    "        return shifted_input_ids.squeeze()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1365d597-7bd4-420e-8077-b021455357df",
   "metadata": {},
   "source": [
    "test_data = SpeechRecognitionDataModule(WAV2VEC2_ARGS, num_workers = 0)\n",
    "test_data.setup()\n",
    "print(next(iter(test_data.val_dataloader())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93bcca1-79eb-4f51-9dd3-368e192fcfcd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pytorch_lightning as pl\n",
    "hparams = argparse.Namespace()\n",
    "hparams.wav2vec2_model = 'facebook/wav2vec2-large-960h-lv60-self' #'openai/whisper-base'# #'openai/whisper-base'\n",
    "hparams.lm_model = 'bert-base-uncased' #'facebook/bart-large'\n",
    "hparams.vocab_size = 30000\n",
    "hparams.learning_rate = 0.00001\n",
    "hparams.batch_size = WAV2VEC2_ARGS.BATCH_SIZE\n",
    "\n",
    "model = Wav2SeqModel(hparams)\n",
    "trainer = Trainer(max_epochs=10,devices=1, accelerator=\"gpu\")\n",
    "trainer.fit(model,SpeechRecognitionDataModule(WAV2VEC2_ARGS,num_workers=4))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3fa8d2a-177c-40b8-b166-108a04b7c4ae",
   "metadata": {},
   "source": [
    "import librosa \n",
    "def create_audio_tensor(audio_path):\n",
    "    audio, sample_rate = librosa.load(audio_path)\n",
    "    audio_tensor = torch.from_numpy(audio).float()\n",
    "    audio_tensor = audio_tensor.unsqueeze(0)\n",
    "    return audio_tensor\n",
    "audio_tensor = create_audio_tensor(\"notebooks/separated/mdx_extra/test_clip/vocals.wav\")\n",
    "print(audio_tensor.unsqueeze(0).shape)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_hat = model.generate(audio_tensor.unsqueeze(0))\n",
    "torch.argmax(y_hat, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac5538f-c4f3-4c19-a729-26f38aff7b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f92b482-a046-4c0f-bcd3-2ca40b2fc33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_tensor = next(iter(test_data.val_dataloader()))[\"input_values\"]\n",
    "audio_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9938cd6-0d0e-4914-89cd-9772ac41a9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_hat = model.generate(audio_tensor.unsqueeze(0))\n",
    "torch.argmax(y_hat, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3838261-9da4-46a1-8a35-543a359e81cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import librosa\n",
    "# import torch\n",
    "# from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "# def create_audio_tensor(audio_path):\n",
    "#     audio, sample_rate = librosa.load(audio_path)\n",
    "#     audio_tensor = torch.from_numpy(audio).float()\n",
    "#     audio_tensor = audio_tensor.unsqueeze(0)\n",
    "#     return audio_tensor\n",
    "\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "# audio_tensor = create_audio_tensor(\"notebooks/separated/mdx_extra/test_clip/vocals.wav\")\n",
    "logits = model(audio_tensor.unsqueeze(0),torch.Tensor(18, 11,  0,  0,  0,  0,  0,  7,  0,  0,  0,  0,  6,  0,  0,  0,  4,  4,\n",
    "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  5,  5,  0,  0,  0,\n",
    "          0, 15, 15,  0,  0,  0,  0,  0,  0, 12,  0,  0,  0,  4,  4,  0,  0,  0,\n",
    "         14,  0,  0,  0,  0, 10,  0,  0, 12, 12,  0,  0,  0,  0,  0, 19,  0,  0,\n",
    "          0,  0,  0,  0,  0,  0,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
    "          0,  0,  0,  0,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  6, 11,\n",
    "          0,  0,  5,  0,  0,  0,  4,  4,  0,  0,  0, 24,  0,  0,  0,  0,  0,  0,\n",
    "          0,  0,  0,  5,  0,  0,  0,  0,  0,  9,  0,  0,  0,  0,  4,  4,  4,  0,\n",
    "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
    "          0,  0,  0))\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "print(audio_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c2fb4-ab29-433a-ab4f-5e895a1aaf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e40856-59d1-4d5e-84c3-ab64b528f23c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96624e0-53a7-47c0-bb31-07da5ad31208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4686fb-f6f8-4b08-af94-a9d18762a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.LogSoftmax(dim=1)\n",
    "input = torch.randn(2, 3)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa479c4-9660-4ee0-b8f1-7cabac8d3480",
   "metadata": {},
   "outputs": [],
   "source": [
    "input,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b60aad4-07ca-422c-a376-fab2cd8cbdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor,BertTokenizer\n",
    "from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC, AutoModel,AutoProcessor\n",
    "\n",
    "from datasets import load_dataset,Dataset,Audio\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "class SpeechRecognitionDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, WAV2VEC2_ARGS: WAV2VEC2_ARGS, num_workers):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.batch_size = WAV2VEC2_ARGS.BATCH_SIZE\n",
    "        self.num_workers = num_workers\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(hparams.wav2vec2_model)\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        #self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(WAV2VEC2_ARGS.MODEL_BACKBONE)\n",
    "        #self.feature_extractor = AutoFeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(hparams.wav2vec2_model)\n",
    "        #self.processor = Wav2Vec2Processor(feature_extractor=self.feature_extractor, tokenizer=self.tokenizer)\n",
    "        #self.processor = AutoProcessor(feature_extractor=self.feature_extractor, tokenizer=self.bert_tokenizer)\n",
    "        self.processor = WhisperProcessor(feature_extractor=self.feature_extractor, tokenizer=self.bert_tokenizer)\n",
    "        self.data_collator = DataCollatorCTCWithPadding(processor=self.processor, padding=True)\n",
    "        self.cpu_count = 4\n",
    "        print(f\"CPU Count = {self.cpu_count}\")\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        train_df = pd.read_csv(WAV2VEC2_ARGS.TRAIN_FILE_PATH).head(10)\n",
    "        validation_df = pd.read_csv(WAV2VEC2_ARGS.TEST_FILE_PATH).head(10)\n",
    "        if stage == 'fit' or stage is None:\n",
    "            print(\"In Stage = Fit\")\n",
    "            train_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(train_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(train_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "            self.train_dataset = train_dataset.map(self.prepare_dataset,remove_columns = train_dataset.column_names)\n",
    "            \n",
    "            val_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(validation_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(validation_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "            self.val_dataset = val_dataset.map(self.prepare_dataset,remove_columns = val_dataset.column_names)\n",
    "\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            print(\"In Stage = Test\")\n",
    "            test_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(validation_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(validation_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "            test_dataset = val_dataset.map(self.prepare_dataset,remove_columns = val_dataset.column_names)\n",
    "            self.test_dataset = test_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        print(\"entering train data loader\")\n",
    "        return DataLoader(\n",
    "            self.train_dataset.with_format(\"torch\"), \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn = self.data_collator\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        print(\"entering val data loader\")\n",
    "        return DataLoader(\n",
    "            self.val_dataset.with_format(\"torch\"), \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn = self.data_collator\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        print(\"entering test data loader\")\n",
    "        return DataLoader(\n",
    "            self.test_dataset.with_format(\"torch\"), \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn = self.data_collator\n",
    "        )\n",
    "\n",
    "    def prepare_dataset(self,batch):\n",
    "        audio = batch[\"audio\"]\n",
    "        # batched output is \"un-batched\" to ensure mapping is correct\n",
    "        batch[\"input_values\"] = self.processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "        batch[\"attention_mask\"] = self.processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).attention_mask[0]\n",
    "        batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "        with self.processor.as_target_processor():\n",
    "            batch[\"labels\"] = self.processor(batch[\"transcription\"]).input_ids\n",
    "            batch[\"label_attention_mask\"] = self.processor(batch[\"transcription\"]).attention_mask\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a286c7f4-b4f8-4849-a121-7b7337b701c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Model, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC, AutoModel\n",
    "\n",
    "class Wav2SeqModel(LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.batch_size = hparams.batch_size\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        #self.wav2vec2 = AutoModelForCTC.from_pretrained(hparams.wav2vec2_model,ctc_zero_infinity=True,ctc_loss_reduction=\"mean\")\n",
    "        #self.wav2vec2.freeze_feature_encoder()\n",
    "        self.wav2vec2 = AutoModel.from_pretrained(hparams.wav2vec2_model)\n",
    "        self.wav2vec2.eval()\n",
    "        print(self.wav2vec2.config)\n",
    "        self.seq2seq = AutoModelForCausalLM.from_pretrained(hparams.lm_model)\n",
    "        self.seq2seq.config.is_decoder = True\n",
    "        self.seq2seq.add_cross_attention = True\n",
    "        print(self.seq2seq.config)\n",
    "\n",
    "    def forward(self, audio, attention_mask, labels, label_attention_mask):\n",
    "        #print(\"entering forward step\")\n",
    "        encoder_outputs = self.wav2vec2(audio[0],\n",
    "                                        attention_mask=attention_mask, #Made a change here by adding a [0]\n",
    "                                        output_hidden_states=True,\n",
    "                                        output_attentions=True)\n",
    "        encoder_hidden_states = encoder_outputs[0]  \n",
    "        print(f\"encoder_hidden_states={encoder_hidden_states}\")\n",
    "        encoder_attention_mask = self.wav2vec2._get_feature_vector_attention_mask(\n",
    "                encoder_hidden_states.shape[1], attention_mask\n",
    "            )\n",
    "        #print(f\"[PRIOR]decoder_input_ids={labels}\")\n",
    "        #print(f\"[PRIOR]decoder_attention_masks={label_attention_mask}\")\n",
    "        decoder_input_ids = self.shift_tokens_right(labels, 0, 101)\n",
    "        decoder_attention_masks = self.shift_tokens_right_mask(label_attention_mask)\n",
    "        #print(f\"[AFTER]decoder_input_ids={decoder_input_ids}\")\n",
    "        #print(f\"[AFTER]decoder_attention_masks={decoder_attention_masks}\")\n",
    "        decoder_outputs = self.seq2seq(input_ids=decoder_input_ids,\n",
    "                                       attention_mask = decoder_attention_masks,\n",
    "                                       encoder_hidden_states=encoder_hidden_states,\n",
    "                                      encoder_attention_mask=encoder_attention_mask) #,attention_mask = x.attentions, decoder_input_ids=predicted_ids\n",
    "        return decoder_outputs\n",
    "\n",
    "    def generate(self, audio):\n",
    "        encoder_outputs = self.wav2vec2(audio[0],output_hidden_states=True,output_attentions=True)\n",
    "        encoder_hidden_states = encoder_outputs[0]\n",
    "        bos_ids = (\n",
    "            torch.ones(\n",
    "                (encoder_hidden_states.size()[0], 1),\n",
    "                dtype=torch.long,\n",
    "                device=self.seq2seq.device,\n",
    "            )\n",
    "            * self.seq2seq.config.pad_token_id\n",
    "        )\n",
    "        return self.seq2seq.generate(\n",
    "            input_ids=bos_ids,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        #print(\"entering training step\")\n",
    "        audio = batch[\"input_values\"].unsqueeze(0)\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        label_attention_mask = batch[\"label_attention_masks\"].unsqueeze(0)\n",
    "        #print(f\"audio={audio}, attention_mask = {attention_mask}, label_attention_mask = {label_attention_mask}\")\n",
    "        labels = batch[\"labels\"]\n",
    "        labels = labels.reshape(self.batch_size,-1)\n",
    "        logits = self(audio,attention_mask,labels,label_attention_mask).logits\n",
    "        logits = logits.reshape(-1, self.batch_size, self.seq2seq.config.vocab_size)\n",
    "        input_lengths = torch.full(size=(self.batch_size,), fill_value=logits.shape[0], dtype=torch.long)\n",
    "        target_lengths = torch.full(size=(self.batch_size,), fill_value=labels.shape[0], dtype=torch.long)\n",
    "        ctc_loss =  nn.CTCLoss(blank=0)\n",
    "        loss = ctc_loss(logits,labels,input_lengths,target_lengths)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch,batch_idx):\n",
    "        #print(\"entering validation step\")\n",
    "        audio = batch[\"input_values\"].unsqueeze(0)\n",
    "        labels = batch[\"labels\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        label_attention_mask = batch[\"label_attention_masks\"].unsqueeze(0)\n",
    "        #print(f\"audio={audio}, attention_mask = {attention_mask}, label_attention_mask = {label_attention_mask}\")\n",
    "        labels = labels.reshape(self.batch_size,-1)\n",
    "        logits = self(audio,attention_mask,labels,label_attention_mask).logits\n",
    "        logits = logits.reshape(-1, self.batch_size, self.seq2seq.config.vocab_size)\n",
    "        input_lengths = torch.full(size=(self.batch_size,), fill_value=logits.shape[0], dtype=torch.long)\n",
    "        target_lengths = torch.full(size=(self.batch_size,), fill_value=labels.shape[0], dtype=torch.long)\n",
    "        ctc_loss =  nn.CTCLoss(blank=0)\n",
    "        loss = ctc_loss(logits,labels,input_lengths,target_lengths)\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        label_decoded = labels.type(torch.int64).tolist()\n",
    "        #print(f\"labels = {labels}\")\n",
    "        #print(f\"predicted ids = {predicted_ids}\")\n",
    "        print(f\"original text = {self.bert_tokenizer.decode(label_decoded[0])},{self.bert_tokenizer.decode(label_decoded[1])}\")\n",
    "        predicted_text = predicted_ids.type(torch.int64)\n",
    "        print(f\"Predicted text = {self.bert_tokenizer.decode(predicted_text[:,0].flatten().tolist())},{self.bert_tokenizer.decode(predicted_text[:,-1].flatten().tolist())}\")\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=hparams.learning_rate)\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
    "        \"\"\"\n",
    "        Shift input ids one token to the right.\n",
    "        \"\"\"\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "        if decoder_start_token_id is None:\n",
    "            raise ValueError(\"Make sure to set the decoder_start_token_id attribute of the model's configuration.\")\n",
    "        shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "    \n",
    "        if pad_token_id is None:\n",
    "            raise ValueError(\"Make sure to set the pad_token_id attribute of the model's configuration.\")\n",
    "        # replace possible -100 values in labels by `pad_token_id`\n",
    "        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "    \n",
    "        return shifted_input_ids\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_tokens_right_mask(input_ids: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Shift input ids one token to the right.\n",
    "        \"\"\"\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[:,:, 1:] = input_ids[:,:, :-1].clone()\n",
    "        shifted_input_ids[:,:, 0] = 0\n",
    "        return shifted_input_ids.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41800a8c-b400-4f13-acc7-af7a56bbfa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(WAV2VEC2_ARGS.TRAIN_FILE_PATH)\n",
    "validation_df = pd.read_csv(WAV2VEC2_ARGS.TEST_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497b1e3a-6e1a-4d93-9f48-a970bf08eea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df.transcription_capitalized.str.len()>200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806eb51a-1af0-4b4b-a02d-f26c16beed6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93f879b6-a836-4798-86ed-6bf40d44fc81",
   "metadata": {},
   "source": [
    "# Readying the codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9aa9d25-0b49-480f-87fd-a7a1439cea0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgreeshmasmenon\u001b[0m (\u001b[33msongslyricstranscription\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230823_012129-s8xfzotr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20transfer%20learning/runs/s8xfzotr' target=\"_blank\">lyric-breeze-210</a></strong> to <a href='https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20transfer%20learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20transfer%20learning' target=\"_blank\">https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20transfer%20learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20transfer%20learning/runs/s8xfzotr' target=\"_blank\">https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20transfer%20learning/runs/s8xfzotr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TRAIN_FILE_PATH\": \"/scratch/users/gmenon/train_song_metadata_en_demucs_cleaned_filtered_095.csv\",\n",
      "    \"TEST_FILE_PATH\": \"/scratch/users/gmenon/validation_song_metadata_en_demucs_cleaned_filtered_005.csv\",\n",
      "    \"MODEL_BACKBONE\": \"facebook/wav2vec2-large-960h-lv60-self\",\n",
      "    \"BATCH_SIZE\": 1,\n",
      "    \"NUM_EPOCHS\": 15,\n",
      "    \"MODEL_SAVE_PATH\": \"/scratch/users/gmenon//model_artefacts/wav2vec2_demucs_en_large-960h-lv60-self_freeze_unfreeze_15epochs_adamw.pt\",\n",
      "    \"FINETUNE_STRATEGY\": [\n",
      "        \"freeze_unfreeze\",\n",
      "        10\n",
      "    ],\n",
      "    \"LR_SCHEDULER\": \"reduce_on_plateau_schedule\"\n",
      "}\n",
      "Namespace(batch_size=1, learning_rate=1e-05, lm_model='bert-base-cased', vocab_size=20000, wav2vec2_model='facebook/wav2vec2-large-960h-lv60-self')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 'facebook/wav2vec2-large-960h-lv60-self' provided by Hugging Face/transformers (https://github.com/huggingface/transformers).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Count = 4\n",
      "In Stage = Fit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ee9343fc0848948c90c64a0afec8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08da94a405a547c18ccd904b51e9731a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type            | Params\n",
      "---------------------------------------------\n",
      "0 | wav2vec2 | Wav2Vec2ForCTC  | 315 M \n",
      "1 | seq2seq  | BertLMHeadModel | 108 M \n",
      "---------------------------------------------\n",
      "423 M     Trainable params\n",
      "0         Non-trainable params\n",
      "423 M     Total params\n",
      "1,695.249 Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering Optimization Step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entering val data loader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text = [CLS] so learn from your mistakes [SEP], labels = tensor([  101,  1177,  3858,  1121,  1240, 12572,   102], device='cuda:0')\n",
      "Predicted text = .... from your mistakes, predicted ids = tensor([  119,   119,   119,   119,  1121,  1240, 12572], device='cuda:0')\n",
      "original text = [CLS] i've been connected to the right line [SEP], labels = tensor([ 101,  178,  112, 1396, 1151, 3387, 1106, 1103, 1268, 1413,  102],\n",
      "       device='cuda:0')\n",
      "Predicted text = ...'ve been'to the right line, predicted ids = tensor([ 119,  119,  119,  112, 1396, 1151,  112, 1106, 1103, 1268, 1413],\n",
      "       device='cuda:0')\n",
      "entering train data loader\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d605ee34fe496db5e2ff713b9bfe58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text = [CLS] so learn from your mistakes [SEP], labels = tensor([  101,  1177,  3858,  1121,  1240, 12572,   102], device='cuda:0')\n",
      "Predicted text = [CLS] i i to the heart [SEP], predicted ids = tensor([ 101,  178,  178, 1106, 1103, 1762,  102], device='cuda:0')\n",
      "original text = [CLS] i've been connected to the right line [SEP], labels = tensor([ 101,  178,  112, 1396, 1151, 3387, 1106, 1103, 1268, 1413,  102],\n",
      "       device='cuda:0')\n",
      "Predicted text = [CLS] i'm been in [SEP] the world way [SEP], predicted ids = tensor([ 101,  178,  112,  182, 1151, 1107,  102, 1103, 1362, 1236,  102],\n",
      "       device='cuda:0')\n",
      "original text = [CLS] the truth to be found [SEP], labels = tensor([ 101, 1103, 3062, 1106, 1129, 1276,  102], device='cuda:0')\n",
      "Predicted text = [CLS] i way of give the [SEP], predicted ids = tensor([ 101,  178, 1236, 1104, 1660, 1103,  102], device='cuda:0')\n",
      "original text = [CLS] he said the way myblue eyes shined [SEP], labels = tensor([  101,  1119,  1163,  1103,  1236,  1139,  1830, 19224,  1257, 18978,\n",
      "         1181,   102], device='cuda:0')\n",
      "Predicted text = [CLS] i'' other to heart [SEP] [SEP] [SEP] [SEP] [SEP], predicted ids = tensor([ 101,  178,  112,  112, 1168, 1106, 1762,  102,  102,  102,  102,  102],\n",
      "       device='cuda:0')\n",
      "original text = [CLS] you leave me once again home alone [SEP], labels = tensor([ 101, 1128, 1817, 1143, 1517, 1254, 1313, 2041,  102], device='cuda:0')\n",
      "Predicted text = [CLS] i'to [SEP] [SEP] again [SEP] [SEP], predicted ids = tensor([ 101,  178,  112, 1106,  102,  102, 1254,  102,  102], device='cuda:0')\n",
      "original text = [CLS] while they are in commend [SEP], labels = tensor([ 101, 1229, 1152, 1132, 1107, 3254, 2354, 1181,  102], device='cuda:0')\n",
      "Predicted text = [CLS] i i'a the [SEP] [SEP] [SEP], predicted ids = tensor([ 101,  178,  178,  112,  170, 1103,  102,  102,  102], device='cuda:0')\n",
      "original text = [CLS] a life all mine [SEP], labels = tensor([ 101,  170, 1297, 1155, 2317,  102], device='cuda:0')\n",
      "Predicted text = [CLS] i girl [SEP] of [SEP], predicted ids = tensor([ 101,  178, 1873,  102, 1104,  102], device='cuda:0')\n",
      "original text = [CLS] so i never went back [SEP], labels = tensor([ 101, 1177,  178, 1309, 1355, 1171,  102], device='cuda:0')\n",
      "Predicted text = [CLS] i i'' to to, predicted ids = tensor([ 101,  178,  178,  112,  112, 1106, 1106], device='cuda:0')\n",
      "original text = [CLS] in you i taste god [SEP], labels = tensor([ 101, 1107, 1128,  178, 5080, 5540,  102], device='cuda:0')\n",
      "Predicted text = [CLS] i the'' like [SEP], predicted ids = tensor([ 101,  178, 1103,  112,  112, 1176,  102], device='cuda:0')\n",
      "original text = [CLS] could stay a while [SEP], labels = tensor([ 101, 1180, 2215,  170, 1229,  102], device='cuda:0')\n",
      "Predicted text = [CLS] i'[SEP] [SEP] [SEP], predicted ids = tensor([101, 178, 112, 102, 102, 102], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os. chdir('/home/users/gmenon/workspace/songsLyricsGenerator/src')\n",
    "from training import lyrics_finetune\n",
    "from constants.mir_constants import TrainingArgs, WAV2VEC2_ARGS\n",
    "import argparse\n",
    "hparams = argparse.Namespace()\n",
    "hparams.wav2vec2_model = 'facebook/wav2vec2-large-960h-lv60-self'\n",
    "hparams.lm_model = 'bert-base-cased' \n",
    "#hparams.lm_model = 'facebook/bart-base'\n",
    "hparams.vocab_size = 20000\n",
    "hparams.learning_rate = 1e-5\n",
    "hparams.batch_size = 1\n",
    "\n",
    "model,trainer = lyrics_finetune.run(hparams=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad626752-ff03-43a3-8e05-51d1652134ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Folder</th>\n",
       "      <th>consolidated_file_path</th>\n",
       "      <th>Length</th>\n",
       "      <th>transcription</th>\n",
       "      <th>transcription_capitalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0aa835068aa64515b2158c2f0b6513b7</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>4.754943</td>\n",
       "      <td>so learn from your mistakes</td>\n",
       "      <td>SO LEARN FROM YOUR MISTAKES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>089cd84d662e4c50954cfa46c07cae14</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.759388</td>\n",
       "      <td>i've been connected to the right line</td>\n",
       "      <td>I'VE BEEN CONNECTED TO THE RIGHT LINE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>127cc2aaff9a44fab21b2a0c4f5c3e5d</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.668254</td>\n",
       "      <td>the truth to be found</td>\n",
       "      <td>THE TRUTH TO BE FOUND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18e03c3980014046a3504ef42fe81ae8</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.895079</td>\n",
       "      <td>he said the way myblue eyes shined</td>\n",
       "      <td>HE SAID THE WAY MYBLUE EYES SHINED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07c8b8c0b0424c1e87389392695a551b</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.666939</td>\n",
       "      <td>you leave me once again home alone</td>\n",
       "      <td>YOU LEAVE ME ONCE AGAIN HOME ALONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0cb795c9b6e64a4abab5553c2b2ed3e0</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.742880</td>\n",
       "      <td>while they are in commend</td>\n",
       "      <td>WHILE THEY ARE IN COMMEND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0655395364614218be31548245853ca0</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>4.697710</td>\n",
       "      <td>a life all mine</td>\n",
       "      <td>A LIFE ALL MINE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>113bb605eb324aeb8db046b4193424a2</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.642948</td>\n",
       "      <td>so i never went back</td>\n",
       "      <td>SO I NEVER WENT BACK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00ecd4b71c094b959ae492ec138bb555</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>3.176508</td>\n",
       "      <td>in you i taste god</td>\n",
       "      <td>IN YOU I TASTE GOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0b5fa72c054e44598ec8aefb76ff855e</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.465011</td>\n",
       "      <td>could stay a while</td>\n",
       "      <td>COULD STAY A WHILE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Folder  \\\n",
       "0  0aa835068aa64515b2158c2f0b6513b7   \n",
       "1  089cd84d662e4c50954cfa46c07cae14   \n",
       "2  127cc2aaff9a44fab21b2a0c4f5c3e5d   \n",
       "3  18e03c3980014046a3504ef42fe81ae8   \n",
       "4  07c8b8c0b0424c1e87389392695a551b   \n",
       "5  0cb795c9b6e64a4abab5553c2b2ed3e0   \n",
       "6  0655395364614218be31548245853ca0   \n",
       "7  113bb605eb324aeb8db046b4193424a2   \n",
       "8  00ecd4b71c094b959ae492ec138bb555   \n",
       "9  0b5fa72c054e44598ec8aefb76ff855e   \n",
       "\n",
       "                              consolidated_file_path    Length  \\\n",
       "0  /scratch/users/gmenon/wav_clips/separated/htde...  4.754943   \n",
       "1  /scratch/users/gmenon/wav_clips/separated/htde...  2.759388   \n",
       "2  /scratch/users/gmenon/wav_clips/separated/htde...  2.668254   \n",
       "3  /scratch/users/gmenon/wav_clips/separated/htde...  2.895079   \n",
       "4  /scratch/users/gmenon/wav_clips/separated/htde...  2.666939   \n",
       "5  /scratch/users/gmenon/wav_clips/separated/htde...  2.742880   \n",
       "6  /scratch/users/gmenon/wav_clips/separated/htde...  4.697710   \n",
       "7  /scratch/users/gmenon/wav_clips/separated/htde...  2.642948   \n",
       "8  /scratch/users/gmenon/wav_clips/separated/htde...  3.176508   \n",
       "9  /scratch/users/gmenon/wav_clips/separated/htde...  2.465011   \n",
       "\n",
       "                           transcription  \\\n",
       "0            so learn from your mistakes   \n",
       "1  i've been connected to the right line   \n",
       "2                  the truth to be found   \n",
       "3     he said the way myblue eyes shined   \n",
       "4     you leave me once again home alone   \n",
       "5              while they are in commend   \n",
       "6                        a life all mine   \n",
       "7                   so i never went back   \n",
       "8                     in you i taste god   \n",
       "9                     could stay a while   \n",
       "\n",
       "               transcription_capitalized  \n",
       "0            SO LEARN FROM YOUR MISTAKES  \n",
       "1  I'VE BEEN CONNECTED TO THE RIGHT LINE  \n",
       "2                  THE TRUTH TO BE FOUND  \n",
       "3     HE SAID THE WAY MYBLUE EYES SHINED  \n",
       "4     YOU LEAVE ME ONCE AGAIN HOME ALONE  \n",
       "5              WHILE THEY ARE IN COMMEND  \n",
       "6                        A LIFE ALL MINE  \n",
       "7                   SO I NEVER WENT BACK  \n",
       "8                     IN YOU I TASTE GOD  \n",
       "9                     COULD STAY A WHILE  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "validation_df = pd.read_csv(WAV2VEC2_ARGS.TEST_FILE_PATH).head(10)\n",
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "887fe345-938f-430f-9e1f-ce4519e0c840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/scratch/users/gmenon/wav_clips/separated/htdemucs/0655395364614218be31548245853ca0/vocals.wav',\n",
       " 'a life all mine')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_num = 6\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor\n",
    "from transformers import BertTokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(hparams.lm_model)\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(sampling_rate=16_000, do_normalize=True, return_attention_mask=True)\n",
    "list(validation_df[\"consolidated_file_path\"])[sample_num],list(validation_df[\"transcription\"])[sample_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8fe6151-0dfd-4b53-a257-7e16a87f4cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset,Audio\n",
    "val_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(validation_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(validation_df[\"transcription\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "audio = val_dataset[\"audio\"][sample_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef057b1-1ca6-4564-b5fa-63f6a0e83437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "474a86b1-ab52-4d84-99b9-b6de63ab6b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape/BOS IDs = torch.Size([1, 1]),tensor([[101]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('a life all mine', '[CLS]')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model.eval()\n",
    "bos_ids = (torch.ones((1,1),dtype=torch.long)* 0)\n",
    "bos_ids[:,0] = 101\n",
    "\n",
    "logits = model.generate(torch.Tensor(feature_extractor(audio[\"array\"]).input_values[0]).unsqueeze(0),bos_ids).logits\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)[0]\n",
    "list(validation_df[\"transcription\"])[sample_num],bert_tokenizer.decode(predicted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7927b124-8f74-433a-9079-7a015f76354f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "544d3245-2648-4434-baf5-2249682358e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = trainer.datamodule\n",
    "# data.setup()\n",
    "# import torch\n",
    "# torch.Tensor(data.val_dataset['input_values'][0]).unsqueeze(0)\n",
    "# model.generate(torch.Tensor(data.val_dataset['input_values'][0]).unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd01a88-f5e5-467d-b7fe-b019deb2b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.Tensor([6506, 3808,1998, 1996,6506,3808,2058,2058,1012])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2911ae-4a22-4957-b03f-4ba66dfcd017",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a92ef7-b8db-4cbd-90f0-3e991c895322",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [101, 2054, 2842, 2023, 2071, 1005, 2310, 2042,  102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d53c214-9bd2-4bc1-a664-f5efb642ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d42a160-2f59-4890-9040-95d0d430c41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edeeb59-e4fb-4458-9072-4995164f53b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c818fd82-a604-428d-8407-291640c1fea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603d0b8-5ec5-4b92-8d23-ce783df5b183",
   "metadata": {},
   "outputs": [],
   "source": [
    "b  = [6506, 3808, 2003,1998,2042,3808,7316,1999,1012]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4a8aab-5759-49ab-8769-4d6973e21bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_2D_list = [[5, 10, 15, 20],\n",
    "                   [25, 30, 35, 40],\n",
    "                   [45, 50, 55, 60]]\n",
    "list_to_tensor = torch.tensor(example_2D_list)\n",
    "print(\"Our New 2D Tensor from 2D List is: \", list_to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc4b9c-8c94-4c66-b9e4-344d1a6a0c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b95338-4fc7-46bc-b7fc-3f8fa3ef4bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = torch.Tensor([[6506, 3808],\n",
    "        [3808, 3808],\n",
    "        [2003, 1998],\n",
    "        [1998, 1996],\n",
    "        [2042, 6506],\n",
    "        [3808, 3808],\n",
    "        [7316, 2058],\n",
    "        [1999, 2058],\n",
    "        [1012, 1012]]).type(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6578baf2-b5f9-4889-9c1f-96498a615ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = tensors[:,:1].flatten().tolist()\n",
    "list2 = tensors[:,-1].flatten().tolist()\n",
    "list1, list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216744cf-ab2d-42fb-bbaf-e215c03a8602",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345721b-6fb6-4d85-a7bb-97ac07da7e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffe275d-62f2-4ed3-8483-8c0d612068bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.Tensor([[ 101, 2054, 2842, 2023, 2071, 1005, 2310, 2042,  102],\n",
    "        [ 101, 2562, 2115, 2192, 1999, 3067,  102, -100, -100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644480ec-0dd4-493b-b237-92fb3112d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4364896-4d8a-4f68-82a9-53dfff78945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe00d13-4a16-4d87-b099-d9523c4b2a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541180e7-ee15-4c38-9b11-5a19d615e4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3af1ef-24f3-4655-ae66-3d76994c8809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54566220-e690-4eba-a75f-557845db3bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f292ed0a-68b9-4164-9e50-cf3e3a2b6598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9880857-0b09-4be1-be14-5e279a0e9ed1",
   "metadata": {},
   "source": [
    "## TESTING HUGGINGFACE's IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3892980d-3369-4945-82ea-7631c6a86e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfb6c4c2-6498-4e6e-bc35-99954667b3f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (2878463929.py, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[25], line 23\u001b[0;36m\u001b[0m\n\u001b[0;31m    loss = model(inputs=input_values,labels).loss\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoFeatureExtractor, SpeechEncoderDecoderModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "encoder_id = \"facebook/wav2vec2-base-960h\"  # acoustic model encoder\n",
    "decoder_id = \"bert-base-uncased\"  # text decoder\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(encoder_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(decoder_id)\n",
    "# Combine pre-trained encoder and pre-trained decoder to form a Seq2Seq model\n",
    "model = SpeechEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_id, decoder_id)\n",
    "\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# load an audio input and pre-process (normalise mean/std to 0/1)\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "input_values = feature_extractor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\").input_values\n",
    "\n",
    "# load its corresponding transcription and tokenize to generate labels\n",
    "labels = tokenizer(ds[0][\"text\"], return_tensors=\"pt\").input_ids\n",
    "\n",
    "# the forward function automatically creates the correct decoder_input_ids\n",
    "loss = model(inputs=input_values,labels).loss\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7af10ae9-64e0-4273-9c1b-ed914dadce5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 12525, 27565,  2121,  2003,  1996, 20121,  1997,  1996,  2690,\n",
       "          4280,  1998,  2057,  2024,  5580,  2000,  6160,  2010,  8036,   102]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3899ce1-5707-4381-9981-ce9c3646f20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the decoder: <class 'transformers.models.mbart.modeling_mbart.MBartForCausalLM'> is overwritten by shared decoder config: MBartConfig {\n",
      "  \"_name_or_path\": \"/home/suraj/projects/mbart-50/hf_models/mbart-50-large-one-to-many/\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_cross_attention\": true,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"MBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"MBart50Tokenizer\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250054\n",
      "}\n",
      "\n",
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Quilter ist der Apostel der Mittelschicht und wir freuen uns, sein Evangelium willkommen heien zu knnen.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, SpeechEncoderDecoderModel\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# load a fine-tuned speech translation model and corresponding processor\n",
    "model = SpeechEncoderDecoderModel.from_pretrained(\"facebook/wav2vec2-xls-r-300m-en-to-15\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-xls-r-300m-en-to-15\")\n",
    "\n",
    "# let's perform inference on a piece of English speech (which we'll translate to German)\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\").input_values\n",
    "\n",
    "# autoregressively generate transcription (uses greedy decoding by default)\n",
    "generated_ids = model.generate(input_values)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09855416-6f50-4a4b-8667-d311a544a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8949546b-5e7d-479e-818f-0431b00a1e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b88539-fb4f-437a-b7b0-1abc3696f966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from datasets.features import Audio\n",
    "from transformers import SpeechEncoderDecoderModel, Wav2Vec2Processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bf4512-0a15-4407-ad72-cb7071af0f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LANG_ID = \"ru\"\n",
    "MODEL_ID = \"bond005/wav2vec2-mbart50-ru\"\n",
    "SAMPLES = 30\n",
    "\n",
    "num_processes = 4\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "model = SpeechEncoderDecoderModel.from_pretrained(MODEL_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9964f298-8e2b-4302-8772-c83fa20e8f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n",
    "\n",
    "if test_dataset.features['audio'].sampling_rate != 16_000:\n",
    "    test_dataset = test_dataset.cast_column(\n",
    "        'audio',\n",
    "        Audio(sampling_rate=16_000)\n",
    "    )\n",
    "\n",
    "audio_data = [test_dataset[i]['audio']['array'] for i in range(SAMPLES)]\n",
    "\n",
    "processed = processor(audio_data, sampling_rate=16_000,\n",
    "                      return_tensors=\"pt\", padding='longest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067f1f7b-a934-4c63-b188-de0bfbd7d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    predicted_ids = model.generate(**processed)\n",
    "\n",
    "predicted_sentences = processor.batch_decode(\n",
    "    predicted_ids,\n",
    "    num_processes=num_processes,\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for i, predicted_sentence in enumerate(predicted_sentences):\n",
    "        print(\"-\" * 100)\n",
    "        print(\"Reference: \", test_dataset[i][\"sentence\"])\n",
    "        print(\"Prediction:\", predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ef0a005-9e5a-488b-a9a0-6d57153ad408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2babd045-1df9-42e6-aa6e-afde7e55d8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Folder</th>\n",
       "      <th>consolidated_file_path</th>\n",
       "      <th>Length</th>\n",
       "      <th>transcription</th>\n",
       "      <th>transcription_capitalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01cef35811fd4a3fa63a3ab8bba5430c</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.447392</td>\n",
       "      <td>right about now i'm fifty fifty</td>\n",
       "      <td>RIGHT ABOUT NOW I'M FIFTY FIFTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0c04880a6eaf4e559fd9b20594eb74cd</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>4.137755</td>\n",
       "      <td>let me run my fingers through your hair</td>\n",
       "      <td>LET ME RUN MY FINGERS THROUGH YOUR HAIR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01a7c7f2e6db4c68abfbc1218dc9ef76</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.923696</td>\n",
       "      <td>but you're yesterday's child to me</td>\n",
       "      <td>BUT YOU'RE YESTERDAY'S CHILD TO ME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0c8c027ad9374db6b9e6c12dc873be7b</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.058390</td>\n",
       "      <td>and whisper so softly</td>\n",
       "      <td>AND WHISPER SO SOFTLY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>08558a8941c04a559a3ea48793ac1a34</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.038073</td>\n",
       "      <td>have a laugh at the rat race</td>\n",
       "      <td>HAVE A LAUGH AT THE RAT RACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>07929e956a9745fa96e91730d8540cd1</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.702948</td>\n",
       "      <td>aaaa haaa haaa</td>\n",
       "      <td>AAAA HAAA HAAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>002fe3610f1b41b59d28546201635043</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>3.507075</td>\n",
       "      <td>but that's not how it used to be</td>\n",
       "      <td>BUT THAT'S NOT HOW IT USED TO BE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0a4e93435d0746d39ea0814667d6fa3a</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>3.020454</td>\n",
       "      <td>and you've seen it all before but the wolf's o...</td>\n",
       "      <td>AND YOU'VE SEEN IT ALL BEFORE BUT THE WOLF'S O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0bde4da0cf2842028bf7d1a712dec1c5</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.734830</td>\n",
       "      <td>and talk is cheap awhen the story is good</td>\n",
       "      <td>AND TALK IS CHEAP AWHEN THE STORY IS GOOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0851f1ca1594443d897d4a32b4eab788</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.288141</td>\n",
       "      <td>turned you into someone new</td>\n",
       "      <td>TURNED YOU INTO SOMEONE NEW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Folder  \\\n",
       "0  01cef35811fd4a3fa63a3ab8bba5430c   \n",
       "1  0c04880a6eaf4e559fd9b20594eb74cd   \n",
       "2  01a7c7f2e6db4c68abfbc1218dc9ef76   \n",
       "3  0c8c027ad9374db6b9e6c12dc873be7b   \n",
       "4  08558a8941c04a559a3ea48793ac1a34   \n",
       "5  07929e956a9745fa96e91730d8540cd1   \n",
       "6  002fe3610f1b41b59d28546201635043   \n",
       "7  0a4e93435d0746d39ea0814667d6fa3a   \n",
       "8  0bde4da0cf2842028bf7d1a712dec1c5   \n",
       "9  0851f1ca1594443d897d4a32b4eab788   \n",
       "\n",
       "                              consolidated_file_path    Length  \\\n",
       "0  /scratch/users/gmenon/wav_clips/separated/htde...  2.447392   \n",
       "1  /scratch/users/gmenon/wav_clips/separated/htde...  4.137755   \n",
       "2  /scratch/users/gmenon/wav_clips/separated/htde...  2.923696   \n",
       "3  /scratch/users/gmenon/wav_clips/separated/htde...  2.058390   \n",
       "4  /scratch/users/gmenon/wav_clips/separated/htde...  2.038073   \n",
       "5  /scratch/users/gmenon/wav_clips/separated/htde...  2.702948   \n",
       "6  /scratch/users/gmenon/wav_clips/separated/htde...  3.507075   \n",
       "7  /scratch/users/gmenon/wav_clips/separated/htde...  3.020454   \n",
       "8  /scratch/users/gmenon/wav_clips/separated/htde...  2.734830   \n",
       "9  /scratch/users/gmenon/wav_clips/separated/htde...  2.288141   \n",
       "\n",
       "                                       transcription  \\\n",
       "0                    right about now i'm fifty fifty   \n",
       "1            let me run my fingers through your hair   \n",
       "2                 but you're yesterday's child to me   \n",
       "3                              and whisper so softly   \n",
       "4                       have a laugh at the rat race   \n",
       "5                                     aaaa haaa haaa   \n",
       "6                   but that's not how it used to be   \n",
       "7  and you've seen it all before but the wolf's o...   \n",
       "8          and talk is cheap awhen the story is good   \n",
       "9                        turned you into someone new   \n",
       "\n",
       "                           transcription_capitalized  \n",
       "0                    RIGHT ABOUT NOW I'M FIFTY FIFTY  \n",
       "1            LET ME RUN MY FINGERS THROUGH YOUR HAIR  \n",
       "2                 BUT YOU'RE YESTERDAY'S CHILD TO ME  \n",
       "3                              AND WHISPER SO SOFTLY  \n",
       "4                       HAVE A LAUGH AT THE RAT RACE  \n",
       "5                                     AAAA HAAA HAAA  \n",
       "6                   BUT THAT'S NOT HOW IT USED TO BE  \n",
       "7  AND YOU'VE SEEN IT ALL BEFORE BUT THE WOLF'S O...  \n",
       "8          AND TALK IS CHEAP AWHEN THE STORY IS GOOD  \n",
       "9                        TURNED YOU INTO SOMEONE NEW  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(WAV2VEC2_ARGS.TRAIN_FILE_PATH).head(10)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b64793a-efaa-407c-94ea-80ece2282045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Folder</th>\n",
       "      <th>consolidated_file_path</th>\n",
       "      <th>Length</th>\n",
       "      <th>transcription</th>\n",
       "      <th>transcription_capitalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0aa835068aa64515b2158c2f0b6513b7</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>4.754943</td>\n",
       "      <td>so learn from your mistakes</td>\n",
       "      <td>SO LEARN FROM YOUR MISTAKES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>089cd84d662e4c50954cfa46c07cae14</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.759388</td>\n",
       "      <td>i've been connected to the right line</td>\n",
       "      <td>I'VE BEEN CONNECTED TO THE RIGHT LINE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>127cc2aaff9a44fab21b2a0c4f5c3e5d</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.668254</td>\n",
       "      <td>the truth to be found</td>\n",
       "      <td>THE TRUTH TO BE FOUND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18e03c3980014046a3504ef42fe81ae8</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.895079</td>\n",
       "      <td>he said the way myblue eyes shined</td>\n",
       "      <td>HE SAID THE WAY MYBLUE EYES SHINED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07c8b8c0b0424c1e87389392695a551b</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.666939</td>\n",
       "      <td>you leave me once again home alone</td>\n",
       "      <td>YOU LEAVE ME ONCE AGAIN HOME ALONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0cb795c9b6e64a4abab5553c2b2ed3e0</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.742880</td>\n",
       "      <td>while they are in commend</td>\n",
       "      <td>WHILE THEY ARE IN COMMEND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0655395364614218be31548245853ca0</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>4.697710</td>\n",
       "      <td>a life all mine</td>\n",
       "      <td>A LIFE ALL MINE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>113bb605eb324aeb8db046b4193424a2</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.642948</td>\n",
       "      <td>so i never went back</td>\n",
       "      <td>SO I NEVER WENT BACK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00ecd4b71c094b959ae492ec138bb555</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>3.176508</td>\n",
       "      <td>in you i taste god</td>\n",
       "      <td>IN YOU I TASTE GOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0b5fa72c054e44598ec8aefb76ff855e</td>\n",
       "      <td>/scratch/users/gmenon/wav_clips/separated/htde...</td>\n",
       "      <td>2.465011</td>\n",
       "      <td>could stay a while</td>\n",
       "      <td>COULD STAY A WHILE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Folder  \\\n",
       "0  0aa835068aa64515b2158c2f0b6513b7   \n",
       "1  089cd84d662e4c50954cfa46c07cae14   \n",
       "2  127cc2aaff9a44fab21b2a0c4f5c3e5d   \n",
       "3  18e03c3980014046a3504ef42fe81ae8   \n",
       "4  07c8b8c0b0424c1e87389392695a551b   \n",
       "5  0cb795c9b6e64a4abab5553c2b2ed3e0   \n",
       "6  0655395364614218be31548245853ca0   \n",
       "7  113bb605eb324aeb8db046b4193424a2   \n",
       "8  00ecd4b71c094b959ae492ec138bb555   \n",
       "9  0b5fa72c054e44598ec8aefb76ff855e   \n",
       "\n",
       "                              consolidated_file_path    Length  \\\n",
       "0  /scratch/users/gmenon/wav_clips/separated/htde...  4.754943   \n",
       "1  /scratch/users/gmenon/wav_clips/separated/htde...  2.759388   \n",
       "2  /scratch/users/gmenon/wav_clips/separated/htde...  2.668254   \n",
       "3  /scratch/users/gmenon/wav_clips/separated/htde...  2.895079   \n",
       "4  /scratch/users/gmenon/wav_clips/separated/htde...  2.666939   \n",
       "5  /scratch/users/gmenon/wav_clips/separated/htde...  2.742880   \n",
       "6  /scratch/users/gmenon/wav_clips/separated/htde...  4.697710   \n",
       "7  /scratch/users/gmenon/wav_clips/separated/htde...  2.642948   \n",
       "8  /scratch/users/gmenon/wav_clips/separated/htde...  3.176508   \n",
       "9  /scratch/users/gmenon/wav_clips/separated/htde...  2.465011   \n",
       "\n",
       "                           transcription  \\\n",
       "0            so learn from your mistakes   \n",
       "1  i've been connected to the right line   \n",
       "2                  the truth to be found   \n",
       "3     he said the way myblue eyes shined   \n",
       "4     you leave me once again home alone   \n",
       "5              while they are in commend   \n",
       "6                        a life all mine   \n",
       "7                   so i never went back   \n",
       "8                     in you i taste god   \n",
       "9                     could stay a while   \n",
       "\n",
       "               transcription_capitalized  \n",
       "0            SO LEARN FROM YOUR MISTAKES  \n",
       "1  I'VE BEEN CONNECTED TO THE RIGHT LINE  \n",
       "2                  THE TRUTH TO BE FOUND  \n",
       "3     HE SAID THE WAY MYBLUE EYES SHINED  \n",
       "4     YOU LEAVE ME ONCE AGAIN HOME ALONE  \n",
       "5              WHILE THEY ARE IN COMMEND  \n",
       "6                        A LIFE ALL MINE  \n",
       "7                   SO I NEVER WENT BACK  \n",
       "8                     IN YOU I TASTE GOD  \n",
       "9                     COULD STAY A WHILE  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df = pd.read_csv(WAV2VEC2_ARGS.TEST_FILE_PATH).head(10)\n",
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5a069d0-7df7-454a-b738-8756510565c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor, BertTokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('facebook/wav2vec2-large-960h-lv60-self')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f95a8161-b007-4da7-bb81-3a4c103eab4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([101,\n",
       "   156,\n",
       "   2346,\n",
       "   149,\n",
       "   12420,\n",
       "   2069,\n",
       "   2249,\n",
       "   143,\n",
       "   21564,\n",
       "   2107,\n",
       "   19141,\n",
       "   2069,\n",
       "   26574,\n",
       "   9272,\n",
       "   1592,\n",
       "   22441,\n",
       "   1708,\n",
       "   102],\n",
       "  [101, 1177, 3858, 1121, 1240, 12572, 102]),\n",
       " ([101,\n",
       "   146,\n",
       "   112,\n",
       "   159,\n",
       "   2036,\n",
       "   139,\n",
       "   27073,\n",
       "   2249,\n",
       "   18732,\n",
       "   2249,\n",
       "   22680,\n",
       "   16647,\n",
       "   10069,\n",
       "   16972,\n",
       "   7462,\n",
       "   155,\n",
       "   23413,\n",
       "   18784,\n",
       "   149,\n",
       "   11607,\n",
       "   2036,\n",
       "   102],\n",
       "  [101, 178, 112, 1396, 1151, 3387, 1106, 1103, 1268, 1413, 102]),\n",
       " ([101,\n",
       "   7462,\n",
       "   157,\n",
       "   2069,\n",
       "   16830,\n",
       "   3048,\n",
       "   16972,\n",
       "   139,\n",
       "   2036,\n",
       "   143,\n",
       "   2346,\n",
       "   27370,\n",
       "   2137,\n",
       "   102],\n",
       "  [101, 1103, 3062, 1106, 1129, 1276, 102]),\n",
       " ([101,\n",
       "   145,\n",
       "   2036,\n",
       "   13411,\n",
       "   9949,\n",
       "   7462,\n",
       "   22751,\n",
       "   3663,\n",
       "   150,\n",
       "   3663,\n",
       "   13360,\n",
       "   24846,\n",
       "   142,\n",
       "   3663,\n",
       "   9919,\n",
       "   17730,\n",
       "   11607,\n",
       "   10069,\n",
       "   102],\n",
       "  [101, 1119, 1163, 1103, 1236, 1139, 1830, 19224, 1257, 18978, 1181, 102]),\n",
       " ([101,\n",
       "   19141,\n",
       "   149,\n",
       "   12420,\n",
       "   17145,\n",
       "   22157,\n",
       "   21748,\n",
       "   10954,\n",
       "   14731,\n",
       "   1592,\n",
       "   11607,\n",
       "   145,\n",
       "   13041,\n",
       "   2036,\n",
       "   18589,\n",
       "   11414,\n",
       "   2036,\n",
       "   102],\n",
       "  [101, 1128, 1817, 1143, 1517, 1254, 1313, 2041, 102]),\n",
       " ([101,\n",
       "   160,\n",
       "   3048,\n",
       "   17656,\n",
       "   2036,\n",
       "   7462,\n",
       "   3663,\n",
       "   22133,\n",
       "   2036,\n",
       "   15969,\n",
       "   18732,\n",
       "   25290,\n",
       "   11680,\n",
       "   2137,\n",
       "   102],\n",
       "  [101, 1229, 1152, 1132, 1107, 3254, 2354, 1181, 102]),\n",
       " ([101, 138, 149, 15499, 2036, 18589, 2162, 26574, 22680, 102],\n",
       "  [101, 170, 1297, 1155, 2317, 102]),\n",
       " ([101,\n",
       "   156,\n",
       "   2346,\n",
       "   146,\n",
       "   26546,\n",
       "   17145,\n",
       "   2069,\n",
       "   160,\n",
       "   11680,\n",
       "   1942,\n",
       "   12465,\n",
       "   1658,\n",
       "   2428,\n",
       "   102],\n",
       "  [101, 1177, 178, 1309, 1355, 1171, 102]),\n",
       " ([101, 15969, 19141, 146, 157, 10719, 12880, 27157, 2137, 102],\n",
       "  [101, 1107, 1128, 178, 5080, 5540, 102]),\n",
       " ([101,\n",
       "   18732,\n",
       "   2591,\n",
       "   20521,\n",
       "   23676,\n",
       "   1592,\n",
       "   3663,\n",
       "   138,\n",
       "   160,\n",
       "   3048,\n",
       "   17656,\n",
       "   2036,\n",
       "   102],\n",
       "  [101, 1180, 2215, 170, 1229, 102])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a for a in zip(bert_tokenizer(list(validation_df[\"transcription_capitalized\"])).input_ids,bert_tokenizer(list(validation_df[\"transcription\"])).input_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1a216e7-276c-45cc-91f7-401946441a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he said the way myblue eyes shined'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.decode(bert_tokenizer(list(validation_df[\"transcription\"])).input_ids[3],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec7d0394-1291-433f-935c-fbe850890b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'so learn from your mistakes'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(validation_df[\"transcription\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "559d4025-cde5-48d9-ac08-294a5ec63ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "label = torch.Tensor([101, 155, 23413, 18784, 16151, 2346, 16830, 24819, 2924, 146, 112, 150, 143, 15499, 16880, 143, 15499, 16880, 102])\n",
    "\n",
    "label = torch.Tensor([[ 101, 1037, 2166, 2035, 3067,  102, -100],[ 101, 2061, 1045, 2196, 2253, 2067,  102]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "317b076f-5be1-4e66-b382-5b23bf56cc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor, BertTokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('facebook/wav2vec2-base-960h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dee7e39-0f87-4918-b1d1-b37c65a653b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] so i never went back [SEP]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.decode(label[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b14e73de-3744-45e5-a44b-f10890399af7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wav_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwav_tokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mdecode(label)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wav_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "wav_tokenizer.decode(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b0e4c8-2c5d-4c8c-bad1-54ff6462c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.Tensor([[ 101, 1037, 2166, 2035, 3067,  102, -100],\n",
    "        [ 101, 2061, 1045, 2196, 2253, 2067,  102]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218963a8-63c0-45d4-9679-139ced75b5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(label[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689de184-6d7c-4a51-846a-c3cddb5e338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(label[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035c37ac-55de-43be-9fd1-0d53040b916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = torch.Tensor([[1996, 1996],\n",
    "        [1999, 1996],\n",
    "        [2061, 1996],\n",
    "        [2061, 1000],\n",
    "        [1000, 1998],\n",
    "        [1000, 2030],\n",
    "        [2061, 2061]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211520c9-f5d0-4712-a337-e279d30858e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(predicted.T[0,:],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c065f3e5-eaa6-4aec-9423-ce4a78b8b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "validation_df = pd.read_csv(WAV2VEC2_ARGS.TEST_FILE_PATH).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e9fc80e-dc3b-4a44-8256-06615ad031a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THE TRUTH TO BE FOUND']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(validation_df[validation_df[\"Folder\"] == '127cc2aaff9a44fab21b2a0c4f5c3e5d'][\"transcription_capitalized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f98d289e-a041-4d28-bc86-74c3e7ed3627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[101, 1996, 3606, 2000, 2022, 2179, 102]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer(list(validation_df[validation_df[\"Folder\"] == '127cc2aaff9a44fab21b2a0c4f5c3e5d'][\"transcription_capitalized\"])).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d44d271a-c5d7-4e6f-876d-f11e0ff2fa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    #batch[\"attention_mask\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).attention_mask[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "    batch[\"labels\"] = bert_tokenizer(batch[\"transcription\"]).input_ids\n",
    "    batch[\"label_attention_mask\"] = bert_tokenizer(batch[\"transcription\"]).attention_mask\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07bcd7e1-c776-4cf8-be39-50b0cfb96688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Audio\n",
    "val_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(validation_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(validation_df[\"transcription_capitalized\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c6a8104b-8f8b-44eb-ab90-7842b9af0954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6514c59c27fa4320b0d4ac9e1c2322f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_dataset1 = val_dataset.map(prepare_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c485df2f-0cae-4b93-a2ec-32387d553eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_dataset1[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ae5938ce-fe60-404b-abc9-cee7a9dbfd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_features = [{\"input_ids\": feature[\"labels\"]} for feature in val_dataset1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8cee0e86-af23-4f98-969f-be4e9084af5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 101, 1996, 3606, 2000, 2022, 2179,  102]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.pad(\n",
    "            label_features,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3eed0fe2-d3a6-46ff-8de0-0c7515c29d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d24f57-5f1b-44b8-88d9-5e61fd829fec",
   "metadata": {},
   "source": [
    "# HF Implementation Speech Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8788e8ca-b9fb-4314-8af5-9d4aacf9a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, Wav2Vec2Config, SpeechEncoderDecoderConfig, SpeechEncoderDecoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9abbde8-e918-4011-bbc6-c08e712a3670",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_encoder = Wav2Vec2Config()\n",
    "config_decoder = BertConfig()\n",
    "\n",
    "config = SpeechEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n",
    "\n",
    "# Initializing a Wav2Vec2Bert model from a Wav2Vec2 & bert-base-uncased style configurations\n",
    "model = SpeechEncoderDecoderModel(config=config)\n",
    "\n",
    "# Accessing the model configuration\n",
    "config_encoder = model.config.encoder\n",
    "config_decoder  = model.config.decoder\n",
    "# set decoder config to causal lm\n",
    "config_decoder.is_decoder = True\n",
    "config_decoder.add_cross_attention = True\n",
    "\n",
    "# Saving the model, including its configuration\n",
    "model.save_pretrained('my-model')\n",
    "\n",
    "# loading model and config from pretrained folder\n",
    "encoder_decoder_config = SpeechEncoderDecoderConfig.from_pretrained('my-model')\n",
    "model = SpeechEncoderDecoderModel.from_pretrained('my-model', config=encoder_decoder_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f0f8980-4ac8-403b-b464-3d7245aeb7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the decoder: <class 'transformers.models.speech_to_text_2.modeling_speech_to_text_2.Speech2Text2ForCausalLM'> is overwritten by shared decoder config: Speech2Text2Config {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"Speech2TextForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"conv_channels\": 1024,\n",
      "  \"conv_kernel_sizes\": [\n",
      "    5,\n",
      "    5\n",
      "  ],\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_attention_heads\": 4,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 7,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 4,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"input_channels\": 1,\n",
      "  \"input_feat_per_channel\": 80,\n",
      "  \"is_decoder\": true,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 200,\n",
      "  \"max_source_positions\": 6000,\n",
      "  \"max_target_positions\": 1024,\n",
      "  \"model_type\": \"speech_to_text_2\",\n",
      "  \"num_beams\": 5,\n",
      "  \"num_conv_layers\": 2,\n",
      "  \"num_hidden_layers\": 7,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 10224\n",
      "}\n",
      "\n",
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "source": [
    "from transformers import SpeechEncoderDecoderModel, Speech2Text2Processor\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "processor = Speech2Text2Processor.from_pretrained('facebook/s2t-wav2vec2-large-en-de')\n",
    "model = SpeechEncoderDecoderModel.from_pretrained('facebook/s2t-wav2vec2-large-en-de')\n",
    "\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24e7f84e-bdc9-4ad1-986e-44a50b922c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "source": [
    "input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\").input_values\n",
    "decoder_input_ids = torch.tensor([[model.config.decoder.decoder_start_token_id]])\n",
    "outputs = model(input_values=input_values, decoder_input_ids=decoder_input_ids)\n",
    "\n",
    "# inference (generation)\n",
    "generated = model.generate(input_values)\n",
    "translation = processor.batch_decode(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "630eb3c9-f407-4cd8-9ad5-83c8f46bbaff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d2d61bea-f700-4cd8-9027-ee6349b84f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0386, 0.0337, 0.0322,  ..., 0.0070, 0.0095, 0.0169]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f518a7a-b12a-4404-982b-e28c4b5c7b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-6.9091, -6.9087, -6.1153,  ..., -6.9070, -6.9084, -6.9099]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac677b27-4b55-4bd1-896e-8f1826e3a618",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = processor.batch_decode(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a523cff2-8ce3-4652-b2b7-483f2d7b3a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2, 1845,  678,  287,  129,   11,    6,  114, 5364,  262,    6, 1267,\n",
       "          106, 2449,    7,  242,   40, 6879,    5,   84, 2623,  348,  143,   54,\n",
       "           15, 1382,  888,  893,    4,    2]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(input_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0feb4d90-9031-4389-b8e1-c7415017f7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s> Herr Quilter ist der Apostel der Mittelschicht und wir sind froh, sein Evangelium zu begren. </s>']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f2cbd32e-49e5-4b52-a07a-2a1a5f049bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4c8c0e3e-df28-4675-8504-38fd1442de45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': '/home/users/gmenon/.cache/huggingface/datasets/downloads/extracted/8762f996c85fa5b5fea87e08f11ad15b13e4d9420634822abf8cd40fd503393b/dev_clean/1272/128104/1272-128104-0000.flac',\n",
       " 'audio': {'path': '/home/users/gmenon/.cache/huggingface/datasets/downloads/extracted/8762f996c85fa5b5fea87e08f11ad15b13e4d9420634822abf8cd40fd503393b/dev_clean/1272/128104/1272-128104-0000.flac',\n",
       "  'array': array([0.00238037, 0.0020752 , 0.00198364, ..., 0.00042725, 0.00057983,\n",
       "         0.0010376 ]),\n",
       "  'sampling_rate': 16000},\n",
       " 'text': 'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL',\n",
       " 'speaker_id': 1272,\n",
       " 'chapter_id': 128104,\n",
       " 'id': '1272-128104-0000'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87da651f-4580-4400-a76c-de566b176b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090fe114-ed83-4eea-b78b-5c9ee0dc3cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a1c3b3-75e2-41d2-b131-57aeeb5f8d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af145b9-4490-46bb-b396-f6738e627f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd671cd-b7b1-4a55-9b0b-fb23118b58fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1525c110-9877-4702-8e9b-eaf106717100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2056091-e49e-44f4-928f-081239396fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a965d4d2-c262-447f-9402-3339a2396df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ae1f6e7-a6e0-4292-880f-7b7ff49a0aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbd0a975-2e5a-4ecb-900c-90e3a45b50e5",
   "metadata": {},
   "source": [
    "# BACKUP OF WORKING SCRIPT !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e2603f-4b14-4914-9c05-65bf05bfecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torchmetrics.text import WordErrorRate\n",
    "from typing import Optional\n",
    "from pytorch_lightning import LightningModule, LightningDataModule, Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from constants.mir_constants import TrainingArgs, WAV2VEC2_ARGS\n",
    "from dataclasses import dataclass, asdict, field # type: ignore\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor, BertTokenizer\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM,AutoModelForCTC, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset, Dataset, Audio\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from flash.audio import SpeechRecognition, SpeechRecognitionData\n",
    "from training.wav2vec2_finetune import Wav2Vec2SpeechRecognition, SpeechRecognitionData\n",
    "\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"SLG - wav2vec2 transfer learning\",log_model=True,)\n",
    "\n",
    "print(json.dumps(asdict(WAV2VEC2_ARGS), indent = 4))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "    \"\"\"\n",
    "# {\n",
    "#     'input_values': tensor([[ 0.1252,  0.1147, -0.0070,  ...,  0.0000,  0.0000,  0.0000], [ 0.0015, -0.0029, -0.0065,  ..., -0.0397, -0.0444,  0.0110]]), \n",
    "#     'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],[1, 1, 1,  ..., 1, 1, 1]]), \n",
    "#     'labels': tensor([[  101,  9294,  1185,  1119,   112,  1325,  1294,  2059,   102], [  101,  1105,   178,  1169,  1437,  1128,  1103, 11711,   102]]), \n",
    "#     'label_attention_masks': tensor([[0, 1, 1, 1, 1, 1, 1, 1, 0], [0, 1, 1, 1, 1, 1, 1, 1, 0]])\n",
    "#  }\n",
    "\n",
    "#{\n",
    "#        'input_values': tensor([[-0.1088,  0.7008,  0.0430,  ...,  0.0174, -0.6597, -1.5689],\n",
    "#                               [ 0.0170,  0.0307,  0.0260,  ...,  0.0000,  0.0000,  0.0000]]),\n",
    "#        'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1], [1, 1, 1,  ..., 0, 0, 0]]),\n",
    "#        'labels': tensor([[  101, 15969, 19141,   146,   157, 10719, 12880, 27157,  2137,   102,  -100,  -100,  -100],\n",
    "#                          [  101, 18732,  2591, 20521, 23676,  1592,  3663,   138,   160,  3048, 17656,  2036,   102]]), \n",
    "#        'label_attention_masks': tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])\n",
    "# }\n",
    "\n",
    "\n",
    "    feature_extractor: Wav2Vec2FeatureExtractor\n",
    "    bert_tokenizer: BertTokenizer\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        label_attention_features =[{\"input_ids\": feature[\"label_attention_mask\"]} for feature in features]\n",
    "        attention_features =[{\"input_values\": feature[\"attention_mask\"]} for feature in features]\n",
    "        \n",
    "        batch = self.feature_extractor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\")\n",
    "\n",
    "        batch_attention = self.feature_extractor.pad(\n",
    "            attention_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\")\n",
    "    \n",
    "        labels_batch = self.bert_tokenizer.pad(\n",
    "            label_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        label_attention_batch = self.bert_tokenizer.pad(\n",
    "            label_attention_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "            )\n",
    "                \n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels_attention = label_attention_batch[\"input_ids\"].masked_fill(labels_batch.input_ids.eq(101), 0).masked_fill(labels_batch.input_ids.eq(102), 0)\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), 0) #Changed from -100 as this is not Wav2Vec2 but it is BERT. The latter's pad token is 0.\n",
    "        batch[\"labels\"] = labels\n",
    "        batch[\"label_attention_masks\"] = labels_attention\n",
    "        batch[\"attention_mask\"] = batch_attention[\"input_values\"]\n",
    "        #print(batch)\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "class SpeechRecognitionDataModule(LightningDataModule):\n",
    "    def __init__(self, WAV2VEC2_ARGS: WAV2VEC2_ARGS, num_workers,hparams):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.batch_size = WAV2VEC2_ARGS.BATCH_SIZE\n",
    "        self.num_workers = num_workers\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(hparams.wav2vec2_model)\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(hparams.lm_model)\n",
    "        #self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(WAV2VEC2_ARGS.MODEL_BACKBONE)\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16_000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "        #self.feature_extractor = AutoFeatureExtractor.from_pretrained(hparams.wav2vec2_model)\n",
    "        #self.processor = Wav2Vec2Processor(feature_extractor=self.feature_extractor, tokenizer=self.bert_tokenizer)\n",
    "        #self.processor = AutoProcessor(feature_extractor=self.feature_extractor, tokenizer=self.bert_tokenizer)\n",
    "        #self.processor = WhisperProcessor(feature_extractor=self.feature_extractor, tokenizer=self.bert_tokenizer)\n",
    "        self.data_collator = DataCollatorCTCWithPadding(feature_extractor=self.feature_extractor, bert_tokenizer=self.bert_tokenizer, padding=True,)\n",
    "        self.cpu_count = 4\n",
    "        print(f\"CPU Count = {self.cpu_count}\")\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        train_df = pd.read_csv(WAV2VEC2_ARGS.TRAIN_FILE_PATH).head(1000)\n",
    "        validation_df = pd.read_csv(WAV2VEC2_ARGS.TEST_FILE_PATH).head(10)\n",
    "        if stage == 'fit' or stage is None:\n",
    "            print(\"In Stage = Fit\")\n",
    "            train_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(train_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(train_df[\"transcription\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "            self.train_dataset = train_dataset.map(self.prepare_dataset,remove_columns = train_dataset.column_names)\n",
    "            \n",
    "            val_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(validation_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(validation_df[\"transcription\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "            self.val_dataset = val_dataset.map(self.prepare_dataset,remove_columns = val_dataset.column_names)\n",
    "\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            print(\"In Stage = Test\")\n",
    "            test_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(validation_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(validation_df[\"transcription\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "            test_dataset = val_dataset.map(self.prepare_dataset,remove_columns = val_dataset.column_names)\n",
    "            self.test_dataset = test_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        print(\"entering train data loader\")\n",
    "        return DataLoader(\n",
    "            self.train_dataset.with_format(\"torch\"), \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn = self.data_collator\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        print(\"entering val data loader\")\n",
    "        return DataLoader(\n",
    "            self.val_dataset.with_format(\"torch\"), \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn = self.data_collator\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        print(\"entering test data loader\")\n",
    "        return DataLoader(\n",
    "            self.test_dataset.with_format(\"torch\"), \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn = self.data_collator\n",
    "        )\n",
    "\n",
    "    def prepare_dataset(self,batch):\n",
    "        audio = batch[\"audio\"]\n",
    "        # batched output is \"un-batched\" to ensure mapping is correct\n",
    "        batch[\"input_values\"] = self.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "        batch[\"attention_mask\"] = self.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).attention_mask[0]\n",
    "        batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "        batch[\"labels\"] = self.bert_tokenizer(batch[\"transcription\"]).input_ids\n",
    "        batch[\"label_attention_mask\"] = self.bert_tokenizer(batch[\"transcription\"]).attention_mask\n",
    "        #print(batch)\n",
    "        # with self.processor.as_target_processor():\n",
    "        #     batch[\"labels\"] = self.processor(batch[\"transcription\"]).input_ids\n",
    "        #     batch[\"label_attention_mask\"] = self.processor(batch[\"transcription\"]).attention_mask\n",
    "        return batch\n",
    "\n",
    "class Wav2SeqModel(LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.batch_size = hparams.batch_size\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(hparams.lm_model)\n",
    "        speech_recognition_task = Wav2Vec2SpeechRecognition(wav2vec2_args=WAV2VEC2_ARGS)\n",
    "        #self.wav2vec2 = AutoModelForCTC.from_pretrained(hparams.wav2vec2_model,ctc_zero_infinity=False,ctc_loss_reduction=\"sum\")\n",
    "        self.wav2vec2 = SpeechRecognition.load_from_checkpoint(\"/scratch/users/gmenon/model_artefacts/wav2vec2_demucs_en_large-960h-lv60-self_freeze_unfreeze_15epochs_adam.pt\").model\n",
    "        #self.wav2vec2.freeze_feature_encoder()\n",
    "        #self.wav2vec2 = AutoModel.from_pretrained(hparams.wav2vec2_model)\n",
    "        self.wav2vec2.eval()\n",
    "        #print(self.wav2vec2.config)\n",
    "        self.seq2seq = AutoModelForCausalLM.from_pretrained(hparams.lm_model)\n",
    "        self.seq2seq.config.is_decoder = True\n",
    "        self.seq2seq.add_cross_attention = True\n",
    "        self.seq2seq.train()\n",
    "        bert_unwanted = np.zeros((1996,), dtype=int)\n",
    "        bert_wanted = np.ones((len(self.bert_tokenizer.get_vocab())-1996,), dtype=int)\n",
    "        self.weights = torch.Tensor(np.concatenate((bert_unwanted, bert_wanted), axis=0)).cuda()\n",
    "        #print(self.seq2seq.config)\n",
    "\n",
    "    def forward(self, audio, attention_mask, labels, label_attention_mask):\n",
    "        #print(\"entering forward step\")\n",
    "        encoder_outputs = self.wav2vec2(audio.squeeze(),\n",
    "                                        attention_mask=attention_mask,\n",
    "                                        output_hidden_states=True,\n",
    "                                        output_attentions=True)\n",
    "        encoder_hidden_states = encoder_outputs[0]  \n",
    "        #encoder_attention_mask = self.wav2vec2._get_feature_vector_attention_mask(\n",
    "        #        encoder_hidden_states.shape[1], \n",
    "        #        attention_mask\n",
    "        #        )\n",
    "        #print(f\"[PRIOR]decoder_input_ids={labels}\")\n",
    "        #print(f\"encoder_hidden_states = {encoder_hidden_states}\")\n",
    "        #print(f\"[PRIOR]decoder_attention_masks={label_attention_mask}\")\n",
    "        #decoder_input_ids = self.shift_tokens_right(labels, 0, 101)\n",
    "        #decoder_attention_masks = self.shift_tokens_right_mask(label_attention_mask)\n",
    "        #print(f\"Shape of labels ={labels.shape}, label_attention_mask = {label_attention_mask.shape}, decoder_input_ids ={decoder_input_ids.shape}, decoder_attention_mask = {decoder_attention_masks.shape}\")\n",
    "        #print(f\"[AFTER]decoder_input_ids={decoder_input_ids}\")\n",
    "        #print(f\"[AFTER]decoder_attention_masks={decoder_attention_masks}\")\n",
    "        decoder_outputs = self.seq2seq(input_ids=labels,\n",
    "                                       attention_mask = label_attention_mask.squeeze(),\n",
    "                                       encoder_hidden_states=encoder_hidden_states)\n",
    "                                       #,encoder_attention_mask=encoder_attention_mask) \n",
    "        # decoder_outputs = self.seq2seq(input_ids=labels, attention_mask = label_attention_mask, encoder_attention_mask=encoder_attention_mask)\n",
    "        return decoder_outputs\n",
    "\n",
    "    def generate(self, audio):\n",
    "        encoder_outputs = self.wav2vec2(audio[0],output_hidden_states=True,output_attentions=True)\n",
    "        encoder_hidden_states = encoder_outputs[0]\n",
    "        bos_ids = (\n",
    "            torch.ones(\n",
    "                (encoder_hidden_states.size()[0], 1),\n",
    "                dtype=torch.long,\n",
    "                device=self.seq2seq.device,\n",
    "            )\n",
    "            * self.seq2seq.config.pad_token_id\n",
    "        )\n",
    "        return self.seq2seq.generate(\n",
    "            input_ids=bos_ids,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        #print(\"entering training step\")\n",
    "        audio = batch[\"input_values\"].unsqueeze(0)\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        label_attention_mask = batch[\"label_attention_masks\"].unsqueeze(0)\n",
    "        #print(f\"audio={audio}, attention_mask = {attention_mask}, label_attention_mask = {label_attention_mask}\")\n",
    "        labels = batch[\"labels\"]\n",
    "        labels = labels.reshape(self.batch_size,-1)\n",
    "        #labels = labels.reshape(-1,self.batch_size) \n",
    "        logits = self(audio,attention_mask,labels,label_attention_mask).logits\n",
    "        logits = logits.reshape(-1, self.batch_size, self.seq2seq.config.vocab_size)\n",
    "        input_lengths = torch.full(size=(self.batch_size,), fill_value=logits.shape[0], dtype=torch.long)\n",
    "        target_lengths = torch.full(size=(self.batch_size,), fill_value=labels.shape[0], dtype=torch.long)\n",
    "        loss = None\n",
    "        ctc_loss =  nn.CTCLoss(blank=0)\n",
    "        loss = ctc_loss(logits,labels,input_lengths,target_lengths)\n",
    "        #print(\"Shape of Logits and Labels\")\n",
    "        #print(logits.shape,labels.shape)\n",
    "        #ce_loss = nn.CrossEntropyLoss(weight=self.weights)\n",
    "        #loss = ce_loss(logits.permute(0,2,1), labels.permute(1,0))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch,batch_idx):\n",
    "        #print(\"entering validation step\")\n",
    "        audio = batch[\"input_values\"].unsqueeze(0)\n",
    "        labels = batch[\"labels\"]\n",
    "        \n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        label_attention_mask = batch[\"label_attention_masks\"].unsqueeze(0)\n",
    "        #print(f\"audio={audio}, attention_mask = {attention_mask}, label_attention_mask = {label_attention_mask}\")\n",
    "        #labels = labels.reshape(-1,self.batch_size) \n",
    "        labels = labels.reshape(self.batch_size,-1)\n",
    "        logits = self(audio,attention_mask,labels,label_attention_mask).logits\n",
    "        logits = logits.reshape(-1, self.batch_size, self.seq2seq.config.vocab_size)\n",
    "        input_lengths = torch.full(size=(self.batch_size,), fill_value=logits.shape[0], dtype=torch.long)\n",
    "        target_lengths = torch.full(size=(self.batch_size,), fill_value=labels.shape[0], dtype=torch.long)\n",
    "        loss=None\n",
    "        ctc_loss =  nn.CTCLoss(blank=0)\n",
    "        loss = ctc_loss(logits,labels,input_lengths,target_lengths)\n",
    "        #print(\"Shape of Logits and Labels\")\n",
    "        #print(logits.shape,labels.shape)\n",
    "        #ce_loss = nn.CrossEntropyLoss(weight=self.weights)\n",
    "        #loss = ce_loss(logits.permute(0,2,1), labels.permute(1,0))\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        #label_decoded = labels.type(torch.int32).tolist()\n",
    "        #print(label_decoded)\n",
    "        #print(f\"labels = {labels}\")\n",
    "        #print(f\"Shape/predicted ids = {predicted_ids.shape},{predicted_ids}\")\n",
    "        #print(f\"original transcript = {batch['transcription']}\")\n",
    "        print(f\"original text = {self.bert_tokenizer.decode(labels[0],skip_special_tokens=False)},{self.bert_tokenizer.decode(labels[1],skip_special_tokens=False)}\")\n",
    "        #predicted_text = predicted_ids.type(torch.int32)\n",
    "        print(f\"Predicted text = {self.bert_tokenizer.decode(predicted_ids.T[0,:],skip_special_tokens=False)},{self.bert_tokenizer.decode(predicted_ids.T[1,:],skip_special_tokens=False)}\")\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        print(\"Entering Optimization Step\")\n",
    "        return torch.optim.AdamW(self.parameters(), lr=hparams.learning_rate)\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
    "        \"\"\"\n",
    "        Shift input ids one token to the right.\n",
    "        \"\"\"\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "        if decoder_start_token_id is None:\n",
    "            raise ValueError(\"Make sure to set the decoder_start_token_id attribute of the model's configuration.\")\n",
    "        shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "    \n",
    "        if pad_token_id is None:\n",
    "            raise ValueError(\"Make sure to set the pad_token_id attribute of the model's configuration.\")\n",
    "        # replace possible -100 values in labels by `pad_token_id`\n",
    "        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "    \n",
    "        return shifted_input_ids\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_tokens_right_mask(input_ids: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Shift input ids one token to the right.\n",
    "        \"\"\"\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[:,:, 1:] = input_ids[:,:, :-1].clone()\n",
    "        shifted_input_ids[:,:, 0] = 0\n",
    "        return shifted_input_ids.squeeze()\n",
    "\n",
    "def run(hparams):\n",
    "    print(hparams)\n",
    "    model = Wav2SeqModel(hparams)\n",
    "    trainer = Trainer(max_epochs=10,devices=1, accelerator=\"gpu\", logger=wandb_logger)\n",
    "    trainer.fit(model,SpeechRecognitionDataModule(WAV2VEC2_ARGS,num_workers=4,hparams=hparams))\n",
    "    return model, trainer\n",
    "\n",
    "\n",
    "hparams = argparse.Namespace()\n",
    "hparams.wav2vec2_model = 'facebook/wav2vec2-large-960h-lv60-self'\n",
    "hparams.lm_model = 'bert-base-uncased' #'facebook/bart-large'\n",
    "hparams.vocab_size = 40000\n",
    "hparams.learning_rate = 1e-3\n",
    "hparams.batch_size = 2\n",
    "\n",
    "model,trainer = run(hparams=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b93d726-a184-4b4a-8f79-a3a0dd7138b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b977664c-3adc-4fbe-9e90-27c2f6122550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e535c21e-e52c-41de-80bc-85cf0cfdfd17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b69eba-5e44-4cc2-9696-75e7b7b10f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904a2de5-a0cd-44d2-9063-b08f49ac89df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3ae905-4e53-463c-8909-2a819e709646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911cad7a-19dc-4aa1-a0e4-e64c1aed73b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926d0c03-552c-47ff-baf5-762b961714a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
