{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e17805b-1f4d-4f26-963b-1f4b2c313176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os. chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35d15bd4-eac4-44fc-ba58-c37ef16b8950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Python 3.8.6\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e88e5d34-1bce-4582-9ec1-d61e3748ff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torchmetrics.text import WordErrorRate\n",
    "from typing import Optional\n",
    "from pytorch_lightning import LightningModule, LightningDataModule, Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from constants.mir_constants import TrainingArgs, WAV2VEC2_ARGS\n",
    "from dataclasses import dataclass, asdict, field # type: ignore\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import WhisperTokenizer, WhisperFeatureExtractor, Wav2Vec2Processor, BertTokenizer,WhisperForConditionalGeneration,BartForConditionalGeneration\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM,AutoModelForCTC, AutoModelForSeq2SeqLM,AutoFeatureExtractor\n",
    "from datasets import load_dataset, Dataset, Audio\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from flash.audio import SpeechRecognition, SpeechRecognitionData\n",
    "from training.wav2vec2_finetune import Wav2Vec2SpeechRecognition, SpeechRecognitionData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76265934-e2f6-4135-ba58-cdfc23f6843d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgreeshmasmenon\u001b[0m (\u001b[33msongslyricstranscription\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230826_070409-kdsm77xh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/songslyricstranscription/SLG%20-%20Whisper%20transfer%20learning/runs/kdsm77xh' target=\"_blank\">smart-pine-17</a></strong> to <a href='https://wandb.ai/songslyricstranscription/SLG%20-%20Whisper%20transfer%20learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/songslyricstranscription/SLG%20-%20Whisper%20transfer%20learning' target=\"_blank\">https://wandb.ai/songslyricstranscription/SLG%20-%20Whisper%20transfer%20learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/songslyricstranscription/SLG%20-%20Whisper%20transfer%20learning/runs/kdsm77xh' target=\"_blank\">https://wandb.ai/songslyricstranscription/SLG%20-%20Whisper%20transfer%20learning/runs/kdsm77xh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TRAIN_FILE_PATH\": \"/scratch/users/gmenon/train_song_metadata_en_demucs_cleaned_filtered_095.csv\",\n",
      "    \"TEST_FILE_PATH\": \"/scratch/users/gmenon/validation_song_metadata_en_demucs_cleaned_filtered_005.csv\",\n",
      "    \"MODEL_BACKBONE\": \"facebook/wav2vec2-large-960h-lv60-self\",\n",
      "    \"BATCH_SIZE\": 1,\n",
      "    \"NUM_EPOCHS\": 15,\n",
      "    \"MODEL_SAVE_PATH\": \"/scratch/users/gmenon//model_artefacts/wav2vec2_demucs_en_large-960h-lv60-self_freeze_unfreeze_15epochs_adamw.pt\",\n",
      "    \"FINETUNE_STRATEGY\": [\n",
      "        \"freeze_unfreeze\",\n",
      "        10\n",
      "    ],\n",
      "    \"LR_SCHEDULER\": \"reduce_on_plateau_schedule\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(project=\"SLG - Whisper transfer learning\",log_model=False,)\n",
    "\n",
    "print(json.dumps(asdict(WAV2VEC2_ARGS), indent = 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a453ca2-eb3e-445b-aae7-f2ffd75be622",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b5943c4-72ae-442c-a4e3-7714fbf5ef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    feature_extractor: AutoFeatureExtractor\n",
    "    tokenizer: AutoTokenizer\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        label_attention_features =[{\"input_ids\": feature[\"label_attention_mask\"]} for feature in features]\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "\n",
    "        label_attention_batch = self.tokenizer.pad(\n",
    "            label_attention_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels_attention = label_attention_batch[\"input_ids\"]\n",
    "        #.masked_fill(labels_batch.input_ids.eq(101), 0).masked_fill(labels_batch.input_ids.eq(102), 0)\n",
    "        labels = labels[:, 1:]\n",
    "        labels_attention = labels_attention[:,1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        batch[\"label_attention_mask\"]  = labels_attention\n",
    "\n",
    "        return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f04da46d-502e-4e38-8531-d4a25be5bcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechRecognitionDataModule(LightningDataModule):\n",
    "    def __init__(self, WAV2VEC2_ARGS: WAV2VEC2_ARGS, num_workers,hparams):\n",
    "        super().__init__()\n",
    "        self.batch_size = WAV2VEC2_ARGS.BATCH_SIZE\n",
    "        self.num_workers = num_workers\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(hparams.lm_model)\n",
    "        self.feature_extractor = WhisperFeatureExtractor.from_pretrained(hparams.whisper_model)\n",
    "        #self.feature_extractor = AutoFeatureExtractor(do_normalize=True, return_attention_mask=True)\n",
    "        self.data_collator = DataCollatorSpeechSeq2SeqWithPadding(feature_extractor=self.feature_extractor, tokenizer=self.tokenizer, padding=True)\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        train_df = pd.read_csv(WAV2VEC2_ARGS.TRAIN_FILE_PATH).head(100)\n",
    "        validation_df = pd.read_csv(WAV2VEC2_ARGS.TEST_FILE_PATH).head(10)\n",
    "        if stage == 'fit' or stage is None:\n",
    "            print(\"In Stage = Fit\")\n",
    "            train_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(train_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(train_df[\"transcription\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "            self.train_dataset = train_dataset.map(self.prepare_dataset,remove_columns = train_dataset.column_names)\n",
    "            \n",
    "            val_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(validation_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(validation_df[\"transcription\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "            self.val_dataset = val_dataset.map(self.prepare_dataset,remove_columns = val_dataset.column_names)\n",
    "\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            print(\"In Stage = Test\")\n",
    "            test_dataset = Dataset.from_dict(\n",
    "                    {\"audio\": list(validation_df[\"consolidated_file_path\"]),\n",
    "                    \"transcription\": list(validation_df[\"transcription\"])}).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "            test_dataset = val_dataset.map(self.prepare_dataset,remove_columns = val_dataset.column_names)\n",
    "            self.test_dataset = test_dataset\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        print(\"entering train data loader\")\n",
    "        return DataLoader(\n",
    "            self.train_dataset.with_format(\"torch\"), \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn = self.data_collator\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        print(\"entering val data loader\")\n",
    "        return DataLoader(\n",
    "            self.val_dataset.with_format(\"torch\"), \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn = self.data_collator\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        print(\"entering test data loader\")\n",
    "        return DataLoader(\n",
    "            self.test_dataset.with_format(\"torch\"), \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn = self.data_collator\n",
    "        )\n",
    "\n",
    "    def prepare_dataset(self,batch):\n",
    "        audio = batch[\"audio\"]\n",
    "        # batched output is \"un-batched\" to ensure mapping is correct\n",
    "        batch[\"input_features\"] = self.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "        batch[\"input_length\"] = len(batch[\"input_features\"])\n",
    "        batch[\"labels\"] = self.tokenizer(batch[\"transcription\"]).input_ids\n",
    "        batch[\"label_attention_mask\"] = self.tokenizer(batch[\"transcription\"]).attention_mask\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0e6dafe-cbbb-4e05-a0cb-53a57a5668b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wav2SeqModel(LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.batch_size = hparams.batch_size\n",
    "        self.learning_rate=hparams.learning_rate\n",
    "        self.save_hyperparameters()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(hparams.lm_model)\n",
    "        self.whisper = WhisperForConditionalGeneration.from_pretrained(hparams.whisper_model).model.encoder\n",
    "        self.seq2seq = AutoModelForCausalLM.from_pretrained(hparams.lm_model)\n",
    "        #self.seq2seq = AutoModelForSeq2SeqLM.from_pretrained(hparams.lm_model)\n",
    "        #self.seq2seq = BartForConditionalGeneration.from_pretrained(hparams.lm_model,\n",
    "        #                                            forced_bos_token_id=0) #https://github.com/huggingface/transformers/issues/15559\n",
    "        self.seq2seq.config.is_decoder = True\n",
    "        self.seq2seq.add_cross_attention = True\n",
    "        self.bridging_layer = nn.Linear(self.whisper.config.hidden_size, self.seq2seq.config.hidden_size)\n",
    "\n",
    "    def forward(self, audio, labels, label_attention_mask):\n",
    "        #print(\"entering forward step\")\n",
    "        self.whisper.eval()\n",
    "        self.seq2seq.train()\n",
    "        encoder_outputs = self.whisper(audio,\n",
    "                                        output_hidden_states=True,\n",
    "                                        output_attentions=True)\n",
    "        encoder_hidden_states = encoder_outputs[0]  \n",
    "        encoder_hidden_states = self.bridging_layer(encoder_hidden_states)\n",
    "        decoder_input_ids = self.shift_tokens_right(labels) \n",
    "        decoder_attention_masks = self.shift_tokens_right_mask(label_attention_mask)\n",
    "        decoder_outputs = self.seq2seq(input_ids=decoder_input_ids,\n",
    "                                       encoder_hidden_states=encoder_hidden_states,)\n",
    "        return decoder_outputs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        #print(\"entering training step\")\n",
    "        audio = batch[\"input_features\"]\n",
    "        label_attention_mask = batch[\"label_attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        logits = self(audio,labels,label_attention_mask).logits\n",
    "        input_lengths = torch.full(size=(self.batch_size,), fill_value=logits.shape[0], dtype=torch.long)\n",
    "        target_lengths = torch.full(size=(self.batch_size,), fill_value=labels.shape[0], dtype=torch.long)\n",
    "        ce_loss = nn.CrossEntropyLoss()\n",
    "        loss = ce_loss(logits.squeeze(),labels.squeeze())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch,batch_idx):\n",
    "        #print(\"validation_step\")\n",
    "        audio = batch[\"input_features\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        #attention_mask = batch[\"attention_mask\"]\n",
    "        label_attention_mask = batch[\"label_attention_mask\"]\n",
    "        logits = self(audio,labels,label_attention_mask).logits\n",
    "        ce_loss = nn.CrossEntropyLoss()\n",
    "        loss = ce_loss(logits.squeeze(),labels.squeeze())\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        print(f\"original text = {self.tokenizer.decode(labels[0],skip_special_tokens=False)}, labels = {labels[0]}\")\n",
    "        print(f\"Predicted text = {self.tokenizer.decode(predicted_ids[0],skip_special_tokens=False)}, predicted ids = {predicted_ids[0]}\")\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        print(\"Entering Optimization Step\")\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int=0, decoder_start_token_id: int=101):\n",
    "        \"\"\"\n",
    "        Shift input ids one token to the right.\n",
    "        \"\"\"\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "        if decoder_start_token_id is None:\n",
    "            raise ValueError(\"Make sure to set the decoder_start_token_id attribute of the model's configuration.\")\n",
    "        shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "    \n",
    "        if pad_token_id is None:\n",
    "            raise ValueError(\"Make sure to set the pad_token_id attribute of the model's configuration.\")\n",
    "        # replace possible -100 values in labels by `pad_token_id`\n",
    "        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "    \n",
    "        return shifted_input_ids\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_tokens_right_mask(input_ids: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Shift input ids one token to the right.\n",
    "        \"\"\"\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "        shifted_input_ids[:, 0] = 0\n",
    "        return shifted_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c7b72eb-e170-49bc-af72-3d694b6fb73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(hparams):\n",
    "    print(hparams)\n",
    "    model = Wav2SeqModel(hparams)\n",
    "    trainer = Trainer(max_epochs=1,devices=1, accelerator=\"gpu\")\n",
    "    trainer.fit(model,SpeechRecognitionDataModule(WAV2VEC2_ARGS,num_workers=4,hparams=hparams))\n",
    "    return model, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57f60aa5-0635-421f-bfb5-73f7e9cf99a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1, learning_rate=1e-07, lm_model='bert-base-uncased', vocab_size=20000, wav2vec2_model='facebook/wav2vec2-large-960h-lv60-self', whisper_model='openai/whisper-large-v2')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m hparams\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-7\u001b[39m\n\u001b[1;32m      7\u001b[0m hparams\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 9\u001b[0m model,trainer \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(hparams)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(hparams):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(hparams)\n\u001b[0;32m----> 3\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mWav2SeqModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m Trainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mfit(model,SpeechRecognitionDataModule(WAV2VEC2_ARGS,num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,hparams\u001b[38;5;241m=\u001b[39mhparams))\n",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m, in \u001b[0;36mWav2SeqModel.__init__\u001b[0;34m(self, hparams)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_hyperparameters()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(hparams\u001b[38;5;241m.\u001b[39mlm_model)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhisper \u001b[38;5;241m=\u001b[39m \u001b[43mWhisperForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhisper_model\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mencoder\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq2seq \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(hparams\u001b[38;5;241m.\u001b[39mlm_model)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#self.seq2seq = AutoModelForSeq2SeqLM.from_pretrained(hparams.lm_model)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#self.seq2seq = BartForConditionalGeneration.from_pretrained(hparams.lm_model,\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#                                            forced_bos_token_id=0) #https://github.com/huggingface/transformers/issues/15559\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/transformers/modeling_utils.py:2937\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2934\u001b[0m     init_contexts\u001b[38;5;241m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   2936\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 2937\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2939\u001b[0m \u001b[38;5;66;03m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[1;32m   2940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_keep_in_fp32_modules:\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/transformers/models/whisper/modeling_whisper.py:1401\u001b[0m, in \u001b[0;36mWhisperForConditionalGeneration.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: WhisperConfig):\n\u001b[1;32m   1400\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m-> 1401\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mWhisperModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1402\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39md_model, config\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/transformers/models/whisper/modeling_whisper.py:1232\u001b[0m, in \u001b[0;36mWhisperModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m WhisperEncoder(config)\n\u001b[0;32m-> 1232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mWhisperDecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_init()\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/transformers/models/whisper/modeling_whisper.py:985\u001b[0m, in \u001b[0;36mWhisperDecoder.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39md_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_positions \u001b[38;5;241m=\u001b[39m WhisperPositionalEmbedding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_target_positions, config\u001b[38;5;241m.\u001b[39md_model)\n\u001b[0;32m--> 985\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([WhisperDecoderLayer(config) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mdecoder_layers)])\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(config\u001b[38;5;241m.\u001b[39md_model)\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/transformers/models/whisper/modeling_whisper.py:985\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39md_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_positions \u001b[38;5;241m=\u001b[39m WhisperPositionalEmbedding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_target_positions, config\u001b[38;5;241m.\u001b[39md_model)\n\u001b[0;32m--> 985\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\u001b[43mWhisperDecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mdecoder_layers)])\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(config\u001b[38;5;241m.\u001b[39md_model)\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/transformers/models/whisper/modeling_whisper.py:542\u001b[0m, in \u001b[0;36mWhisperDecoderLayer.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39md_model\n\u001b[0;32m--> 542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m \u001b[43mWhisperAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder_attention_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_decoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mdropout\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn \u001b[38;5;241m=\u001b[39m ACT2FN[config\u001b[38;5;241m.\u001b[39mactivation_function]\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/transformers/models/whisper/modeling_whisper.py:339\u001b[0m, in \u001b[0;36mWhisperAttention.__init__\u001b[0;34m(self, embed_dim, num_heads, dropout, is_decoder, bias)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(embed_dim, embed_dim, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(embed_dim, embed_dim, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m--> 339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/torch/nn/modules/linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/slg_finetuned/lib/python3.8/site-packages/torch/nn/init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hparams = argparse.Namespace()\n",
    "hparams.wav2vec2_model = 'facebook/wav2vec2-large-960h-lv60-self'\n",
    "hparams.whisper_model = 'openai/whisper-large-v2'#'openai/whisper-large'\n",
    "hparams.lm_model = 'bert-base-uncased' #'bert-base-uncased' #\n",
    "hparams.vocab_size = 20000\n",
    "hparams.learning_rate = 1e-7\n",
    "hparams.batch_size = 1\n",
    "\n",
    "model,trainer = run(hparams=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faa8d72-33d8-4fb5-aa62-cdc42c3c170e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(hparams.whisper_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(hparams.lm_model)\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(feature_extractor=feature_extractor, tokenizer=tokenizer, padding=True)\n",
    "dataset = SpeechRecognitionDataModule(WAV2VEC2_ARGS,num_workers=4,hparams=hparams)\n",
    "dataset.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27880bec-9e43-40ea-86fd-7e5f1df3ec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = next(iter(DataLoader(\n",
    "            dataset.val_dataset.with_format(\"torch\"), \n",
    "            batch_size=1, \n",
    "            num_workers=1,\n",
    "            collate_fn = data_collator)))\n",
    "model.eval()\n",
    "output= model(data[\"input_features\"],torch.Tensor([[101,0,0,0,0,0,0,0]]).type(torch.int32),data[\"label_attention_mask\"])\n",
    "tokenizer.decode(torch.argmax(output.logits,dim=-1)[0]),tokenizer.decode(data[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d6cc3e-0031-4034-99f5-ec820f9723d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e9afab-873e-4075-8912-5151706a30ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d3bb95-d49c-4b98-bcf1-6830ae485692",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b359ab5-5965-4cba-a65f-707078b581a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.Tensor([[101,0,0,0,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce8e5f3-7c99-40ce-a837-140eb3eacf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(torch.Tensor(dataset.val_dataset[0][\"input_features\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a213f3-1152-4ca0-bd96-d3a8a4aa6f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'input_features': tensor([[[-0.0521, -0.1011, -0.1179,  ..., -0.6646, -0.6646, -0.6646],\n",
    "#          [-0.2894, -0.3708, -0.3870,  ..., -0.6646, -0.6646, -0.6646],\n",
    "#          [-0.3774, -0.6646, -0.5762,  ..., -0.6646, -0.6646, -0.6646],\n",
    "#          ...,\n",
    "#          [-0.3111, -0.6646, -0.6646,  ..., -0.6646, -0.6646, -0.6646],\n",
    "#          [-0.3033, -0.6646, -0.6646,  ..., -0.6646, -0.6646, -0.6646],\n",
    "#          [-0.2985, -0.6646, -0.6646,  ..., -0.6646, -0.6646, -0.6646]]],\n",
    "#        device='cuda:0'), 'labels': tensor([[ 2061,  4553,  2013,  2115, 12051,   102]], device='cuda:0'), 'label_attention_mask': tensor([[1, 1, 1, 1, 1, 0]], device='cuda:0')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceef870-e290-4db5-a5fc-e2f802ecfaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WhisperConfig {\n",
    "#   \"_name_or_path\": \"openai/whisper-large\",\n",
    "#   \"activation_dropout\": 0.0,\n",
    "#   \"activation_function\": \"gelu\",\n",
    "#   \"apply_spec_augment\": false,\n",
    "#   \"architectures\": [\n",
    "#     \"WhisperForConditionalGeneration\"\n",
    "#   ],\n",
    "#   \"attention_dropout\": 0.0,\n",
    "#   \"begin_suppress_tokens\": [\n",
    "#     220,\n",
    "#     50257\n",
    "#   ],\n",
    "#   \"bos_token_id\": 50257,\n",
    "#   \"classifier_proj_size\": 256,\n",
    "#   \"d_model\": 1280,\n",
    "#   \"decoder_attention_heads\": 20,\n",
    "#   \"decoder_ffn_dim\": 5120,\n",
    "#   \"decoder_layerdrop\": 0.0,\n",
    "#   \"decoder_layers\": 32,\n",
    "#   \"decoder_start_token_id\": 50258,\n",
    "#   \"dropout\": 0.0,\n",
    "#   \"encoder_attention_heads\": 20,\n",
    "#   \"encoder_ffn_dim\": 5120,\n",
    "#   \"encoder_layerdrop\": 0.0,\n",
    "#   \"encoder_layers\": 32,\n",
    "#   \"eos_token_id\": 50257,\n",
    "#   \"forced_decoder_ids\": [\n",
    "#     [\n",
    "#       1,\n",
    "#       50358\n",
    "#     ],\n",
    "#     [\n",
    "#       2,\n",
    "#       50363\n",
    "#     ]\n",
    "#   ],\n",
    "#   \"init_std\": 0.02,\n",
    "#   \"is_encoder_decoder\": true,\n",
    "#   \"mask_feature_length\": 10,\n",
    "#   \"mask_feature_min_masks\": 0,\n",
    "#   \"mask_feature_prob\": 0.0,\n",
    "#   \"mask_time_length\": 10,\n",
    "#   \"mask_time_min_masks\": 2,\n",
    "#   \"mask_time_prob\": 0.05,\n",
    "#   \"max_length\": 448,\n",
    "#   \"max_source_positions\": 1500,\n",
    "#   \"max_target_positions\": 448,\n",
    "#   \"median_filter_width\": 7,\n",
    "#   \"model_type\": \"whisper\",\n",
    "#   \"num_hidden_layers\": 32,\n",
    "#   \"num_mel_bins\": 80,\n",
    "#   \"pad_token_id\": 50257,\n",
    "#   \"scale_embedding\": false,\n",
    "#   \"suppress_tokens\": [\n",
    "#     1,\n",
    "#     2,\n",
    "#     7,\n",
    "#     8,\n",
    "#     9,\n",
    "#     10,\n",
    "#     14,\n",
    "#     25,\n",
    "#     26,\n",
    "#     27,\n",
    "#     28,\n",
    "#     29,\n",
    "#     31,\n",
    "#     58,\n",
    "#     59,\n",
    "#     60,\n",
    "#     61,\n",
    "#     62,\n",
    "#     63,\n",
    "#     90,\n",
    "#     91,\n",
    "#     92,\n",
    "#     93,\n",
    "#     359,\n",
    "#     503,\n",
    "#     522,\n",
    "#     542,\n",
    "#     873,\n",
    "#     893,\n",
    "#     902,\n",
    "#     918,\n",
    "#     922,\n",
    "#     931,\n",
    "#     1350,\n",
    "#     1853,\n",
    "#     1982,\n",
    "#     2460,\n",
    "#     2627,\n",
    "#     3246,\n",
    "#     3253,\n",
    "#     3268,\n",
    "#     3536,\n",
    "#     3846,\n",
    "#     3961,\n",
    "#     4183,\n",
    "#     4667,\n",
    "#     6585,\n",
    "#     6647,\n",
    "#     7273,\n",
    "#     9061,\n",
    "#     9383,\n",
    "#     10428,\n",
    "#     10929,\n",
    "#     11938,\n",
    "#     12033,\n",
    "#     12331,\n",
    "#     12562,\n",
    "#     13793,\n",
    "#     14157,\n",
    "#     14635,\n",
    "#     15265,\n",
    "#     15618,\n",
    "#     16553,\n",
    "#     16604,\n",
    "#     18362,\n",
    "#     18956,\n",
    "#     20075,\n",
    "#     21675,\n",
    "#     22520,\n",
    "#     26130,\n",
    "#     26161,\n",
    "#     26435,\n",
    "#     28279,\n",
    "#     29464,\n",
    "#     31650,\n",
    "#     32302,\n",
    "#     32470,\n",
    "#     36865,\n",
    "#     42863,\n",
    "#     47425,\n",
    "#     49870,\n",
    "#     50254,\n",
    "#     50258,\n",
    "#     50358,\n",
    "#     50359,\n",
    "#     50360,\n",
    "#     50361,\n",
    "#     50362\n",
    "#   ],\n",
    "#   \"torch_dtype\": \"float32\",\n",
    "#   \"transformers_version\": \"4.33.0.dev0\",\n",
    "#   \"use_cache\": true,\n",
    "#   \"use_weighted_layer_sum\": false,\n",
    "#   \"vocab_size\": 51865\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "144eb862-8cd4-4678-ba4e-193639850361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70734fc8-cd1e-429b-afe2-0998abdeb7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgreeshmasmenon\u001b[0m (\u001b[33msongslyricstranscription\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230826_093027-7l60eeuf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/songslyricstranscription/SLG%20-%20Whisper%20transfer%20learning/runs/7l60eeuf' target=\"_blank\">firm-sun-37</a></strong> to <a href='https://wandb.ai/songslyricstranscription/SLG%20-%20Whisper%20transfer%20learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/songslyricstranscription/SLG%20-%20Whisper%20transfer%20learning' target=\"_blank\">https://wandb.ai/songslyricstranscription/SLG%20-%20Whisper%20transfer%20learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/songslyricstranscription/SLG%20-%20Whisper%20transfer%20learning/runs/7l60eeuf' target=\"_blank\">https://wandb.ai/songslyricstranscription/SLG%20-%20Whisper%20transfer%20learning/runs/7l60eeuf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TRAIN_FILE_PATH\": \"/scratch/users/gmenon/train_song_metadata_en_demucs_cleaned_filtered_095.csv\",\n",
      "    \"TEST_FILE_PATH\": \"/scratch/users/gmenon/validation_song_metadata_en_demucs_cleaned_filtered_005.csv\",\n",
      "    \"MODEL_BACKBONE\": \"facebook/wav2vec2-large-960h-lv60-self\",\n",
      "    \"BATCH_SIZE\": 1,\n",
      "    \"NUM_EPOCHS\": 15,\n",
      "    \"MODEL_SAVE_PATH\": \"/scratch/users/gmenon//model_artefacts/wav2vec2_demucs_en_large-960h-lv60-self_freeze_unfreeze_15epochs_adamw.pt\",\n",
      "    \"FINETUNE_STRATEGY\": [\n",
      "        \"freeze_unfreeze\",\n",
      "        10\n",
      "    ],\n",
      "    \"LR_SCHEDULER\": \"reduce_on_plateau_schedule\"\n",
      "}\n",
      "Namespace(batch_size=1, learning_rate=1e-05, lm_model='bert-base-uncased', max_epochs=3, vocab_size=20000, wav2vec2_model='facebook/wav2vec2-large-960h-lv60-self', whisper_model='openai/whisper-large-v2')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Stage = Fit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa60dd85e30c44b38e6ede2b4888e093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8553210ec234dd7b0b7c3ef3a209e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type            | Params\n",
      "---------------------------------------------------\n",
      "0 | whisper        | WhisperEncoder  | 636 M \n",
      "1 | seq2seq        | BertLMHeadModel | 109 M \n",
      "2 | bridging_layer | Linear          | 983 K \n",
      "3 | wer            | WordErrorRate   | 0     \n",
      "---------------------------------------------------\n",
      "747 M     Trainable params\n",
      "0         Non-trainable params\n",
      "747 M     Total params\n",
      "2,989.131 Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering Optimization Step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entering val data loader\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text = so learn from your mistakes [SEP], labels = tensor([ 2061,  4553,  2013,  2115, 12051,   102], device='cuda:0')\n",
      "Predicted text = and and and and its and, predicted ids = tensor([1998, 1998, 1998, 1998, 2049, 1998], device='cuda:0')\n",
      "original text = i've been connected to the right line [SEP], labels = tensor([1045, 1005, 2310, 2042, 4198, 2000, 1996, 2157, 2240,  102],\n",
      "       device='cuda:0')\n",
      "Predicted text = and.. t been. to the right line, predicted ids = tensor([1998, 1012, 1012, 1056, 2042, 1012, 2000, 1996, 2157, 2240],\n",
      "       device='cuda:0')\n",
      "entering train data loader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/irisgpfs/users/gmenon/workspace/songsLyricsGenerator/src/slg_finetuned/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2571aefb34490c975ce34ef858d4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text = so learn from your mistakes [SEP], labels = tensor([ 2061,  4553,  2013,  2115, 12051,   102], device='cuda:0')\n",
      "Predicted text = and i i i i and, predicted ids = tensor([1998, 1045, 1045, 1045, 1045, 1998], device='cuda:0')\n",
      "original text = i've been connected to the right line [SEP], labels = tensor([1045, 1005, 2310, 2042, 4198, 2000, 1996, 2157, 2240,  102],\n",
      "       device='cuda:0')\n",
      "Predicted text = and........., predicted ids = tensor([1998, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012],\n",
      "       device='cuda:0')\n",
      "original text = the truth to be found [SEP], labels = tensor([1996, 3606, 2000, 2022, 2179,  102], device='cuda:0')\n",
      "Predicted text = and \". \" be., predicted ids = tensor([1998, 1000, 1012, 1000, 2022, 1012], device='cuda:0')\n",
      "original text = he said the way myblue eyes shined [SEP], labels = tensor([ 2002,  2056,  1996,  2126,  2026, 16558,  5657,  2159, 12342,  2094,\n",
      "          102], device='cuda:0')\n",
      "Predicted text = and.. \"..... \" \", predicted ids = tensor([1998, 1012, 1012, 1000, 1012, 1012, 1012, 1012, 1012, 1000, 1000],\n",
      "       device='cuda:0')\n",
      "original text = you leave me once again home alone [SEP], labels = tensor([2017, 2681, 2033, 2320, 2153, 2188, 2894,  102], device='cuda:0')\n",
      "Predicted text = and. you you. he. you, predicted ids = tensor([1998, 1012, 2017, 2017, 1012, 2002, 1012, 2017], device='cuda:0')\n",
      "original text = while they are in commend [SEP], labels = tensor([2096, 2027, 2024, 1999, 4012, 3549, 2094,  102], device='cuda:0')\n",
      "Predicted text = and and is is in.. is, predicted ids = tensor([1998, 1998, 2003, 2003, 1999, 1012, 1012, 2003], device='cuda:0')\n",
      "original text = a life all mine [SEP], labels = tensor([1037, 2166, 2035, 3067,  102], device='cuda:0')\n",
      "Predicted text = and i \" \"., predicted ids = tensor([1998, 1045, 1000, 1000, 1012], device='cuda:0')\n",
      "original text = so i never went back [SEP], labels = tensor([2061, 1045, 2196, 2253, 2067,  102], device='cuda:0')\n",
      "Predicted text = and i i i i., predicted ids = tensor([1998, 1045, 1045, 1045, 1045, 1012], device='cuda:0')\n",
      "original text = in you i taste god [SEP], labels = tensor([1999, 2017, 1045, 5510, 2643,  102], device='cuda:0')\n",
      "Predicted text = and. i i i i, predicted ids = tensor([1998, 1012, 1045, 1045, 1045, 1045], device='cuda:0')\n",
      "original text = could stay a while [SEP], labels = tensor([2071, 2994, 1037, 2096,  102], device='cuda:0')\n",
      "Predicted text = and \" i \" i, predicted ids = tensor([1998, 1000, 1045, 1000, 1045], device='cuda:0')\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text = so learn from your mistakes [SEP], labels = tensor([ 2061,  4553,  2013,  2115, 12051,   102], device='cuda:0')\n",
      "Predicted text = . i i i you., predicted ids = tensor([1012, 1045, 1045, 1045, 2017, 1012], device='cuda:0')\n",
      "original text = i've been connected to the right line [SEP], labels = tensor([1045, 1005, 2310, 2042, 4198, 2000, 1996, 2157, 2240,  102],\n",
      "       device='cuda:0')\n",
      "Predicted text = ...''.. \".., predicted ids = tensor([1012, 1012, 1012, 1005, 1005, 1012, 1012, 1000, 1012, 1012],\n",
      "       device='cuda:0')\n",
      "original text = the truth to be found [SEP], labels = tensor([1996, 3606, 2000, 2022, 2179,  102], device='cuda:0')\n",
      "Predicted text = . \" \" \" \" \", predicted ids = tensor([1012, 1000, 1000, 1000, 1000, 1000], device='cuda:0')\n",
      "original text = he said the way myblue eyes shined [SEP], labels = tensor([ 2002,  2056,  1996,  2126,  2026, 16558,  5657,  2159, 12342,  2094,\n",
      "          102], device='cuda:0')\n",
      "Predicted text = .. \" \" \".. \" \" \" \", predicted ids = tensor([1012, 1012, 1000, 1000, 1000, 1012, 1012, 1000, 1000, 1000, 1000],\n",
      "       device='cuda:0')\n",
      "original text = you leave me once again home alone [SEP], labels = tensor([2017, 2681, 2033, 2320, 2153, 2188, 2894,  102], device='cuda:0')\n",
      "Predicted text = . you you you you you you you, predicted ids = tensor([1012, 2017, 2017, 2017, 2017, 2017, 2017, 2017], device='cuda:0')\n",
      "original text = while they are in commend [SEP], labels = tensor([2096, 2027, 2024, 1999, 4012, 3549, 2094,  102], device='cuda:0')\n",
      "Predicted text = . \" \" \" \".. \", predicted ids = tensor([1012, 1000, 1000, 1000, 1000, 1012, 1012, 1000], device='cuda:0')\n",
      "original text = a life all mine [SEP], labels = tensor([1037, 2166, 2035, 3067,  102], device='cuda:0')\n",
      "Predicted text = .'\" \" \", predicted ids = tensor([1012, 1005, 1000, 1000, 1000], device='cuda:0')\n",
      "original text = so i never went back [SEP], labels = tensor([2061, 1045, 2196, 2253, 2067,  102], device='cuda:0')\n",
      "Predicted text = . i'' i., predicted ids = tensor([1012, 1045, 1005, 1005, 1045, 1012], device='cuda:0')\n",
      "original text = in you i taste god [SEP], labels = tensor([1999, 2017, 1045, 5510, 2643,  102], device='cuda:0')\n",
      "Predicted text = .'i i i i, predicted ids = tensor([1012, 1005, 1045, 1045, 1045, 1045], device='cuda:0')\n",
      "original text = could stay a while [SEP], labels = tensor([2071, 2994, 1037, 2096,  102], device='cuda:0')\n",
      "Predicted text = . \" i \" \", predicted ids = tensor([1012, 1000, 1045, 1000, 1000], device='cuda:0')\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text = so learn from your mistakes [SEP], labels = tensor([ 2061,  4553,  2013,  2115, 12051,   102], device='cuda:0')\n",
      "Predicted text = and'i and you., predicted ids = tensor([1998, 1005, 1045, 1998, 2017, 1012], device='cuda:0')\n",
      "original text = i've been connected to the right line [SEP], labels = tensor([1045, 1005, 2310, 2042, 4198, 2000, 1996, 2157, 2240,  102],\n",
      "       device='cuda:0')\n",
      "Predicted text = and''''.'' '., predicted ids = tensor([1998, 1005, 1005, 1005, 1005, 1012, 1005, 1005, 1005, 1012],\n",
      "       device='cuda:0')\n",
      "original text = the truth to be found [SEP], labels = tensor([1996, 3606, 2000, 2022, 2179,  102], device='cuda:0')\n",
      "Predicted text = and'''' ', predicted ids = tensor([1998, 1005, 1005, 1005, 1005, 1005], device='cuda:0')\n",
      "original text = he said the way myblue eyes shined [SEP], labels = tensor([ 2002,  2056,  1996,  2126,  2026, 16558,  5657,  2159, 12342,  2094,\n",
      "          102], device='cuda:0')\n",
      "Predicted text = and.. '. '...'', predicted ids = tensor([1998, 1012, 1012, 1005, 1012, 1005, 1012, 1012, 1012, 1005, 1005],\n",
      "       device='cuda:0')\n",
      "original text = you leave me once again home alone [SEP], labels = tensor([2017, 2681, 2033, 2320, 2153, 2188, 2894,  102], device='cuda:0')\n",
      "Predicted text = and you you you you you you you, predicted ids = tensor([1998, 2017, 2017, 2017, 2017, 2017, 2017, 2017], device='cuda:0')\n",
      "original text = while they are in commend [SEP], labels = tensor([2096, 2027, 2024, 1999, 4012, 3549, 2094,  102], device='cuda:0')\n",
      "Predicted text = and \" you you you.. \", predicted ids = tensor([1998, 1000, 2017, 2017, 2017, 1012, 1012, 1000], device='cuda:0')\n",
      "original text = a life all mine [SEP], labels = tensor([1037, 2166, 2035, 3067,  102], device='cuda:0')\n",
      "Predicted text = and'''', predicted ids = tensor([1998, 1005, 1005, 1005, 1005], device='cuda:0')\n",
      "original text = so i never went back [SEP], labels = tensor([2061, 1045, 2196, 2253, 2067,  102], device='cuda:0')\n",
      "Predicted text = and'''i., predicted ids = tensor([1998, 1005, 1005, 1005, 1045, 1012], device='cuda:0')\n",
      "original text = in you i taste god [SEP], labels = tensor([1999, 2017, 1045, 5510, 2643,  102], device='cuda:0')\n",
      "Predicted text = and'''' you, predicted ids = tensor([1998, 1005, 1005, 1005, 1005, 2017], device='cuda:0')\n",
      "original text = could stay a while [SEP], labels = tensor([2071, 2994, 1037, 2096,  102], device='cuda:0')\n",
      "Predicted text = and'''', predicted ids = tensor([1998, 1005, 1005, 1005, 1005], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os. chdir('/home/users/gmenon/workspace/songsLyricsGenerator/src')\n",
    "import lyrics_finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c0047f-74fa-49d7-8d46-0652e376a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os. chdir('/home/users/gmenon/workspace/songsLyricsGenerator/src')\n",
    "from training import lyrics_finetune\n",
    "#from constants.mir_constants import TrainingArgs, WAV2VEC2_ARGS\n",
    "#import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4eb08b-92b5-4d4e-a0a4-5944c17e4129",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = argparse.Namespace()\n",
    "hparams.wav2vec2_model = 'facebook/wav2vec2-large-960h-lv60-self'\n",
    "hparams.whisper_model = 'openai/whisper-large-v2'#'openai/whisper-large'\n",
    "hparams.lm_model = 'bert-base-uncased' #'bert-base-uncased' #\n",
    "hparams.vocab_size = 20000\n",
    "hparams.learning_rate = 1e-6\n",
    "hparams.batch_size = 1\n",
    "\n",
    "model,trainer = lyrics_finetune.run(hparams=hparams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
