{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c390f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install lightning-flash\n",
    "! pip install 'lightning-flash[audio,text]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468edd16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2018c0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/users/gmenon/.cache/huggingface/datasets/json/default-7435436ee2901bc5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37c9e6fe0aa473a99c0e43a3a088b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0453e95d275c4feb899df60ac1fd5b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating data split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/users/gmenon/.cache/huggingface/datasets/json/default-7435436ee2901bc5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70465adabda34f66820853302d525a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/users/gmenon/.cache/huggingface/datasets/json/default-0e24d7a652abdd34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8c50ea5c7e4a1f9e6a035bdabfa40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4b92a6f1ea481b98912378094c682b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating data split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/users/gmenon/.cache/huggingface/datasets/json/default-0e24d7a652abdd34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6711434190543878ad2937ce371c011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/gmenon/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3460: FutureWarning: Please pass an instantiated object of the `InputTransform` class. Passing the Class and keyword arguments separately has been deprecated since v0.8.0 and will be removed in v0.9.0.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import flash\n",
    "import torch\n",
    "from flash.audio import SpeechRecognition, SpeechRecognitionData\n",
    "from flash.core.data.utils import download_data\n",
    "\n",
    "# 1. Create the DataModule\n",
    "download_data(\"https://pl-flash-data.s3.amazonaws.com/timit_data.zip\", \"./data\")\n",
    "\n",
    "datamodule = SpeechRecognitionData.from_json(\n",
    "    \"file\",\n",
    "    \"text\",\n",
    "    train_file=\"data/timit/train.json\",\n",
    "    test_file=\"data/timit/test.json\",\n",
    "    batch_size=4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81a715cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 'facebook/wav2vec2-base-960h' provided by Hugging Face/transformers (https://github.com/huggingface/transformers) and PyTorch/fairseq (https://github.com/pytorch/fairseq).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Build the task\n",
    "model = SpeechRecognition(backbone=\"facebook/wav2vec2-base-960h\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c94298eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[W socket.cpp:426] [c10d] The server socket has failed to bind to [::]:19007 (errno: 98 - Address already in use).\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[W socket.cpp:426] [c10d] The server socket has failed to bind to 0.0.0.0:19007 (errno: 98 - Address already in use).\n",
      "[E socket.cpp:462] [c10d] The server socket has failed to listen on any local network address.\n",
      "[W socket.cpp:426] [c10d] The server socket has failed to bind to [::]:19007 (errno: 98 - Address already in use).\n",
      "[W socket.cpp:426] [c10d] The server socket has failed to bind to 0.0.0.0:19007 (errno: 98 - Address already in use).\n",
      "[E socket.cpp:462] [c10d] The server socket has failed to listen on any local network address.\n"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 1 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/users/gmenon/.local/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/home/users/gmenon/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 139, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/home/users/gmenon/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 650, in _fit_impl\n    self._run(model, ckpt_path=self.ckpt_path)\n  File \"/home/users/gmenon/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1048, in _run\n    self.strategy.setup_environment()\n  File \"/home/users/gmenon/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp_spawn.py\", line 138, in setup_environment\n    self.setup_distributed()\n  File \"/home/users/gmenon/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp_spawn.py\", line 174, in setup_distributed\n    _init_dist_connection(\n  File \"/home/users/gmenon/.local/lib/python3.8/site-packages/lightning_fabric/utilities/distributed.py\", line 245, in _init_dist_connection\n    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)\n  File \"/home/users/gmenon/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 888, in init_process_group\n    store, rank, world_size = next(rendezvous_iterator)\n  File \"/home/users/gmenon/.local/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 245, in _env_rendezvous_handler\n    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\n  File \"/home/users/gmenon/.local/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 176, in _create_c10d_store\n    return TCPStore(\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:19007 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:19007 (errno: 98 - Address already in use).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 3. Create the trainer and finetune the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m flash\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, gpus\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count())\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinetune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfreeze\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/flash/core/trainer.py:164\u001b[0m, in \u001b[0;36mTrainer.finetune\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule, strategy, train_bn)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Runs the full optimization routine. Same as :meth:`pytorch_lightning.Trainer.fit`, but unfreezes layers of\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03mthe backbone throughout training layers of the backbone throughout training.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m    train_bn: Whether to train Batch Norm layer\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resolve_callbacks(model, strategy, train_bn\u001b[38;5;241m=\u001b[39mtrain_bn)\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 608\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:36\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py:113\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     process_args \u001b[38;5;241m=\u001b[39m [trainer, function, args, kwargs, return_queue]\n\u001b[0;32m--> 113\u001b[0m \u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapping_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_processes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m worker_output \u001b[38;5;241m=\u001b[39m return_queue\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/multiprocessing/spawn.py:197\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/multiprocessing/spawn.py:160\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    158\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-- Process \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with the following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m error_index\n\u001b[1;32m    159\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m original_trace\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process\u001b[38;5;241m.\u001b[39mpid)\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 1 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/users/gmenon/.local/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/home/users/gmenon/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 139, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/home/users/gmenon/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 650, in _fit_impl\n    self._run(model, ckpt_path=self.ckpt_path)\n  File \"/home/users/gmenon/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1048, in _run\n    self.strategy.setup_environment()\n  File \"/home/users/gmenon/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp_spawn.py\", line 138, in setup_environment\n    self.setup_distributed()\n  File \"/home/users/gmenon/.local/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp_spawn.py\", line 174, in setup_distributed\n    _init_dist_connection(\n  File \"/home/users/gmenon/.local/lib/python3.8/site-packages/lightning_fabric/utilities/distributed.py\", line 245, in _init_dist_connection\n    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)\n  File \"/home/users/gmenon/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 888, in init_process_group\n    store, rank, world_size = next(rendezvous_iterator)\n  File \"/home/users/gmenon/.local/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 245, in _env_rendezvous_handler\n    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\n  File \"/home/users/gmenon/.local/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 176, in _create_c10d_store\n    return TCPStore(\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:19007 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:19007 (errno: 98 - Address already in use).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Create the trainer and finetune the model\n",
    "trainer = flash.Trainer(max_epochs=1, gpus=torch.cuda.device_count())\n",
    "trainer.finetune(model, datamodule=datamodule, strategy=\"freeze\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1fbcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Predict on audio files!\n",
    "datamodule = SpeechRecognitionData.from_files(predict_files=[\"data/timit/example.wav\"], batch_size=4)\n",
    "predictions = trainer.predict(model, datamodule=datamodule)\n",
    "print(predictions)\n",
    "\n",
    "# 5. Save the model!\n",
    "trainer.save_checkpoint(\"speech_recognition_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e011a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_gmenon",
   "language": "python",
   "name": "venv_gmenon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
