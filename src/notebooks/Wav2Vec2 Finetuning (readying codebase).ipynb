{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6beaeeda-3625-4093-8cd7-ccff652b23de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os. chdir('../')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f9f87fe4-f68c-4696-a4ca-d102b91c5119",
   "metadata": {},
   "source": [
    "! pip install pyctcdecode\n",
    "! pip install https://github.com/kpu/kenlm/archive/master.zip"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5b68904-1ef0-4856-928e-d9eaedac1360",
   "metadata": {},
   "source": [
    "! python wav2vec2_training.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5948413-854d-4ed0-b1df-08f3ac03fa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants.mir_constants import WAV2VEC2_ARGS\n",
    "from training.wav2vec2_finetune import Wav2Vec2SpeechRecognition, SpeechRecognitionData\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "import pandas as pd\n",
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bef4a0c2-8b7d-43d3-ae5a-6069ea20509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34df477b-e593-4a1b-ab65-97f2ecd4e848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run = wandb.init(\n",
    "#     project=\"song lyrics transcription\",\n",
    "#     notes=\"\",\n",
    "#     tags=[\"AdamW\", \"Finetune Wav2vec2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9fc9ec9-41ff-4e2b-8ad4-bddf798ea927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb_logger = WandbLogger()\n",
    "# trainer = Trainer(logger=wandb_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f88e8f2-3a31-4f49-bbd3-fa780103e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.config = asdict(WAV2VEC2_ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a359071b-4a39-4cf9-b89a-5ec410e4d144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TRAIN_FILE_PATH\": \"/scratch/users/gmenon/train_song_metadata_en_demucs_cleaned_filtered_095.csv\",\n",
      "    \"TEST_FILE_PATH\": \"/scratch/users/gmenon/validation_song_metadata_en_demucs_cleaned_filtered_005.csv\",\n",
      "    \"MODEL_BACKBONE\": \"facebook/wav2vec2-large-960h-lv60-self\",\n",
      "    \"BATCH_SIZE\": 1,\n",
      "    \"NUM_EPOCHS\": 15,\n",
      "    \"MODEL_SAVE_PATH\": \"/scratch/users/gmenon//model_artefacts/wav2vec2_demucs_en_large-960h-lv60-self_finetuned_15epochs.pt\",\n",
      "    \"FINETUNE_STRATEGY\": \"no_freeze_deepspeed\",\n",
      "    \"LR_SCHEDULER\": \"reduce_on_plateau_schedule\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgreeshmasmenon\u001b[0m (\u001b[33msongslyricstranscription\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230819_051306-6qe07rrx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20finetuning/runs/6qe07rrx' target=\"_blank\">devoted-dream-57</a></strong> to <a href='https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20finetuning' target=\"_blank\">https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20finetuning/runs/6qe07rrx' target=\"_blank\">https://wandb.ai/songslyricstranscription/SLG%20-%20wav2vec2%20finetuning/runs/6qe07rrx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 'facebook/wav2vec2-large-960h-lv60-self' provided by Hugging Face/transformers (https://github.com/huggingface/transformers).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"training\": [\n",
      "        9538,\n",
      "        5\n",
      "    ],\n",
      "    \"validation\": [\n",
      "        507,\n",
      "        5\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print hyperparameters used for training this model into wandb\n",
    "print(json.dumps(asdict(WAV2VEC2_ARGS), indent = 4))\n",
    "\n",
    "speech_recognition_task = Wav2Vec2SpeechRecognition(wav2vec2_args=WAV2VEC2_ARGS)\n",
    "\n",
    "#print shape of data in console. Will be visible in wandb logger\n",
    "print(json.dumps(speech_recognition_task.shape(), indent = 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41802556-5179-400d-a63e-67a945dbead4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type           | Params\n",
      "-------------------------------------------------\n",
      "0 | model         | Wav2Vec2ForCTC | 315 M \n",
      "1 | train_metrics | ModuleDict     | 0     \n",
      "2 | val_metrics   | ModuleDict     | 0     \n",
      "3 | test_metrics  | ModuleDict     | 0     \n",
      "-------------------------------------------------\n",
      "315 M     Trainable params\n",
      "0         Non-trainable params\n",
      "315 M     Total params\n",
      "630.943   Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e1f17c17e64321931217f303a0f05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "speech_recognition_task.finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "482b0836-ecf6-4ca7-8d5b-82d3f61aed31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 'facebook/wav2vec2-large-960h-lv60-self' provided by Hugging Face/transformers (https://github.com/huggingface/transformers).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a57f97edd14f919bfa651a720e4eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 9538it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "inference_files_lst = [\"/home/users/gmenon/workspace/songsLyricsGenerator/src/notebooks/separated/mdx_extra/ad887cbfa84749e5a9789f303e2f5c30/vocals.wav\",\n",
    "                  \"/home/users/gmenon/dali/DALI_v1.0/audio/wav_clips/ad887cbfa84749e5a9789f303e2f5c30.wav\"]\n",
    "\n",
    "inference_predictions = speech_recognition_task.inference(inference_files=inference_files_lst,\n",
    "                   batch_size=4,\n",
    "                   model_path=WAV2VEC2_ARGS.MODEL_SAVE_PATH,\n",
    "                  wav2vec2_trainer = speech_recognition_task.wav2vec2_trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db5c5e11-9db4-431b-a540-75e1b8efb6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Predictions = [['', '']]\n",
      "Starting Validation dataset Word Error Rate Metric Calculation\n"
     ]
    }
   ],
   "source": [
    "#Inference Predictions = [['I AM NOT THE ONE I YU TO CLA', 'I AM  O I A LA']]\n",
    "#Starting Validation dataset Word Error Rate Metric Calculation\n",
    "print(f\"Inference Predictions = {inference_predictions}\")\n",
    "\n",
    "print(\"Starting Validation dataset Word Error Rate Metric Calculation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfacfbc6-7337-4d30-a03a-8df172bb6dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(WAV2VEC2_ARGS.TEST_FILE_PATH)\n",
    "\n",
    "test_datamodule = SpeechRecognitionData.from_files(\n",
    "    predict_files=list(test_data[\"consolidated_file_path\"]), \n",
    "    batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0873eb66-bf82-449a-bf57-e91ae50b1710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16b74b2c9be46afa4281b318434d22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 9538it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finetuned_predictions = speech_recognition_task.wav2vec2_trainer.predict(speech_recognition_task.wav2vec2_model, \n",
    "                                                datamodule=test_datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4350f187-3d4d-4982-a9f5-5bf1adcf8391",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"finetuned_predictions\"] = finetuned_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74001a46-e9a1-482e-b9ab-708e5eee7f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "finetuned_pred_transformed = []\n",
    "for predictions in finetuned_predictions:\n",
    "    finetuned_pred_transformed.append(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5ec758f-0fb0-41f0-bc7d-2bb502c7d8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = list(test_data[\"transcription_capitalized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bc7b5b4-ca21-45a9-adc9-71b073899309",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = finetuned_pred_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44bbe521-2848-42ae-9423-aae864cd4a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset Word Error Rate = 1.0\n"
     ]
    }
   ],
   "source": [
    "error = wer(reference, hypothesis)\n",
    "print(f\"Test Dataset Word Error Rate = {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb07ab2-2929-4e2c-a390-cf46a469156b",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a546567-4a63-4954-9f0a-3c6d4a36e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(WAV2VEC2_ARGS.TRAIN_FILE_PATH)\n",
    "\n",
    "train_datamodule = SpeechRecognitionData.from_files(\n",
    "    predict_files=list(train_data[\"consolidated_file_path\"]), \n",
    "    batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17b47f0c-d0d1-46c2-9f13-307809390812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc1b83fa9cf4ada8756a1c7b407ddda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 9538it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finetuned_predictions = speech_recognition_task.wav2vec2_trainer.predict(speech_recognition_task.wav2vec2_model, \n",
    "                                                datamodule=train_datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1ed4baf-7ca1-4946-896d-280d1b618651",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"finetuned_predictions\"] = finetuned_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2dc18c74-b753-4a80-b4a3-acf0f81360a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finetuned_pred_transformed = []\n",
    "for predictions in finetuned_predictions:\n",
    "    finetuned_pred_transformed.append(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccb810d-9f9d-4761-ae4b-ed44d57a6c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = list(train_data[\"transcription_capitalized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cf469d-3537-4459-9fa0-edc7899e114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = finetuned_pred_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23f7341-ac28-470b-8fed-4df7baf6b7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = wer(reference, hypothesis)\n",
    "print(f\"Train Dataset Word Error Rate = {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e00fdcc-41d2-4e4b-bc5f-dcbc957ab7df",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77c410e-96d8-4f82-928a-50fd85b769de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
