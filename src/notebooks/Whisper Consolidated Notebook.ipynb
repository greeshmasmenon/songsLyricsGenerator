{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc04f475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False /bin/bash: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "    print('Not connected to a GPU')\n",
    "    NO_GPU = True\n",
    "else:\n",
    "    NO_GPU = False\n",
    "    print(NO_GPU, gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0343b921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-tupg05u1\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-tupg05u1\n",
      "  Resolved https://github.com/huggingface/transformers to commit 1689aea73346816b936b84932e12b774974e61a6\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from transformers==4.32.0.dev0) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from transformers==4.32.0.dev0) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from transformers==4.32.0.dev0) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from transformers==4.32.0.dev0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from transformers==4.32.0.dev0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from transformers==4.32.0.dev0) (2023.6.3)\n",
      "Requirement already satisfied: requests in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from transformers==4.32.0.dev0) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from transformers==4.32.0.dev0) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from transformers==4.32.0.dev0) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from transformers==4.32.0.dev0) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.32.0.dev0) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.32.0.dev0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from requests->transformers==4.32.0.dev0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from requests->transformers==4.32.0.dev0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from requests->transformers==4.32.0.dev0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from requests->transformers==4.32.0.dev0) (2020.6.20)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: librosa in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (0.10.0.post1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from librosa) (3.0.0)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from librosa) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from librosa) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from librosa) (1.2.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from librosa) (1.3.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from librosa) (4.4.2)\n",
      "Requirement already satisfied: numba>=0.51.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from librosa) (0.56.4)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from librosa) (1.7.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from librosa) (0.3.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from librosa) (4.7.1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from librosa) (0.1)\n",
      "Requirement already satisfied: msgpack>=1.0 in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from librosa) (1.0.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from numba>=0.51.0->librosa) (0.39.1)\n",
      "Requirement already satisfied: setuptools in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from numba>=0.51.0->librosa) (50.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from numba>=0.51.0->librosa) (6.0.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from pooch>=1.0->librosa) (3.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from pooch>=1.0->librosa) (23.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from pooch>=1.0->librosa) (2.28.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from scikit-learn>=0.20.0->librosa) (2.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2020.6.20)\n",
      "Requirement already satisfied: zipp>=0.5 in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from importlib-metadata->numba>=0.51.0->librosa) (3.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/evaluate\n",
      "  Cloning https://github.com/huggingface/evaluate to /tmp/pip-req-build-7zqgftx8\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/evaluate /tmp/pip-req-build-7zqgftx8\n",
      "  Resolved https://github.com/huggingface/evaluate to commit af3c30561d840b83e54fc5f7150ea58046d6af69\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from evaluate==0.4.1.dev0) (2.14.1.dev0)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from evaluate==0.4.1.dev0) (1.23.5)\n",
      "Requirement already satisfied: dill in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from evaluate==0.4.1.dev0) (0.3.6)\n",
      "Requirement already satisfied: pandas in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from evaluate==0.4.1.dev0) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from evaluate==0.4.1.dev0) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from evaluate==0.4.1.dev0) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from evaluate==0.4.1.dev0) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from evaluate==0.4.1.dev0) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from evaluate==0.4.1.dev0) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from evaluate==0.4.1.dev0) (0.16.4)\n",
      "Requirement already satisfied: packaging in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from evaluate==0.4.1.dev0) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from evaluate==0.4.1.dev0) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate==0.4.1.dev0) (12.0.1)\n",
      "Requirement already satisfied: aiohttp in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate==0.4.1.dev0) (3.8.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate==0.4.1.dev0) (6.0)\n",
      "Requirement already satisfied: filelock in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from huggingface-hub>=0.7.0->evaluate==0.4.1.dev0) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from huggingface-hub>=0.7.0->evaluate==0.4.1.dev0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from requests>=2.19.0->evaluate==0.4.1.dev0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from requests>=2.19.0->evaluate==0.4.1.dev0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from requests>=2.19.0->evaluate==0.4.1.dev0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from requests>=2.19.0->evaluate==0.4.1.dev0) (2020.6.20)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from pandas->evaluate==0.4.1.dev0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from pandas->evaluate==0.4.1.dev0) (2020.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1.dev0) (20.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1.dev0) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1.dev0) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1.dev0) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1.dev0) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1.dev0) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->evaluate==0.4.1.dev0) (1.15.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jiwer in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (3.0.2)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from jiwer) (8.1.6)\n",
      "Requirement already satisfied: rapidfuzz==2.13.7 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from jiwer) (2.13.7)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: gradio in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (3.38.0)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (22.1.0)\n",
      "Requirement already satisfied: aiohttp~=3.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (3.8.4)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (5.0.1)\n",
      "Requirement already satisfied: fastapi in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (0.100.0)\n",
      "Requirement already satisfied: ffmpy in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (0.3.1)\n",
      "Requirement already satisfied: gradio-client>=0.2.10 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (0.2.10)\n",
      "Requirement already satisfied: httpx in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (0.24.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.14.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (0.16.4)\n",
      "Requirement already satisfied: jinja2<4.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (2.2.0)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (2.0.1)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (3.7.1)\n",
      "Requirement already satisfied: mdit-py-plugins<=0.3.3 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (0.3.3)\n",
      "Requirement already satisfied: numpy~=1.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (1.23.5)\n",
      "Requirement already satisfied: orjson~=3.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (3.9.2)\n",
      "Requirement already satisfied: packaging in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (23.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (1.5.3)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (9.4.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (2.0.3)\n",
      "Requirement already satisfied: pydub in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (0.0.6)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (6.0)\n",
      "Requirement already satisfied: requests~=2.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (2.28.2)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (4.7.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (0.23.1)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio) (11.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from aiohttp~=3.0->gradio) (20.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from aiohttp~=3.0->gradio) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from aiohttp~=3.0->gradio) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from aiohttp~=3.0->gradio) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from aiohttp~=3.0->gradio) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from aiohttp~=3.0->gradio) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from aiohttp~=3.0->gradio) (1.3.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jsonschema>=3.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from altair<6.0,>=4.2.0->gradio) (4.17.3)\n",
      "Requirement already satisfied: toolz in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: fsspec in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from gradio-client>=0.2.10->gradio) (2023.6.0)\n",
      "Requirement already satisfied: filelock in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from huggingface-hub>=0.14.0->gradio) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from huggingface-hub>=0.14.0->gradio) (4.65.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
      "Requirement already satisfied: linkify-it-py<3,>=1 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (2.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (4.39.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (5.12.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from pandas<3.0,>=1.0->gradio) (2020.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.3.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio) (2.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from requests~=2.0->gradio) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from requests~=2.0->gradio) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from requests~=2.0->gradio) (2020.6.20)\n",
      "Requirement already satisfied: click>=7.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from uvicorn>=0.14.0->gradio) (8.1.6)\n",
      "Requirement already satisfied: h11>=0.8 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from fastapi->gradio) (0.27.0)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from httpx->gradio) (0.17.3)\n",
      "Requirement already satisfied: sniffio in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from httpx->gradio) (1.3.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.6.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib~=3.0->gradio) (3.3.0)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (1.3.10)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.17.3)\n",
      "Requirement already satisfied: uc-micro-py in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio) (1.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.15.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: accelerate in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (0.22.0.dev0)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from accelerate) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from accelerate) (5.7.2)\n",
      "Requirement already satisfied: pyyaml in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from accelerate) (2.0.0)\n",
      "Requirement already satisfied: filelock in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: sympy in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /mnt/irisgpfs/apps/resif/iris-rhel8/2020b/gpu/software/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (50.3.0)\n",
      "Requirement already satisfied: wheel in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (0.40.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cmake in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.26.0)\r\n",
      "Requirement already satisfied: lit in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (15.0.7)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.0.1)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /mnt/irisgpfs/users/gmenon/.local/lib/python3.8/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets>=2.6.1\n",
    "!pip install git+https://github.com/huggingface/transformers\n",
    "!pip install librosa\n",
    "# !pip install evaluate>=0.30\n",
    "!pip install git+https://github.com/huggingface/evaluate\n",
    "!pip install jiwer\n",
    "!pip install gradio\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a46f565",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "In this particular section, we will prepare the data for the Medium and Small Whisper. This is a timeconsuming affair and hence, am doing it once and saving it in the disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb34eaa",
   "metadata": {},
   "source": [
    "### MEDIUM WHISPER MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11fa733f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login() #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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "761ea1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from accelerate import Accelerator\n",
    "#accelerator = Accelerator(cpu=True)\n",
    "song_metadata = pd.read_csv(\"/home/users/gmenon/notebooks/home/users/gmenon/notebooks/song_metadata_en_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "05b263c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/users/gmenon/dali/DALI_v1.0/audio/wav_cl...</td>\n",
       "      <td>betrogen kluge worte was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/users/gmenon/dali/DALI_v1.0/audio/wav_cl...</td>\n",
       "      <td>boy my life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/users/gmenon/dali/DALI_v1.0/audio/wav_cl...</td>\n",
       "      <td>ain't what it used to be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/users/gmenon/dali/DALI_v1.0/audio/wav_cl...</td>\n",
       "      <td>since you went out the door</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/users/gmenon/dali/DALI_v1.0/audio/wav_cl...</td>\n",
       "      <td>all the times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166655</th>\n",
       "      <td>/home/users/gmenon/dali/DALI_v1.0/audio/wav_cl...</td>\n",
       "      <td>so take what i left you for the pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166656</th>\n",
       "      <td>/home/users/gmenon/dali/DALI_v1.0/audio/wav_cl...</td>\n",
       "      <td>and do your best to forget my name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166657</th>\n",
       "      <td>/home/users/gmenon/dali/DALI_v1.0/audio/wav_cl...</td>\n",
       "      <td>chealternativeavrei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166658</th>\n",
       "      <td>/home/users/gmenon/dali/DALI_v1.0/audio/wav_cl...</td>\n",
       "      <td>do retta soloa me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166659</th>\n",
       "      <td>/home/users/gmenon/dali/DALI_v1.0/audio/wav_cl...</td>\n",
       "      <td>dentro me</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>166660 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                file_name  \\\n",
       "0       /home/users/gmenon/dali/DALI_v1.0/audio/wav_cl...   \n",
       "1       /home/users/gmenon/dali/DALI_v1.0/audio/wav_cl...   \n",
       "2       /home/users/gmenon/dali/DALI_v1.0/audio/wav_cl...   \n",
       "3       /home/users/gmenon/dali/DALI_v1.0/audio/wav_cl...   \n",
       "4       /home/users/gmenon/dali/DALI_v1.0/audio/wav_cl...   \n",
       "...                                                   ...   \n",
       "166655  /home/users/gmenon/dali/DALI_v1.0/audio/wav_cl...   \n",
       "166656  /home/users/gmenon/dali/DALI_v1.0/audio/wav_cl...   \n",
       "166657  /home/users/gmenon/dali/DALI_v1.0/audio/wav_cl...   \n",
       "166658  /home/users/gmenon/dali/DALI_v1.0/audio/wav_cl...   \n",
       "166659  /home/users/gmenon/dali/DALI_v1.0/audio/wav_cl...   \n",
       "\n",
       "                               transcription  \n",
       "0                   betrogen kluge worte was  \n",
       "1                                boy my life  \n",
       "2                   ain't what it used to be  \n",
       "3                since you went out the door  \n",
       "4                              all the times  \n",
       "...                                      ...  \n",
       "166655  so take what i left you for the pain  \n",
       "166656    and do your best to forget my name  \n",
       "166657                   chealternativeavrei  \n",
       "166658                     do retta soloa me  \n",
       "166659                             dentro me  \n",
       "\n",
       "[166660 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\%\\$\\&\\^\\*\\@\\#\\<\\>\\/\\+\\\\=\\_\\\\}\\{\\)\\(\\]\\[\\`1234567890]'\n",
    "song_metadata[\"transcription\"] = song_metadata[\"transcription\"].replace(chars_to_ignore_regex, '', regex=True)\n",
    "song_metadata = song_metadata[song_metadata.transcription.str.len()>8]\n",
    "song_metadata = song_metadata[['file_name','transcription']]\n",
    "if NO_GPU:=False:\n",
    "    song_metadata[\"language\"] = song_metadata[\"transcription\"].apply(is_english)\n",
    "    song_metadata = song_metadata.loc[song_metadata[\"language\"] == \"en\"]\n",
    "song_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc97f9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'sentence'],\n",
       "        num_rows: 133328\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'sentence'],\n",
       "        num_rows: 33332\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset,Dataset, Audio\n",
    "#audio_dataset = Dataset.from_dict({\"audio\": list(dali_info['AUDIO_PATH'])}).cast_column(\"audio\", Audio())\n",
    "\n",
    "audio_dataset = Dataset.from_dict({\"audio\": list(song_metadata[\"file_name\"]), \"sentence\": list(song_metadata[\"transcription\"])}).cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "audio_dataset = audio_dataset.train_test_split(test_size=0.2, shuffle=True)\n",
    "audio_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec9071e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "from transformers import WhisperFeatureExtractor\n",
    "MODEL_NAME = 'openai/whisper-medium.en'\n",
    "\n",
    "from datasets import Audio\n",
    "\n",
    "\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(MODEL_NAME, language=\"English\", task=\"transcribe\")\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME, language=\"English\", task=\"transcribe\")\n",
    "\n",
    "audio_dataset = audio_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f038306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c90e95d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9143971a912466d89ccebb7eae43417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=50):   0%|          | 0/133328 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142b909c43084c1da1d9d38adaedcdf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=50):   0%|          | 0/33332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "audio_dataset = audio_dataset.map(prepare_dataset, remove_columns=audio_dataset.column_names[\"train\"], num_proc = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc52e159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6741221c82a643b3b103825c0f3a12b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/257 shards):   0%|          | 0/133328 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b55461816b41a1bbf4a081a09073ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/65 shards):   0%|          | 0/33332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "audio_dataset.save_to_disk(\"/scratch/users/gmenon/dali_medium_en_all_transformed.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "008b65b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_features', 'labels'],\n",
       "        num_rows: 133328\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_features', 'labels'],\n",
       "        num_rows: 33332\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4ca6000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03a2352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_from_disk\n",
    "# audio_dataset1 = load_from_disk(\"/scratch/users/gmenon/dali_medium_en_all_transformed.hf\")\n",
    "# audio_dataset1\n",
    "#del audio_dataset, audio_dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a451f40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208f005832584c7daa87ff3b74c5d066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/257 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c1c761767842b0952e5b567df6f0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1123573e92242eab037d615bcfb64e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://huggingface.co/api/datasets/gmenon/dali_medium_en_all_transformed/commit/main (Request ID: Root=1-64c47b25-49c28ebf0722f69844b26f3d;46829274-6cca-4f31-bdf5-e88c46da2f3d)\n\nYou have exceeded our hourly quotas for action: commit. We invite you to retry later.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:261\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 261\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/datasets/gmenon/dali_medium_en_all_transformed/commit/main",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43maudio_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgmenon/dali_medium_en_all_transformed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/datasets/dataset_dict.py:1641\u001b[0m, in \u001b[0;36mDatasetDict.push_to_hub\u001b[0;34m(self, repo_id, config_name, private, token, branch, max_shard_size, num_shards, embed_external_files)\u001b[0m\n\u001b[1;32m   1639\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPushing split \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1640\u001b[0m \u001b[38;5;66;03m# The split=key needs to be removed before merging\u001b[39;00m\n\u001b[0;32m-> 1641\u001b[0m repo_id, split, uploaded_size, dataset_nbytes, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_push_parquet_shards_to_hub\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1644\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprivate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprivate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbranch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbranch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_shards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_shards\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43membed_external_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_external_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1652\u001b[0m total_uploaded_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m uploaded_size\n\u001b[1;32m   1653\u001b[0m total_dataset_nbytes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dataset_nbytes\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/datasets/arrow_dataset.py:5307\u001b[0m, in \u001b[0;36mDataset._push_parquet_shards_to_hub\u001b[0;34m(self, repo_id, data_dir, split, private, token, branch, max_shard_size, num_shards, embed_external_files)\u001b[0m\n\u001b[1;32m   5305\u001b[0m         shard\u001b[38;5;241m.\u001b[39mto_parquet(buffer)\n\u001b[1;32m   5306\u001b[0m         uploaded_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m buffer\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m-> 5307\u001b[0m         \u001b[43m_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5308\u001b[0m \u001b[43m            \u001b[49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5309\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m   5310\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath_or_fileobj\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5311\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath_in_repo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_path_in_repo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5312\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrepo_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5313\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5314\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrepo_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5315\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrevision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbranch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5316\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5317\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHTTPError\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5318\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstatus_codes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m504\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5319\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_wait_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5320\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5321\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_wait_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5322\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5323\u001b[0m     shards_path_in_repo\u001b[38;5;241m.\u001b[39mappend(shard_path_in_repo)\n\u001b[1;32m   5325\u001b[0m \u001b[38;5;66;03m# Cleanup to remove unused files\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/datasets/utils/file_utils.py:283\u001b[0m, in \u001b[0;36m_retry\u001b[0;34m(func, func_args, func_kwargs, exceptions, status_codes, max_retries, base_wait_time, max_wait_time)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retry \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_retries \u001b[38;5;129;01mor\u001b[39;00m (status_codes \u001b[38;5;129;01mand\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m status_codes):\n\u001b[0;32m--> 283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m         sleep_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(max_wait_time, base_wait_time \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mretry)  \u001b[38;5;66;03m# Exponential backoff\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/datasets/utils/file_utils.py:280\u001b[0m, in \u001b[0;36m_retry\u001b[0;34m(func, func_args, func_kwargs, exceptions, status_codes, max_retries, base_wait_time, max_wait_time)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m retry \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_retries \u001b[38;5;129;01mor\u001b[39;00m (status_codes \u001b[38;5;129;01mand\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m status_codes):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/huggingface_hub/hf_api.py:828\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m--> 828\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/huggingface_hub/hf_api.py:3221\u001b[0m, in \u001b[0;36mHfApi.upload_file\u001b[0;34m(self, path_or_fileobj, path_in_repo, repo_id, token, repo_type, revision, commit_message, commit_description, create_pr, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   3213\u001b[0m commit_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   3214\u001b[0m     commit_message \u001b[38;5;28;01mif\u001b[39;00m commit_message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpload \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_in_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with huggingface_hub\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3215\u001b[0m )\n\u001b[1;32m   3216\u001b[0m operation \u001b[38;5;241m=\u001b[39m CommitOperationAdd(\n\u001b[1;32m   3217\u001b[0m     path_or_fileobj\u001b[38;5;241m=\u001b[39mpath_or_fileobj,\n\u001b[1;32m   3218\u001b[0m     path_in_repo\u001b[38;5;241m=\u001b[39mpath_in_repo,\n\u001b[1;32m   3219\u001b[0m )\n\u001b[0;32m-> 3221\u001b[0m commit_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_commit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3224\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent_commit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_commit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3231\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_info\u001b[38;5;241m.\u001b[39mpr_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3234\u001b[0m     revision \u001b[38;5;241m=\u001b[39m quote(_parse_revision_from_pr_url(commit_info\u001b[38;5;241m.\u001b[39mpr_url), safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/huggingface_hub/hf_api.py:828\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m--> 828\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/huggingface_hub/hf_api.py:2728\u001b[0m, in \u001b[0;36mHfApi.create_commit\u001b[0;34m(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   2726\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2727\u001b[0m     commit_resp \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mpost(url\u001b[38;5;241m=\u001b[39mcommit_url, headers\u001b[38;5;241m=\u001b[39mheaders, data\u001b[38;5;241m=\u001b[39mdata, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m-> 2728\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommit_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcommit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2730\u001b[0m     e\u001b[38;5;241m.\u001b[39mappend_to_message(_CREATE_COMMIT_NO_REPO_ERROR_MESSAGE)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:303\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/datasets/gmenon/dali_medium_en_all_transformed/commit/main (Request ID: Root=1-64c47b25-49c28ebf0722f69844b26f3d;46829274-6cca-4f31-bdf5-e88c46da2f3d)\n\nYou have exceeded our hourly quotas for action: commit. We invite you to retry later."
     ]
    }
   ],
   "source": [
    "audio_dataset.push_to_hub(\"gmenon/dali_medium_en_all_transformed\", private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2808c5ab",
   "metadata": {},
   "source": [
    "### SMALL WHISPER MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65d3811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "from transformers import WhisperFeatureExtractor\n",
    "MODEL_NAME = 'openai/whisper-small'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6fb998dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import Audio\n",
    "\n",
    "\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(MODEL_NAME, language=\"English\", task=\"transcribe\")\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME, language=\"English\", task=\"transcribe\")\n",
    "\n",
    "audio_dataset = audio_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7dd64f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': [{'path': '/home/users/gmenon/dali/DALI_v1.0/audio/wav_clips/769ec653efc24afaaf5ef537dc06a2bd.wav',\n",
       "   'array': array([-0.07713105, -0.11468691, -0.11935695, ..., -0.24585995,\n",
       "          -0.1966546 , -0.09040548]),\n",
       "   'sampling_rate': 16000},\n",
       "  {'path': '/home/users/gmenon/dali/DALI_v1.0/audio/wav_clips/7d4e34998991418b8c4ed0dace25d389.wav',\n",
       "   'array': array([-0.06239897, -0.16798243, -0.0176416 , ...,  0.13757551,\n",
       "           0.0514433 , -0.01852267]),\n",
       "   'sampling_rate': 16000}],\n",
       " 'sentence': ['then you married', \"and they'll be placing\"]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_dataset[\"train\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2bba253c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce4219b8a384c52a8a37c685ae7f154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/133328 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef7b292df094cdcb06ffd4bf9218740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/33332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "audio_dataset.save_to_disk(\"/scratch/users/gmenon/dali_small_en_all_transformed.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e51117c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca383ecd8c2d490a981bebd30a342cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/44443 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c824282b36904640b1540ae32266bc93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb52c72d60124ddc87ca5d7ee17fa229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/445 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320699cbbb7745a793c1b7fb1e9b0e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/44443 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739242d30ad646fe9caf76afc3e57404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/445 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e62a24fc1814d85a1d77a4c3ed1f756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/44442 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "audio_dataset.push_to_hub(\"gmenon/dali_small_en_all_transformed\", private=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a67cc3",
   "metadata": {},
   "source": [
    "# MODEL TRAINING \n",
    "\n",
    "In this particular section, the whisper models will be trained and saved to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bc70fa",
   "metadata": {},
   "source": [
    "#### MEDIUM MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecc3ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "audio_dataset = load_from_disk(\"/scratch/users/gmenon/dali_medium_en_all_transformed.hf\")\n",
    "MODEL_NAME = 'openai/whisper-medium.en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3616be4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ec699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de4a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dee16d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4495fa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eac813",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce35d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "fp16 = True\n",
    "    \n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=MODEL_NAME + \"v0.1\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=1500,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=fp16,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=4,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    "    resume_from_checkpoint=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cffbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=audio_dataset[\"train\"],\n",
    "    eval_dataset=audio_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a208c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cf79ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b14e67",
   "metadata": {},
   "source": [
    "#### SMALL MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6528834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "audio_dataset = load_from_disk(\"/scratch/users/gmenon/dali_small_en_all_transformed.hf\")\n",
    "MODEL_NAME = 'openai/whisper-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8aa085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e724583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04af4dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d23209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa69ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1337b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16084de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "fp16 = True\n",
    "    \n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=MODEL_NAME + \"v0.1\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=1500,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=fp16,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=4,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    "    resume_from_checkpoint=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3afc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=audio_dataset[\"train\"],\n",
    "    eval_dataset=audio_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ae2f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59e53c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30de748",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
