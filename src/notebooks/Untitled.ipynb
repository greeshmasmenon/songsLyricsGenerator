{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db9aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.path.expanduser(\"/home/users/gmenon/workspace/songsLyricsGenerator/src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c00bb179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants.mir_constants import WAV2VEC2_ARGS\n",
    "from training.wav2vec2_finetune import Wav2Vec2SpeechRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbff528e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2835697/1797219780.py:1: FutureWarning: Please pass an instantiated object of the `InputTransform` class. Passing the Class and keyword arguments separately has been deprecated since v0.8.0 and will be removed in v0.9.0.\n",
      "  wav2vec2 = Wav2Vec2SpeechRecognition(WAV2VEC2_ARGS)\n"
     ]
    }
   ],
   "source": [
    "wav2vec2 = Wav2Vec2SpeechRecognition(wav2vec2_args=WAV2VEC2_ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "118e1ca5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwav2vec2\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "len(wav2vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7a816d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846f9467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c12d17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61089db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c17359e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0e5489",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav2vec2 = Wav2Vec2SpeechRecognition(WAV2VEC2_ARGS)\n",
    "\n",
    "print(\"Length of training and validation datasets\" + str(len(wav2vec2))\n",
    "\n",
    "wav2vec2.model = \n",
    "wav2vec2_model = SpeechRecognition(backbone=wav2vec2_args.MODEL_BACKBONE)\n",
    "\n",
    "# Create the trainer, finetune and save the model\n",
    "wav2vec2_trainer = flash.Trainer(accumulate_grad_batches=10,\n",
    "                        precision=16,\n",
    "                        max_epochs=wav2vec2_args.NUM_EPOCHS, \n",
    "                        gpus=wav2vec2_args.NUM_GPUS)\n",
    "\n",
    "wav2vec2_trainer.finetune(wav2vec2_model,\n",
    "                 datamodule=datamodule, \n",
    "                 strategy=wav2vec2_args.FINETUNE_STRATEGY)\n",
    "\n",
    "wav2vec2_trainer.save_checkpoint(wav2vec2_args.MODEL_SAVE_PATH)\n",
    "\n",
    "\n",
    "# Whisper \n",
    "whisper_model = SpeechRecognition(backbone=whisper_args.MODEL_BACKBONE)\n",
    "\n",
    "# Create the trainer, finetune and save the model\n",
    "whisper_trainer = flash.Trainer(accumulate_grad_batches=10,\n",
    "                                precision=16, \n",
    "                                max_epochs=whisper_args.NUM_EPOCHS,\n",
    "                                gpus=whisper_args.NUM_GPUS)\n",
    "\n",
    "whisper_trainer.finetune(whisper_model, datamodule=datamodule, \n",
    "                         strategy=whisper_args.FINETUNE_STRATEGY)\n",
    "\n",
    "whisper_trainer.save_checkpoint(whisper_args.MODEL_SAVE_PATH)\n",
    "\n",
    "\n",
    "# 4. Predict on audio files!\n",
    "test_datamodule = SpeechRecognitionData.from_files(\n",
    "    predict_files=[\"/home/users/gmenon/workspace/songsLyricsGenerator/test_clip.wav\"], \n",
    "    batch_size=training_args.BATCH_SIZE)\n",
    "\n",
    "wav2vec2_predictions = wav2vec2_trainer.predict(wav2vec2_model, \n",
    "                                                datamodule=test_datamodule)\n",
    "\n",
    "print(\"Wav2vec2 predictions \")\n",
    "print(wav2vec2_predictions)\n",
    "\n",
    "print(\"Whisper predictions \")\n",
    "whisper_predictions = whisper_trainer.predict(whisper_model, datamodule=test_datamodule)\n",
    "print(whisper_predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
